{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils module\n",
    "\n",
    "> Provides utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_init.ipynb.\n",
      "Converted 01_graph.ipynb.\n",
      "Converted 02_tree.ipynb.\n",
      "Converted 03_synteny.ipynb.\n",
      "Converted 04_utils.ipynb.\n",
      "Converted 05_export.ipynb.\n",
      "Converted 05_exportDev.ipynb.\n",
      "Converted deBruijnGraphProcessing.ipynb.\n",
      "Converted dev.ipynb.\n",
      "Converted graphTesting.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections.abc import Iterable\n",
    "# from copy import deepcopy\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from redis import Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path files operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pathFileToPathDict(filePath,directional=True,sort=True):\n",
    "    '''\n",
    "    Reads path file (ASCII file) and translates it to path dictionary for `GenGraph` class constructor.\n",
    "    \n",
    "    Path file has a path on each line in the following format:\n",
    "    <path name>: <nodeID[+|-]>[,<nodeID[+,-]>]\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `filePath`: str. Absolute path to the path file.\n",
    "    `directional`: boolean (default: True). Whether the path file contains \n",
    "                   directionality (whether nodeID has `+` or `-` at the end). \n",
    "                   If False (the file does not contain directionality marks), \n",
    "                   then all nodes in all paths are treated as positive (not inverted).\n",
    "    `sort`: boolean, str or iterable (e.g. list). If Boolean, true means that the paths should be sorted\n",
    "            in lexicographic order (by names) and appear in the path dict in sorted order. If False,\n",
    "            then no sorting is done and paths will appear as they are in the file. If iterable with \n",
    "            indexes of the paths (from 0 to n-1 for n paths), then this particular order will be used\n",
    "            in the paths dictionary. If str, then it should provide a single path name that should appear \n",
    "            first with the rest being sorted in lexicographic order.\n",
    "            \n",
    "    Return\n",
    "    ======\n",
    "    \n",
    "    `paths`: a dictionary with path names as keys and lists of nodeID (str) with directionality markers (`+` or `-`) \n",
    "             at the end as values.\n",
    "    \n",
    "    '''\n",
    "    _paths = {}\n",
    "    with open(filePath) as f:\n",
    "        for line in f:\n",
    "            pathName,pathNodeList = line.strip(' \\n\\t').split(':')\n",
    "            if directional:\n",
    "                _paths[pathName] = pathNodeList.strip().split(',')\n",
    "            else:\n",
    "                _paths[pathName] = [f'{node}+' for node in pathNodeList.strip().split(',')]\n",
    "    \n",
    "    if isinstance(sort,bool):\n",
    "        if sort:\n",
    "            paths = {}\n",
    "            for accession in sorted(list(_paths.keys())):\n",
    "                paths[accession] = _paths[accession]\n",
    "            del _paths\n",
    "        else:\n",
    "            paths = _paths\n",
    "    elif isinstance(sort,str):\n",
    "        paths = {}\n",
    "        pathNamesList = list(_paths.keys())\n",
    "        try:\n",
    "            paths[sort] = _paths[sort]\n",
    "            pathNamesList.remove(sort)\n",
    "        except KeyError:\n",
    "            Warning(f'Path name {sort} was not found in path file. All paths are sorted in lexicographic order')\n",
    "        for accession in sorted(pathNamesList):\n",
    "            paths[accession] = _paths[accession]\n",
    "        del _paths\n",
    "    elif isinstance(sort,Iterable):\n",
    "        paths = {}\n",
    "        pathNamesList = list(_paths.keys())\n",
    "        for pathInd in sort:\n",
    "            accession = pathNamesList[pathInd]\n",
    "            paths[accession] = _path[accession]\n",
    "        del _paths\n",
    "    else:\n",
    "        paths = _paths\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TAIR10', '10002', '10015', '10024', '1741', '22001', '22002', '22003', '22004', '22005', '22006', '22007', '6024', '6069', '6124', '6244', '6909', '6966', '8236', '9075', '9537', '9543', '9638', '9728', '9764', '9888', '9905', '9981'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE UNIT TESTS FOR pathFileToPathDict!!!\n",
    "import os\n",
    "pathfileDir = '../../1001G/coreGraph'\n",
    "pathsfile = 'paths_f2.1_Ref_v03.txt'\n",
    "\n",
    "paths = pathFileToPathDict(f'{pathfileDir}{os.path.sep}{pathsfile}',True,sort='TAIR10')\n",
    "paths.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export parameters processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pathConvert(inputPath, suffix=''):\n",
    "    outputPath = os.path.dirname(inputPath)\n",
    "    outputName = '.'.join(os.path.splitext(os.path.basename(inputPath))[:-1])+suffix\n",
    "    return outputPath,outputName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def checkZoomLevels(zoomLevels):\n",
    "    '''\n",
    "    Check that each previous zoom level is factor of next one\n",
    "    '''\n",
    "    _zoomLevels = np.array(zoomLevels)\n",
    "    div = _zoomLevels[1:]/_zoomLevels[:-1]\n",
    "    return not np.any(div - div.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adjustZoomLevels(zoomLevels):\n",
    "    '''\n",
    "    If there is no zoom level 1, adds it to the list.\n",
    "    '''\n",
    "    if not checkZoomLevels(zoomLevels):\n",
    "        raise ValueError('Zoom level list is incorrect. Each next level \\\n",
    "                          should have previous one as factor.')\n",
    "    if min(zoomLevels) > 1:\n",
    "        zoomLevels = [1] + zoomLevels\n",
    "    return zoomLevels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy to JSON encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://stackoverflow.com/questions/50916422/python-typeerror-object-of-type-int64-is-not-json-serializable\n",
    "# Class for encoding np types to JSON\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional dict structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class bidict(dict):\n",
    "    '''\n",
    "    Here is a class for a bidirectional dict, inspired by Finding key from\n",
    "    value in Python dictionary and modified to allow the following 2) and 3).\n",
    "\n",
    "    Note that :\n",
    "\n",
    "    1) The inverse directory bd.inverse auto-updates itself when the standard\n",
    "        dict bd is modified.\n",
    "    2) The inverse directory bd.inverse[value] is always a list of keys such\n",
    "        that value in bd[key] for each key.\n",
    "    3) Unlike the bidict module from https://pypi.python.org/pypi/bidict,\n",
    "        here we can have 2 keys having same value, this is very important.\n",
    "    4) After modification, values in the \"forward\" (not inversed) dict\n",
    "        can be lists (or any iterables theoretically,\n",
    "        but only list was tested).\n",
    "\n",
    "    For implementing 4), new method `add` was introduced.\n",
    "    If d[key].append(value) attempted, the link between main and inversed dict\n",
    "    will be broken. Method `add` can accept both\n",
    "\n",
    "    Credit:\n",
    "    Implemented as an answer to\n",
    "    https://stackoverflow.com/questions/3318625/how-to-implement-an-efficient-bidirectional-hash-table\n",
    "    by Basj (https://stackoverflow.com/users/1422096/basj).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.inverse = {}\n",
    "        for key, value in self.items():\n",
    "            if isinstance(value, Iterable):\n",
    "                for v in value:\n",
    "                    self.inverse.setdefault(v, []).append(key)\n",
    "            else:\n",
    "                self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            keyV = self[key]\n",
    "            if isinstance(keyV, Iterable):\n",
    "                for v in keyV:\n",
    "                    self.inverse[v].remove(key)\n",
    "            else:\n",
    "                self.inverse[keyV].remove(key)\n",
    "        super(bidict, self).__setitem__(key, value)\n",
    "        if isinstance(value, Iterable):\n",
    "            for v in value:\n",
    "                self.inverse.setdefault(v, []).append(key)\n",
    "        else:\n",
    "            self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        value = self[key]\n",
    "        if isinstance(value, Iterable):\n",
    "            for v in value:\n",
    "                self.inverse.setdefault(v, []).remove(key)\n",
    "                if v in self.inverse and not self.inverse[v]:\n",
    "                    del self.inverse[v]\n",
    "        else:\n",
    "            self.inverse.setdefault(value, []).remove(key)\n",
    "            if value in self.inverse and not self.inverse[value]:\n",
    "                del self.inverse[value]\n",
    "        super(bidict, self).__delitem__(key)\n",
    "\n",
    "    def add(self, key, value):\n",
    "        valKey = self.setdefault(key, [])\n",
    "        if isinstance(valKey,Iterable):\n",
    "            valKey = set(valKey)\n",
    "        else:\n",
    "            valKey = set([valKey])\n",
    "\n",
    "        if isinstance(value, Iterable):\n",
    "            valKey = valKey.union(value)\n",
    "        else:\n",
    "            valKey.add(value)\n",
    "\n",
    "        self[key] = list(valKey)\n",
    "    \n",
    "    def remove(self,key,value):\n",
    "        valKey = set(self.setdefault(key,[]))\n",
    "        \n",
    "        if isinstance(value,Iterable):\n",
    "            valKey.difference_update(value)\n",
    "        else:\n",
    "            valKey.remove(value)\n",
    "        \n",
    "        self[key] = list(valKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def getDBID(pathToDictFile,caseName):\n",
    "    dbid = None\n",
    "    if os.path.exists(pathToDictFile):\n",
    "        caseToDBID = joblib.load(pathToDictFile)\n",
    "    else:\n",
    "        caseToDBID = {caseName:0}\n",
    "        dbid = 0\n",
    "\n",
    "    dbid = caseToDBID.get(caseName,None)\n",
    "\n",
    "    if dbid is None:\n",
    "        dbidArray = sorted(list(caseToDBID.values()))\n",
    "        if dbidArray[0]>0:\n",
    "            dbid = 0\n",
    "        else:\n",
    "            prevID = 0\n",
    "            if len(dbidArray)>1:\n",
    "                for curID in dbidArray[1:]:\n",
    "                    if prevID + 1 < curID:\n",
    "                        break\n",
    "                    prevID = curID\n",
    "            dbid = prevID + 1\n",
    "        caseToDBID[caseName] = dbid\n",
    "\n",
    "    joblib.dump(caseToDBID,pathToDictFile)\n",
    "\n",
    "    return dbid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB cleaning and maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resetDB(pathToDictFile,redisServer='redis',port=6379):\n",
    "    os.remove(pathToDictFile)\n",
    "    conn = Redis(host=redisServer,port=port,db=0)\n",
    "    conn.flushall()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions implementing secondary interval set in Redis database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iset_add(r,name,intervalMapping):\n",
    "    '''\n",
    "        Add members with intervals to interval set. If interval set does not exist, it will be created. \n",
    "        In reality, it will create two Redis Sorted Sets for starts and ends of the intervals.\n",
    "        The rest of the functions ``iset_`` will know what to do with them.\n",
    "        \n",
    "        ``r``: Redis object. Redis client.\n",
    "        ``name``: string. Name of the interval set.\n",
    "        ``intervalMapping``: dict. Dictionary with names of intervals as keys and \n",
    "                tuples with start and end of intervals.\n",
    "                \n",
    "        \n",
    "        Return number of added intervals. In reality, it adds equal number of elements \n",
    "        to two sorted sets, if number of added elements are not equal, DataError is raised.\n",
    "        \n",
    "    '''\n",
    "    starts = {f'{n}_{seqnum}':int(interval[0]) for n,inv in intervalMapping.items() for seqnum,interval in enumerate(inv)}\n",
    "    ends = {f'{n}_{seqnum}':int(interval[1]) for n,inv in intervalMapping.items() for seqnum,interval in enumerate(inv)}\n",
    "    numAddedStarts = r.zadd(f'{name}Start',mapping=starts)\n",
    "    numAddedEnds = r.zadd(f'{name}End',mapping=ends)\n",
    "    if numAddedStarts!=numAddedEnds:\n",
    "        raise DataError(f'Not equal number of starts and ends were added to DB. For consistency, the sorted sets {name}Start and {name}End should be checked and/or recreated')\n",
    "    return numAddedStarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iset_get(r,name,member=None):\n",
    "    '''\n",
    "        Return either the whole interval set or specific name(s) with its interval.\n",
    "        \n",
    "        ``r``: Redis object. Redis client.\n",
    "        ``name``: string. Name of the interval set.\n",
    "        ``member``: string, list, tuple or None. If None, function return all members with their respective intervals.\n",
    "            If string, returns a single member with its interval,\n",
    "            if list or tuple, returns all requested members with their respecitve intervals.\n",
    "\n",
    "        Return a dictionary with member names as keys and tuples with interval starts and ends as values.\n",
    "        For member names not found in interval set, the value for the given key will be a tuple (None,None).\n",
    "    '''\n",
    "    if member is None:\n",
    "        starts = {k.decode():v for k,v in r.zrange(f'{name}Start',0,-1,withscores=True)}\n",
    "        ends = {k.decode():v for k,v in r.zrange(f'{name}End',0,-1,withscores=True)}\n",
    "        return {k:(starts[k],ends[k])for k in starts.keys()}\n",
    "    elif isinstance(member,str):\n",
    "        intStart = r.zscore(f'{name}Start',member)\n",
    "        intEnd = r.zscore(f'{name}End',member)\n",
    "        return {member: (intStart, intEnd)}\n",
    "    else:\n",
    "        res = {}\n",
    "        for mm in member:\n",
    "            intStart = r.zscore(f'{name}Start',mm)\n",
    "            intEnd = r.zscore(f'{name}End',mm)\n",
    "            res[mm] = (intStart, intEnd)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iset_score(r,name,start,end=None):\n",
    "    '''\n",
    "        Returns all member names whose interval contains a given value or intersects with the given interval\n",
    "        \n",
    "        ``r``: Redis object. Redis client.\n",
    "        ``name``: string. Name of the interval set\n",
    "        ``start``: int. Query value or the start of query interval.\n",
    "        ``end``: int or None. If None, ``start`` is treated as a single query value. \n",
    "                If int, then ``start`` is the start of the query interval, \n",
    "                ``end`` is the end of the query interval.\n",
    "                \n",
    "        Returns a list of members whose intervals either contain query value or intersects with query interval.\n",
    "    '''\n",
    "    if end:\n",
    "        _endPos = end\n",
    "    else:\n",
    "        _endPos = start\n",
    "    if _endPos<start:\n",
    "        raise ValueError('``start`` should be less or equal to ``end``.')\n",
    "    tid = random.randint(1e8,1e9-1)\n",
    "#     r.execute_command('ZRANGESTORE',*['startSetTemp','geneStart','-inf',_endPos,'BYSCORE'])\n",
    "#     r.execute_command('ZRANGESTORE',*['endSetTemp','geneEnd',start,'inf','BYSCORE'])\n",
    "    r.zrangestore(f'startSetTemp_{tid}',f'{name}Start','-inf',_endPos,byScore=True)\n",
    "    r.zrangestore(f'endSetTemp_{tid}',f'{name}End',start,'inf',byScore=True)\n",
    "    res = ['_'.join(el.decode().split('_')[:-1]) for el in r.zinter([f'startSetTemp_{tid}',f'endSetTemp_{tid}'])]\n",
    "    r.delete(f'startSetTemp_{tid}',f'endSetTemp_{tid}')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iset_not_score(r,name,start,end=None):\n",
    "    '''\n",
    "        Returns all intervals (member names only) where query value is not contained or query interval is not intersecting.\n",
    "        Inverison of ``iset_score()`` function\n",
    "        \n",
    "        ``r``: Redis object. Redis client.\n",
    "        ``name``: string. Name of the interval set\n",
    "        ``start``: int. Query value or the start of query interval.\n",
    "        ``end``: int or None. If None, ``start`` is treated as a single query value. \n",
    "                If int, then ``start`` is the start of the query interval, \n",
    "                ``end`` is the end of the query interval.\n",
    "                \n",
    "        Returns a list of members whose intervals either does not contain query value or does not intersect with query interval.\n",
    "    \n",
    "    '''\n",
    "    if end:\n",
    "        _endPos = end\n",
    "    else:\n",
    "        _endPos = start\n",
    "    if _endPos<start:\n",
    "        raise ValueError('``start`` should be less or equal to ``end``.')\n",
    "    tid = random.randint(1e8,1e9-1)\n",
    "\n",
    "    r.zrangestore(f'startSetTemp_{tid}',f'{name}Start','-inf',_endPos,byScore=True)\n",
    "    r.zrangestore(f'endSetTemp_{tid}',f'{name}End',start,'inf',byScore=True)\n",
    "    r.zinterstore(f'foundSetTemp_{tid}',[f'startSetTemp_{tid}',f'endSetTemp_{tid}'])\n",
    "    r.zrangestore(f'allSetTemp_{tid}',f'{name}Start','-inf','inf',byScore=True)\n",
    "    res = [el.decode() for el in (r.zdiff([f'allSetTemp_{tid}',f'foundSetTemp_{tid}']))]\n",
    "    r.delete(f'startSetTemp_{tid}',f'endSetTemp_{tid}',f'allSetTemp_{tid}',f'foundSetTemp_{tid}')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iset_del(r,name,member=None):\n",
    "    '''\n",
    "        Return either the whole interval set or specific name(s) with its interval.\n",
    "        \n",
    "        ``r``: Redis object. Redis client.\n",
    "        ``name``: string. Name of the interval set.\n",
    "        ``member``: string, list, tuple or None. If None, function return all members with their respective intervals.\n",
    "            If string, returns a single member with its interval,\n",
    "            if list or tuple, returns all requested members with their respecitve intervals.\n",
    "\n",
    "        Return number of removed intervals. In reality, it removes equal number of elements \n",
    "        from two sorted sets, if number of added elements are not equal, DataError is raised.\n",
    "    '''\n",
    "    if member is None:\n",
    "        keyRemovedStart = r.delete(f'{name}Start')\n",
    "        keyRemovedEnd = r.delete(f'{name}End')\n",
    "        if keyRemovedStart==1 and keyRemovedEnd==1:\n",
    "            return 1\n",
    "        else:\n",
    "            raise DataError('Less than two sorted sets were deleted. Something is wrong with the Redis DB.')\n",
    "    elif isinstance(member,str):\n",
    "        removedStartCount = r.zrem(f'{name}Start',member)\n",
    "        removedEndCount = r.zrem(f'{name}End',member)\n",
    "    else:\n",
    "        removedStartCount = r.zrem(f'{name}Start',*member)\n",
    "        removedEndCount = r.zrem(f'{name}End',*member)\n",
    "    \n",
    "    if removedStartCount==removedEndCount:\n",
    "        return removedStartCount\n",
    "    else:\n",
    "        raise DataError(f'Not equal number of starts and ends were deleted from DB. \\\n",
    "        For consistency, the sorted sets {name}Start and {name}End should be checked and/or recreated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De Bruijn Graphs experimental functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processdeBruijnGFA(gfaFile,kmerSize=None,accessionFASTA=None,accessionsToRemove=None):\n",
    "    baseName = os.path.splitext(os.path.basename(gfaFile))[0]\n",
    "    dirPath = os.path.dirname(gfaFile)\n",
    "\n",
    "    print(f'Loading graph from {gfaFile}')\n",
    "\n",
    "    prevPathSegment = None\n",
    "\n",
    "    gfaList = open(gfaFile,mode='r').readlines()\n",
    "\n",
    "    headerList = gfaList[0].split('\\t')\n",
    "\n",
    "    gfaVersion = [el.rstrip() for el in headerList if el[:2]=='VN'][0][5:]\n",
    "    if kmerSize is None:\n",
    "        kmerSize = int([el.rstrip() for el in headerList if el[:2]=='KL'][0][5:])\n",
    "\n",
    "    if gfaVersion == '2.0':\n",
    "        raise NotImplementedError('At the moment only GFA v1 is supported')\n",
    "    elif gfaVersion != '1.0':\n",
    "        warnings.warn('Cannot identify version of GFA. Assuming it is GFA v1. If it is not, expect unpredictable results.',category=ResourceWarning)\n",
    "\n",
    "    segmentList = [el for el in gfaList if el.lower().startswith('s')]\n",
    "    linkList = [el for el in gfaList if el.lower().startswith('l')]\n",
    "    pathStringsList = [el for el in gfaList if el.lower().startswith('p')]\n",
    "\n",
    "    nodeNameToID = {}\n",
    "\n",
    "    nodes = []\n",
    "    nodesData = []\n",
    "\n",
    "    numSegments = len(segmentList)\n",
    "    numSegmentDigits = int(np.ceil(np.log10(numSegments)))\n",
    "\n",
    "    for nodeID,node in enumerate(segmentList):\n",
    "        print(f'\\nLoading segment {nodeID+1:0{numSegmentDigits}}/{numSegments:0{numSegmentDigits}}',end='')\n",
    "        segmentArray = node.rstrip().split(sep='\\t')\n",
    "        segID,segGFAData = segmentArray[1:3]\n",
    "        segSeq = segGFAData\n",
    "\n",
    "        segName = str(segID)\n",
    "\n",
    "        nodeNameToID[segID] = len(nodes)+1\n",
    "\n",
    "        nodes.append(segName)\n",
    "        nodesData.append(segSeq)\n",
    "\n",
    "    print('\\nLoading segments finished.')\n",
    "\n",
    "    numLinks = len(linkList)\n",
    "    numLinkDigits = int(np.ceil(np.log10(numLinks)))\n",
    "\n",
    "    forwardLinks = {}\n",
    "\n",
    "    for linkID,link in enumerate(linkList):\n",
    "        print(f'\\nLoading link {linkID+1:0{numLinkDigits}}/{numLinks:0{numLinkDigits}}',end='')\n",
    "        linkArray = link.rstrip().split(sep='\\t')\n",
    "        fromNodeID,fromStrand,toNodeID,toStrand = linkArray[1:5]\n",
    "        fromNode = nodeNameToID[fromNodeID]\n",
    "        toNode = nodeNameToID[toNodeID]\n",
    "\n",
    "        curLink = forwardLinks.setdefault(fromNode,{})\n",
    "        curStrand = curLink.setdefault(fromStrand,[])\n",
    "        if (toNode,toStrand) not in curStrand:\n",
    "            curStrand.append((toNode,toStrand))\n",
    "    print('\\nLoading links finished')\n",
    "\n",
    "    paths = []\n",
    "    accessions = []\n",
    "\n",
    "    numPaths = len(pathStringsList)\n",
    "    numPathDigits = int(np.ceil(np.log10(numPaths)))\n",
    "    addedPaths = 0\n",
    "    ignoredPaths = 0\n",
    "    for pathID,pathString in enumerate(pathStringsList):\n",
    "        print(f'\\nLoading path {pathID+1:0{numPathDigits}}/{numPaths:0{numPathDigits}}',end='')\n",
    "        pathArray = pathString.rstrip().split(sep='\\t')\n",
    "        seqID,path = pathArray[1:3]\n",
    "        useAccession = True\n",
    "        if isinstance(accessionsToRemove,list):\n",
    "            for accessionTemplate in accessionsToRemove:\n",
    "                if seqID.find(accessionTemplate)!=-1:\n",
    "                    ignoredPaths += 1\n",
    "                    useAccession = False\n",
    "                    break\n",
    "        if useAccession:\n",
    "            paths.append(path.split(','))\n",
    "            accessions.append(seqID)\n",
    "            addedPaths += 1\n",
    "    print(f'\\nLoading paths finished. {addedPaths} paths added, {ignoredPaths} paths ignored.')\n",
    "\n",
    "    return nodes,nodesData,nodeNameToID,forwardLinks,paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = '../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/full_transpose.gfa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/full_transpose.gfa\n",
      "Loading segment 4/4\n",
      "Loading segments finished.\n",
      "Loading link 4/4\n",
      "Loading links finished\n",
      "Loading path 2/2\n",
      "Loading paths finished. 2 paths added, 0 paths ignored.\n"
     ]
    }
   ],
   "source": [
    "nodes,nodesData,nodeNameToID,forwardLinks,paths = processdeBruijnGFA(paths,kmerSize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1059', '695', '5571', '4423']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _pathCount(self):\n",
    "        # self.inPath = []\n",
    "        # self.outPath = []\n",
    "        self.nodePass = [0]*len(self.nodes)\n",
    "        \n",
    "        self.nodeStrandPaths = [[0,0] for _ in range(len(self.nodes))]\n",
    "        self.pathStarts = [0]*len(self.nodes)\n",
    "        self.inPath = [0]*len(self.nodes)\n",
    "        self.outPath = [0]*len(self.nodes)\n",
    "        self.edgePaths = {}\n",
    "        \n",
    "        for path in self.paths:\n",
    "            \n",
    "            previousNode = None\n",
    "            for nodeStrand in path:\n",
    "                pathNode = self.nodeNameToID[nodeStrand[:-1]]-1\n",
    "                pathStrand = int(nodeStrand[-1]=='-') # 0 if \"+\", 1 if \"-\"\n",
    "                self.nodePass[pathNode] += 1\n",
    "                self.nodeStrandPaths[pathNode][pathStrand] += 1\n",
    "                self.outPath[pathNode] += 1\n",
    "                self.inPath[pathNode] += 1\n",
    "                if previousNode is not None:\n",
    "                    self.edgePaths.setdefault((previousNode+1,pathNode+1),0)\n",
    "                    self.edgePaths[(previousNode+1,pathNode+1)] += 1\n",
    "                else:\n",
    "                    self.pathStarts[pathNode] += 1\n",
    "                    self.inPath[pathNode] -= 1\n",
    "                previousNode = pathNode\n",
    "            self.outPath[previousNode] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1059': 1, '695': 2, '5571': 3, '4423': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodeNameToID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'-': [(2, '-')]},\n",
       " 4: {'-': [(3, '-')]},\n",
       " 2: {'-': [(4, '-')]},\n",
       " 3: {'-': [(1, '-')]}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1059-', '695-', '4423-'], ['4423-', '5571-', '1059-']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
