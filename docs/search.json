[
  {
    "objectID": "tree.html",
    "href": "tree.html",
    "title": "Tree module",
    "section": "",
    "text": "AmbigouosRootNode\nRaised if more than 1 root node is found in a tree.\n\n\n\nAmbigouosTreeEdge\nRaised if special edge in Tremaux tree cannot be identified as loop or bubble.\n\n\n\nTremauxTree\n\n TremauxTree (graphToTree=None, parentGraph=None, byPath=True, **attr)\n\nThis is the main class for manipulation of special type of Tremaux tree, which is used purely by GenomeGraph class.\nAt the moment, I do not see scenario in which a package user will use this class separately, so, the documentation for its API is low priority."
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Script module",
    "section": "",
    "text": "pantograph_script\n\n pantograph_script ()\n\n\n\n\npaths2graph_script\n\n paths2graph_script (args)\n\n\n\n\nann2graph_script\n\n ann2graph_script (args)\n\n\n\n\nsort_graph_script\n\n sort_graph_script (args)\n\n\n\n\nexport_project_script\n\n export_project_script (args)"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "pygengraph",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitLab under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "dev.html",
    "href": "dev.html",
    "title": "Development notebook",
    "section": "",
    "text": "import os\nimport glob\nimport warnings\n\nimport pdb\n\nimport numpy as np\n\nfrom pygengraph.graph import GenomeGraph\nfrom pygengraph.graph import initialPathAnalysis,calcNodeLengths,getNodesStructurePathNodeInversionRate,pathNodeDirToCombinedArray,getNextNodePath\nfrom pygengraph.graph import convertPathsToGraph\n\nimport networkx as nx"
  },
  {
    "objectID": "dev.html#functions",
    "href": "dev.html#functions",
    "title": "Development notebook",
    "section": "Functions",
    "text": "Functions\n\nConvert GenomeGraph to NetworkX graph (When?)\nIdentify all + -> - points (do we need to do the same for all valid - -> + separately? No! It should be taken care inside inversion blocks, if any left unattended, then they should be precessed separately.)\n\nProcess each such point (do not forget that there can be several paths within onemajor block and all of them should be considered).\n\n\nisolate each reversible block and do the reversion (applied to the original graph or to NetworkX graph?)\n\n\nЕсли при обходе инверсии номер узла увеличивается, то все между предыдущим (или первым) и нынешним добавляются в search terms. Если же номер узла уменьшается, то те номера между предыдущим и нынешним которые уже есть в поисковой группе убираются, а те, которых нет, добавляются.\n\nIdentify loops\n\nFor each loop identify best link to break and remove it from the graph\n\n\nIdentify type of loop and process it accordingly (real loops - recorded and processed, continuous virtual loops - link for pairing, non-continuous link - consider loop ending at the loop link end).\n\nIdentify bubble ends\n\nProcess each bubble\n\nRecord pairs and double pairs if any.\n\n\ndef identifyPotentialInvStarts(graph):\n    '''\n    Identify all + -> - points or - nodes as start nodes in the graph.\n    \n    We do not need to do the same for all - -> + separately. \n    It should be taken care inside inversion blocks, if any left unattended, \n    then they should be precessed separately.\n    \n    The function also identify all graph end nodes (the nodes which do not have any outgoing links)\n    \n    '''\n    graphEnds = []\n    invStartPoints = []\n\n    for fromNode,nodeLinks in graph:\n        if len(nodeLinks)==0:\n            graphEnds.append(fromNode)\n\n        if '-' in nodeLinks:\n            # The node is passed in inverted direction.\n            incomingLinks = graph.backLinks[fromNode]['-']\n            if len(incomingLinks)>0:\n                # There are links coming towards the inverted version of the node.\n                for incomingNode,incomingDir in incomingLinks:\n                    if incomingDir=='+':\n                        # If there is any edge coming to inverted node comes from positive node, then add it\n                        # to the list of potential inversion starts.\n                        invStartPoints.append((incomingNode,fromNode))\n            else:\n                # If there is a inverted node without incoming edge, then it is the start of at least one path\n                # It should also be considered as the potential inverstion start\n                invStartPoints.append(None,fromNode)\n                \n    return invStartPoints,graphEnds\n\n\nConvert GenomeGraph to\n\n\nBlock processing\n\n\nLink processing\n\n\nRearrangement blocks\n\n\nWrapper"
  },
  {
    "objectID": "dev.html#running",
    "href": "dev.html#running",
    "title": "Development notebook",
    "section": "Running",
    "text": "Running\n\nnotebook2script()\n\nConverted 00_init.ipynb.\nConverted 01_graph.ipynb.\nConverted 02_tree.ipynb.\nConverted 03_synteny.ipynb.\nConverted 04_utils.ipynb.\nConverted 05_export.ipynb.\nConverted 05_exportDev.ipynb.\nConverted deBruijnGraphProcessing.ipynb.\nConverted dev.ipynb.\nConverted graphTesting.ipynb.\nConverted index.ipynb.\n\n\n\nsorted(glob.glob(f'{fileDir}{os.path.sep}*.gfa'))\n\n['../../1001G/GraphCollapsing/TestGraphs/inversion_test_01.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_02.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_03.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_04.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_05.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_06.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_07.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_08.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_09.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/inversion_test_10.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_01.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_02.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_03.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_04.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_05.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_06.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_07.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_08.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_09.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_10.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_11.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/repeats_test_12.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/transposition_test_01.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/transposition_test_02.gfa',\n '../../1001G/GraphCollapsing/TestGraphs/transposition_test_03.gfa']\n\n\n\nfilename = 'inversion_test_01.gfa'\ninversionThreshold=0.5\n\n\ngraph = GenomeGraph(gfaPath=f'{fileDir}{os.path.sep}{filename}',isGFASeq=False, doBack=True)\n\nLoading graph from ../../1001G/GraphCollapsing/TestGraphs/inversion_test_01.gfa\nFound node annotation file ../../1001G/GraphCollapsing/TestGraphs/annotation_inversion_test_01.dat, loading associations.\nLoading segment 6/6\nLoading segments finished.\nLoading link 12/12\nLoading links finished\nLoading path 3/3\nLoading paths finished. 3 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 6/6\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 3/3\nFinished preprocessing paths\n\n\n\n\ngraph.paths\n\n[['1+', '2+', '3+', '4-', '5+', '6+'],\n ['1+', '2-', '3+', '4+', '5+', '6+'],\n ['1+', '2+', '3+', '5-', '4+', '6+']]\n\n\n\ninvStartPoints,graphEnds = identifyPotentialInvStarts(graph)\n\n\ngraphEnds\n\n[6]\n\n\n\ninvStartPoints\n\n[(1, 2), (3, 4), (3, 5)]\n\n\nWe follow links by the following rules: - The initiation link is a+ -> b-. We get all nodes between a and b (exclusive) as search nodes. - Node b is added to the inversion stretch list. - Next node is either inverted node (irrespective of current search nodes) on normal node which is in current search nodes list. If there are several options, each option should be considered. Done through a special queue for each step. - () If node c+ is found to be added to the stretch, and it is returning node, i.e. lets call max(search terms) asd and min(search terms) as e: - if c<d and c<b (the link to it goes to the left), then flip (meaning that if a node is in search terms, then remove it, otherwise, add it) in search terms all nodes between c and d (inclusive). - if c>e and c>b (the link to it goes to the right), then flip (meaning that if a node is in search terms, then remove it, otherwise, add it) in search terms all nodes between e and c (inclusive). - If we come through x+ -> y- in the stretch, it should be removed from initiation links list (possibly, each of them should be processed separately, but after that the stretches from initiation links that were used in other stretched should be removed. - If in () all search terms are wiped, then c+ will be the end edge of the stretch and stretch closed. - If there is no more options of either inverted node, or normal node from search nodes linked from current one, then this particular stretch is finished. In this case, the next closest normal node (not in the stretch, i.e. not in min:max+1) linked from current one is the right end of the stretch.\n\nprocessingQueue = []\n\ninversionBlocks = []\nfor invStart in invStartPoints:\n    blockStart = invStart[0]\n    block = []\n    blockEnd = None\n    \n    firstNode = min(invStart)\n    lastNode = max(invStart)\n    searchNodes = set(range(firstNode+1,lastNode))\n    \n    curNode,curDir = invStart[1],'-'\n    \n    while blockEnd is None:\n        nextNodes = graph.forwardLinks[curNode]['curDir']\n        pass\n    \n    \n    print(searchNodes)\n\n[]\n[]\n[4]"
  },
  {
    "objectID": "synteny.html",
    "href": "synteny.html",
    "title": "Synteny module",
    "section": "",
    "text": "readTransMap\n\n readTransMap (transMapFile, ATaccessionName='araport',\n               similarityGroupColName='Orthogroup')\n\nRead trandmap file which is pretty much a tsv file with first column containing orthogroup, or similarity group ID, and the rest of columns correspond to each accession with unique gene/interval IDs in each columns for the given orthogroup. If one accession has several genes in the given orthogroup, then they will be in a comma separated list.\n\n\n\ngenerateOrder\n\n generateOrder (files, priorityAccession='TIAR10')\n\nGenrate a list of files and float a file which contains specific priority accession name (or any given string) on top of the list.\nIt is not used any more, but left if will be needed in the future.\n\n\n\ngetIDs\n\n getIDs (iterator)\n\n\n\n\nreadGFF\n\n readGFF (gffFile:str)\n\nFunction which reads GFF3 file into a dict structure. For large GFF files random search is extremely slow and is impossible to use. This greatly speed up the process (from hours to seconds).\n\n\n\naddPangenomePositions\n\n addPangenomePositions (pangenomeFiles)\n\n\n\n\nprocessAccessions\n\n processAccessions (annotationFiles, ATmap=None, pangenomeDict=None,\n                    similarityIDKey=None, similarityIDAssignment=None,\n                    pangenomeFiles=None, sequenceFilesDict=None,\n                    seqidJoinSym='_', ATsplitSym=',')\n\nThis function process only custom annotation for a group of accession with similarity IDs. It also creates separate ATmap and returns the last number of unmatchedID for consistency. There is a separate function for processing reference annotation.\nsequenceFilesDict can either be None or dict with accession IDs as keys and paths to FASTA files with sequences of annotated elements as values.\npangenomeFiles: list[str] or None. If list of strings is provided, then its length should be the same length as annotationFiles and should be in the same order as in annotationFiles, i.e. pangenomeFiles[i] should correspond to annotationFiles[i]\nsimilarityIDAssignment: str. Can be either ‘gene’ or ‘mRNA’. If it is not any of these an error will be raised.\n\n\n\nrecordSegment\n\n recordSegment (name, segmentIDs, segmentIDToNumDict, sequence=None,\n                gfaFile=None, segmentData=None)\n\n\n\n\nrecordAnnotation\n\n recordAnnotation (nodeID, accessionID, sequenceID, chrID, start, end, og,\n                   atList, sequence, nodesMetadata, pstart=-1, pend=-1)\n\n\n\n\nrecordAltChr\n\n recordAltChr (nodeID, accessionID, chrID, start, end, nodesMetadata)\n\n\n\n\naddLink\n\n addLink (links, prevPathSegment, name, forward)\n\nlinks: mutable prevPathSegment: mutable\n\n\n\ngeneratePathsLinks\n\n generatePathsLinks (genesAll, ATmap, accessionID, sequences, OGList,\n                     segmentIDs, nodesMetadata, segmentIDToNumDict, links,\n                     usCounter, chromosomeID=None, doUS=True,\n                     segmentData=None, gfaFile=None)\n\nThis function takes a list of genes in specific format (genesAll) and some extra data and pretty much generates a graph (gene graph from annotations).\ngfaFile: file handle to write segments to GFA file OGList: mutable links: mutable usCounter: mutable\n\n\n\nreadSegmentIDs\n\n readSegmentIDs (path)\n\n\n\n\nwriteSegmentIDs\n\n writeSegmentIDs (path, segmentIDs)\n\n\n\n\nwritePath\n\n writePath (gfaFile, AccessionID, path, cigar, doCigars)\n\n\n\n\nwriteLinks\n\n writeLinks (gfaFile, links, doCigars=True)"
  },
  {
    "objectID": "graph.html#path-conversion-utility-functions",
    "href": "graph.html#path-conversion-utility-functions",
    "title": "Graph module",
    "section": "Path conversion utility functions",
    "text": "Path conversion utility functions\nThese functions are utility, but because they are directly related to graph structures, they are places in this module and not in Util module\n\n\ncalcNodeLengths\n\n calcNodeLengths (graph)\n\nSimple function that calculates node lengths (in visual columns).\nIf it is nucleotide graph, it will actually calculate a number of nucleotides in each node, but if it is non-nucleotide graph, then it will return 1 for each node.\n/home/pigrenok/.pyenv/versions/3.10.9/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Return\n  else: warn(msg)\n/home/pigrenok/.pyenv/versions/3.10.9/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Example\n  else: warn(msg)\n\n\n\ninitialPathAnalysis\n\n initialPathAnalysis (graph, nodeLengths)\n\nThis function creates auxiliary data structures to make it easier to work with paths and their relationships with nodes.\n\n\n\ngetNodesStructurePathNodeInversionRate\n\n getNodesStructurePathNodeInversionRate (pathNodeArray, pathDirArray,\n                                         pathLengths,\n                                         inversionThreshold=0.5)\n\nGenerate a dict of dicts which stores information about inversion rate of each node for each path.\n\n\n\nconvertPathsToGraph\n\n convertPathsToGraph (fullPath, doSorting=False, v2=False)\n\n\n\n\ngetPathNodeInversionRate\n\n getPathNodeInversionRate (pathNodeArray, pathDirArray, pathLengths,\n                           inversionThreshold=0.5)\n\ndeprecated Generate a dict of dicts which stores information about inversion rate of each node for each path.\n/home/pigrenok/.pyenv/versions/3.10.9/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\n\n\npathNodeDirToCombinedArray\n\n pathNodeDirToCombinedArray (pathNodeArray, pathDirArray)\n\nCombines path node and direction arrays (provided by graph.initialPathAnalysis function as pathNodeArray and pathDirArray (output indices 1 and 3)).\n\n\n\ngetNextNodePath\n\n getNextNodePath (pathNodeArray, pathLengths)\n\nWhen following path to find next in graph order, it will not necessarily be 𝑘+1 , it can be 𝑘+𝑝 if 𝑘+1,…,𝑘+𝑝−1 are not passed by the path.\nCreate a list of all unique node numbers in each path by - Either set(path) - *** or np.unique(path) Preferable option would be selected according to the selection of the options for next step\nAnd then do one of the following - Just leave the list as it is and in a loop check if 𝑘+𝑝 (with 𝑝=1,… ) is in the path (k+p in pathUnique) Probably slow - Sort the list pathUnique and for every node where we need to find next in order just find its index and take the next one pathUnique[pathUnique.index(k)+1] - Create a dict for each node (except the last one) with key as 𝑘 and value as 𝑘+𝑝 . On break in path at position 𝑘 we get the next as dict[k] - *** Do np.diff(np.sort(pathUnique)) and create a dict for each node after which diff!=1 with key as 𝑘 and value as 𝑘+𝑝 . Then when we get to the break in path at position k. We check k in dict and if True, then the next is dict[k], otherwise it is k+1\nThis function implements generating the dict as described in *** (triple starred) options, which are currently used options."
  },
  {
    "objectID": "graph.html#assistant-class-implementing-link-getting-mechanism.",
    "href": "graph.html#assistant-class-implementing-link-getting-mechanism.",
    "title": "Graph module",
    "section": "Assistant class implementing link getting mechanism.",
    "text": "Assistant class implementing link getting mechanism.\n\n\nLinkGetter\n\n LinkGetter (nodes, links)\n\nThis auxiliary class allows creating virtual subscribable structure inside the main GenomeGraph class to access links as Iterable and Subscribable object. Full links (with directionality) are available in the main class. This class provides simplified view on the links without directionality (the fact that a link goes from node A to node B)."
  },
  {
    "objectID": "graph.html#main-class-holding-all-data-and-main-methods-operating-the-graph",
    "href": "graph.html#main-class-holding-all-data-and-main-methods-operating-the-graph",
    "title": "Graph module",
    "section": "Main class holding all data and main methods operating the graph",
    "text": "Main class holding all data and main methods operating the graph\n\nGraph definition, constructor and some utils for constructor\n\n\n\nGenomeGraph\n\n GenomeGraph (gfaPath=None, doOverlapCleaning=True, paths=None,\n              nodes=None, nodesData=None, links=None, pathsDict=None,\n              sequenceFilesDict=None, annotationFiles=None,\n              pangenomeFiles=None, doBack=False, verbose=True, **kwargs)\n\nThis is a constructor of a class GenomeGraph. This class allows to hold not only vanila genome graph, but also a lot of extra information and also manipulate graphs in multiple ways, including sorting graphs, adding and deleting nodes and links, and manipulating metadata etc.\nAt the moment, there are four ways an instance can be created. It depends on what parameters are passed to the constructor. If parameters for more than 1 option is passed, there is a priority queue which constructor follows. Each option and its priorities are provided below.\nPriority 1: If you pass gfaPath as actual path to gfa file, then it will be loaded as is. In this case, the following options are available: accessionsToRemove: list or None (default). If not None, a list of strings, if any of the string contains in pathname, the path will be ignored while loading. isGFASeq: boolean (default: True). Whether the graph should be considered as sequence graph (True) or as gene/block graph (False).\nPriority 2: If nodes, links and paths are provided (not None), they should be as following: nodes: list[str]: a list of strings with node IDs (unique)\n`links`: dict{int:dict{str:list[tuple(int,str)]}}: a dict with keys integers with 1-basednode numbers \n(in the order as in self.nodes) from which the link starts. Value is a dict with key of directionality of\nthe from node ('+' for normal direction or '-' for inverse). Value is a list of tuples with two values:\nfirst integer is 1-based node number of node to which the link is going and second string is '+' or '-'\nrepresenting the directionality in which this node is represented in this link.\n\n'paths': list[list[str]]: List, which contains a list for each accession/path, which is represented by\na list of strings, each of which has a format '{1-based node number}{directionality}', where \n{1-based node number} is an integer 1-based number of node using the order as in `nodes`, \n{directionality} is either '+' for normal direction, or '-' for inverted.\nPriority 3: if pathsDict is provided then the graph is created from the paths for multiple accessions. pathsDict is a dict{int:list[str]}; keys are names of accessions, and values are lists of strings of the following format ‘{node name}{directionality}’, where {node name} is identifiable unique name which identifies the node, {directionality} is either ‘+’ for normal direction, or ‘-’ for inverted. Note, that this can create no-sequence graph only (e.g. gene graph). Sequences can be added later on through adding sequences to GenomeGraph.nodesData list. An extra optional parameter is: nodeNameLengths: list[int] or None, a list of alternative node lengths. By default, each node will be represented as a single cell/column, but if provided, variable length can be provided. Priority 4: If annotationFiles is not None, but is a list of paths to annotation (gff3) files, then the following extra options are available: ‘sequenceFilesDict’: a dict{str:str}, where keys are IDs of accessions used in annotation files and value is a path to FASTA file (relative to gff files). Assumption is that FASTA sequence names are the same as GFF3 sequence names. pangenomeFiles: a list[str], a list of GFF3 files for the same intervals as in annotationFiles, ID fields in GFF2 metadata should be the same. accOrder: list or None (default), Order of accessions in the graph. If None, accessions will be sorted in alphabetical order. chromosome: str or None, if None, create one graph for all chromosomes (not fully implemented, see manual), otherwise, create only one graph for given chromosome. doUS: boolean (default: False) Add unrelated sequence blocks between annotated genes/blocks. refAnnotationFile: str. If given, it has to be a path to gff3 file with reference annotation with reference notation for gene names. In main annotations reference gene names should be identified either in “gene” records under “AT” key (prioritised), or in “mRNA” records under “Name” key (fallback). If ATMap is provided, then refSequenceFile: str or None (default). If provided with path, then it will be used to obtain sequences of each block/gene. transmapFile: a tab delimited file with column “Orthogroup”, which contains similarity ID for genes and a column with name given by transmapFileRefCol which contains reference annotation gene names. transmapFileRefCol: str or None, a name of column for reference gene names in transmapFile refAccession: str or None (default). Accession ID for reference annotation (should be provided if refAnnotationFile if provided).\n\n\nImport graph from file\n\nFrom GFA\n\n\nFrom Paths\n\n\nFrom annotations\n\n\n\nImport or create annotation\n\nFrom annotation files\n\n\n\n\nGenomeGraph.loadAnnotations\n\n GenomeGraph.loadAnnotations (annotationPath, seqSuffix)\n\nThis function should allow adding interval metadata (annotations) to sequence (nucleotide) graph. It has never been properly tested.\n\nCreate annotation from nodes (artificial annotation)\n\n\n\n\nGenomeGraph.updateAnnotationFromNodes\n\n GenomeGraph.updateAnnotationFromNodes (isSeq=True)\n\nThis function is used only for primitive block graphs (e.g. gene and chain graphs) if there is no proper annotation available (e.g. graph was created from paths and some extra information about nodes is needed).\nIt takes “name” of each node either from graph.nodes (if isSeq is False) or from graph.nodesData (if isSeq is True).\nParameters ##########\nisSeq: Whether it contains names as names or as seq.\n\n\nGraph sorting\n\n\n\nGenomeGraph.generateTremauxTree\n\n GenomeGraph.generateTremauxTree (byPath=True)\n\nThis function generates Tremaux tree for our graph. It is not a simple Tremaux tree and requires an adjustment process, which happens inside the TremauxTree class constructor.\n\n\n\nGenomeGraph.treeSort\n\n GenomeGraph.treeSort (byPath=True, bubblePriorityThreshold=0.5)\n\nThis is the main function for sorting graph. It requires some further work, but works relatively well as is.\n\n\nExport/Save (to GFA)\n\n\n\nGenomeGraph.toGFA\n\n GenomeGraph.toGFA (gfaFile, doSeq=True)\n\nRecording existing graph structures to GFA v1 file + some json and joblib files to preserve extra metadata.\n\n\nElements operations (for nodes, links, annotations, etc.)\n\nAdd node, link and accession (not properly implemented or not tested)\nHere another function is needed to add accession with relevant provate nodes and links. Then possibly the functions below will be used but as private function, not external API.\n\n\n\n\nGenomeGraph.addAccessionAnnotation\n\n GenomeGraph.addAccessionAnnotation (annotationFile, sequenceFile=None)\n\nIdeally, a function should be able to add one accesstion to existing graph. When implemented, _graphFromAnnotation should be using this function.\n\n\n\nGenomeGraph.addLink\n\n GenomeGraph.addLink (fromNode, fromStrand, toNode, toStrand)\n\nNeed testing. Not sure if it actually makes sense as links not present in any of the paths does not play any role.\n\n\n\nGenomeGraph.addNode\n\n GenomeGraph.addNode (nodeID, data=None)\n\nNeed testing. Again, there is no point of adding a node to a graph if this node will not be present in any of the paths.\n\nNode Inversion\n\n\n\n\nGenomeGraph.invertNodes\n\n GenomeGraph.invertNodes ()\n\nThis function look at inversion/strand of each node in each path. If more than half of paths passing node in “inverted” direction, then inverstion should be switched over (currently inverted passes should become normal and normal passes should become inverted.) It is done every time a graph is loaded. Possibly, it should be possible to not doing it as it will take a lot of time for larger graphs.\n\nNode removal methods\n\n\n\n\nGenomeGraph.removeNodes\n\n GenomeGraph.removeNodes (nodeIDsToRemove)\n\nThis (and related to it) function allows removal of a node from a graph. In normal situation, it should not be done to a graph as it will make it invalid in most cases, it is very important functionality for removal of overlaps (see below).\n\nNode Substitution (not implemented)\n\n\nNode overlap removal\n\n\n\n\nGenomeGraph.removeOverlaps\n\n GenomeGraph.removeOverlaps ()\n\nWhen the graph (nucleotide only) is loaded, overlaps are allowed and can be provided using CIGAR strings (it will not be checked). Such overlaps can appear for instance when a compacted de bruijn graph is used (e.g. generated by CUTTLEFISH). They should be removed in order to make the graph not artificially overcomplicated.\nUnfortunately, this current implementation is not working properly and needs to be looked at in details. It is most probably overcomplicated and overthought. It should be relatively easy to do.\n\n\nUtility methods\n\nLink conversion methods\nThere are two different types of data sructure for links: based on sets and dicts. Normally, dict type is used, but for some specific operation sets type is needed. The following two function does the convertion between the two.\n\n\nClass enumerator (nodes) and subscription (links)"
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "Export module",
    "section": "",
    "text": "warnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "export.html#functions-intro",
    "href": "export.html#functions-intro",
    "title": "Export module",
    "section": "Functions intro",
    "text": "Functions intro\nNotation and terminology\nIn documentation, we refer to graph nucleotides, columns and components. Components contain columns and columns contain nucleotides.\nIn the code variable names and comments use slightly different notation. Columns in documentation are bins in code and comments, whereas graph nucleotides in documentation are called columns in the code and comments. This happened for the legacy reasons, i.e. originally there was no nucleotide numbers (columns) in the visualised graph structure and components were split into bins (literally, equal sized bins). It is not true anymore, but old terminology left here.\nIdeally all variable names and comments should be changes in line with documentation notation, but I have no idea when this can happen.\nFor various operational or legacy reasons, some of the data structures (usually, lists/array) use 0-based indexing, whereas some others (usually dicts) can be 0-based or 1-based. Here are the main structures with numerical indexing and their index bases:\n\ncomponents: keys: 0-based, values: occupants: 0-based, binNumbers: 0-based\n\ncomponentToNode: keys: 0-based, values: 1-based\n\nnodeToComponent: keys: 0-based, values: 1-based\n\nnewToOldInd and oldToNewInd: both index and values are 0-based numbers of components in previous and current zoomlayer.\n\nfromLinks: top level keys (from nodes): 1-based, bottom level keys (to nodes): 1-based, values (list of participants): 0-based\n\ntoLinks: top level keys (to nodes): 1-based, bottom level keys (from nodes): 1-based, values (list of participants): 0-based\n\nfromComponentLinks: top level keys (from components): 1-based, bottom level keys (to components): 1-based, values (set of participants): 0-based\n\ntoComponentLinks: top level keys (to components): 1-based, bottom level keys (from components): 1-based, values (set of participants): 0-based"
  },
  {
    "objectID": "export.html#generating-base-layer",
    "href": "export.html#generating-base-layer",
    "title": "Export module",
    "section": "Generating base layer",
    "text": "Generating base layer\nThis set of functions generate the data structures for initial, lowest level zoom (nucleotide or minimum unit resolution). The main orchestration function is baseLayerZoom.\n\nFunctions\n\n\n\noutLeftRight\n\n outLeftRight (nodeInversionInPath, leftFarLink, rightFarLink, reason,\n               debug=False, inversionThreshold=0.5)\n\n\n\n\nrecordLinks\n\n recordLinks (nodeIdx, nextNode, pathID, step, nodeInversionInPath,\n              nonLinearCond, pathNodeArray, fromLinks, toLinks,\n              debug=False, inversionThreshold=0.5)\n\n\n\n\ncheckForBreak\n\n checkForBreak (nodeIdx, nodeLen, nodePathsIdx, nodeSeqInPath,\n                uniqueNodePathsIDs, pathNodeCount, pathLengths,\n                pathNodeArray, pathDirArray, occupancy, inversion,\n                fromLinks, toLinks, nBins, maxLengthComponent, blockEdges,\n                inversionThreshold=0.5, debug=False)\n\nFunction to check whether the component should be broken before (left) and/or after (right) it.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnodeIdx\n\n\n\n\n\nnodeLen\n\n\n\n\n\nnodePathsIdx\n\n\n\n\n\nnodeSeqInPath\n\n\n\n\n\nuniqueNodePathsIDs\n\n\n\n\n\npathNodeCount\n\n\n\n\n\npathLengths\n\n\n\n\n\npathNodeArray\n\n\n\n\n\npathDirArray\n\n\n\n\n\noccupancy\n\n\n\n\n\ninversion\n\n\n\n\n\nfromLinks\n\n\n\n\n\ntoLinks\n\n\n\n\n\nnBins\n\n\n\n\n\nmaxLengthComponent\n\n\n\n\n\nblockEdges\n\n\n\n\n\ninversionThreshold\nfloat\n0.5\n\n\n\ndebug\nbool\nFalse\n\n\n\nReturns\nleftFarLink: bool. Shows whether there is a far link on the left that will require component break.\n\n\n\n\n\n/home/pigrenok/.pyenv/versions/3.10.9/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Return\n  else: warn(msg)\n\n\n\nnodeStat\n\n nodeStat (nodeIdx, pathNodeArray, nodeLengths)\n\nFunction calculate information about node as part of the overall graph.\n\n\n\nfinaliseComponentBase\n\n finaliseComponentBase (component, components, componentNucleotides,\n                        matrix, occupants, nBins, componentLengths,\n                        nucleotides, zoomLevel, accessions,\n                        inversionThreshold=0.5)\n\n\n\n\nprocessAnnotationInterval\n\n processAnnotationInterval (posStart, posEnd, annotation, res)\n\n\n\n\ncombineAnnotation\n\n combineAnnotation (combAnnotation)\n\n\n\n\nupdateEdges\n\n updateEdges (accEdge, edgeAccessions, compNum)\n\nFunction fills up either accStarts or accEnds (on which component each accession starts and on which ends). compNum is assumed to be 1-based.\n\n\nWrapper\nNow ‘positions’ key in metadata contains either one position (chr:posStart..posEnd) or two comma separated positions where one is genomic position, and another one is pangenomic position.\n\n\n\nbaseLayerZoom\n\n baseLayerZoom (graph, outputPath, outputName, pathNodeArray,\n                pathDirArray, pathLengths, nodeLengths,\n                pathNodeLengthsCum, maxLengthComponent, blockEdges,\n                CPUS=32, inversionThreshold=0.5, isSeq=True, debug=False,\n                debugTime=False)"
  },
  {
    "objectID": "export.html#transfer-from-nodes-to-components-links-and-other-structures",
    "href": "export.html#transfer-from-nodes-to-components-links-and-other-structures",
    "title": "Export module",
    "section": "Transfer from nodes to components (links and other structures)",
    "text": "Transfer from nodes to components (links and other structures)\nThis is one of the first processes happening while exporting graph. While graph operates with nodes (which can be linearly connected with each other in all paths), then exporting works with components. In almost all cases, components have at least some non-linear links with other components on both sides. The only exclusion is when a component is too large and split into several ones. In this case two components will be connected by 100% linear links. Also, graph operates with paths along with nodes, whereas exporting works with components and accession-specific links between them.\nThese functions (with main orchestrating one is nodeToComponentLinks) are converting nodes and paths to components and links.\n\n\nsplitforwardInversedNodeComp\n\n splitforwardInversedNodeComp (pathList, component, isInverse)\n\n\n\n\nfillLinksBase\n\n fillLinksBase (nodeInComp, nodeToComponent, fromLinks, toLinks,\n                fromComponentLinks, toComponentLinks, compNum, components,\n                doLeft=True, doRight=True)\n\n\n\n\nconvertLink\n\n convertLink (linkFrom, linkTo, translateDict, forwardLinks, isZoom)\n\n\n\n\nrecordUpdatedPairedLink\n\n recordUpdatedPairedLink (firstLinkSet, secondLinkSet, firstLink,\n                          secondLink, substituteLink, pairedLinksConv)\n\n\n\n\nconvertRemovableComponents\n\n convertRemovableComponents (translateDict, linkLengths, pairedLinks,\n                             interconnectedLinks, blockEdges,\n                             forwardLinks, isZoom=True)\n\ntranslateDict should be a dict in format {<old node/component id 0-based>:<new component id 1-based>} pathNodeInv should be a dict of dicts of the following structure: {:{<nodeId 1-based>:}}\nThis is done through fromLinks and toLinks and throught associated directions of available accessions. For this we need to loop through strands and do it separately for each strand.\nFor paired links there is a possibility that a single node link will give several component links. In this case, the cross product of all first and second links will be added to converted paired links.\n❗The substitute links should be added only to the paths that contained both first and second links in the first place. This should be controlled in link removal routine.\n\n\n\nnodeToComponentLinks\n\n nodeToComponentLinks (components, componentToNode, nodeToComponent,\n                       fromLinks, toLinks, graph, fromComponentLinks,\n                       toComponentLinks, linkLengths=None,\n                       pairedLinks=None, interconnectedLinks=None,\n                       blockEdges=None, debug=False)"
  },
  {
    "objectID": "export.html#identifying-collapsible-links-and-rearrangement-blocks-works-incorrectly-left-now-for-compatibility.",
    "href": "export.html#identifying-collapsible-links-and-rearrangement-blocks-works-incorrectly-left-now-for-compatibility.",
    "title": "Export module",
    "section": "Identifying collapsible links and rearrangement blocks (works incorrectly, left now for compatibility).",
    "text": "Identifying collapsible links and rearrangement blocks (works incorrectly, left now for compatibility).\nIn order to be able to generate multiple zoom levels of the graph view, non-linear links describing small (too small to show at the given zoom level) rearrangements should disappear whereas links describing larger blocks should persist. This will allow to see larger rearrangements clearly on higher zoom levels.\nIn order to do it, each link should be associated with some size (or rearrangement), so, that when each zoom level is generated, they can be removed when the rearrangement cannot be shown at the given zoom level.\nSome links are also associated with each other, and when they are removed new links (usually linear ones) should be reinstated to make larger rearrangements clearer.\nAt the moment, the process of identifying these sizes is not working great as it leaves too much non-linear links to the very top level where suddenly all non-linear links disappear and the whole graph from over-complicated jumps to pretty much trivial without any rearrangements. If to use digital map analogy, most of country roads persist while you zoom out on the map until almost the whole Earth is in view and then at some point the view becomes just a blue/green ball with very rough boundary of continents and oceans.\nAt the moment, all associated links get into a pool of so called interconnected links and if one link gets associated with specific size, then all links get the same association, and then maximum size is selected. But that means that if one link describes one small and also on the edge of large rearrangement, and another link is only associated with large rearrangement, then the latter link will also be associated with the size of large rearrangement and will stay until the zoom level where the large rearrangement is too small to show. That is incorrect.\nI think, each link should get its own associations with sizes (and maximum should be taken) and clearing of the link should happen individually. Yet, if one link with smaller size and one with larger size are paired, the reinstated link should appear after smaller link removed.\nAnother alternative is just to get contiguous blocks in each path and associate each link pair (describing start and end of each block) as a pair of links that needs to be cleared in association with the size of this block. Need control of repeats in these blocks. If it happens, then a single link can describe a whole rearrangement. In addition, an extra control for inversion is also needed. In particular, if outside the block the numbers do not create a range to fin the inverted node (e.g. 1+,4-,3+, or 3+,2-,5+), then it should be ignorred for this step. It means there is a smaller rearrangement within larger one.\nAnother alternative (described in TODO) is to convert paths of nodes to paths of edges and operate with them. I guess, it is not far away from the previous paragraph.\n\nIdentifying path breaks\n\n\n\nfindBreaksInPath\n\n findBreaksInPath (combinedArray, nextNodeDict)\n\n\n\n\nidentifyPathBreaks\n\n identifyPathBreaks (combinedNodeDirArray, pathLengths, pathNextNode)\n\n\n\nBlock processing\n\n\n\ninterweaveArrays\n\n interweaveArrays (a, b)\n\n\n\n\nextractGapsBlocks\n\n extractGapsBlocks (block, path, nodeLengths, getComplex=False)\n\nThis function either split block by gaps (e.g. block [1,2,4,5,6,8] will yield [1,2],[4,5,6],[8])\nIf getComplex is set to True, then first gaps are filtered for nodes that are not passed by the path. After that, edges are identified and then for them nodes not passed by the path are filtered out. Then we find the longest block out of edges, and then the longest edge combine with all gaps and find the shortest one. That shortest one is going to be the one returned.\nE.g. block [1,2,4,5,8] will give edges [1,2],[4,5],[8] and gaps [3],[6,7].\nIf path does not contain 6, then edges will be the same, but gaps will be [3],[6]\nIf path does not contain 3, then edges will be [1,2,4,5],[8] and gaps [6,7]\nThe exact block which will be returned depends on sizes of each node.\n\n\n\ncheckSplitBlock\n\n checkSplitBlock (block, gapList=None)\n\nNot used at the moment\nFunction checks if the block has any gaps and split into a list of blocks between gaps (alternatively fill gaps or leave things as they are). At the moment the gapped block will be converted to list of blocks between gaps\n\n\n\nblockListToLengths\n\n blockListToLengths (blockList, nodeLengths)\n\n\n\n\nconvertBlocksToLengths\n\n convertBlocksToLengths (linksBlocks, nodeLengths)\n\nConverting blocks associated with each link to lengths and then selecting the longest one (?)\n\n\nLink processing\n\n\n\naddToLinkPool\n\n addToLinkPool (link1, link2, interconnectedLinks)\n\n\n\n\nblockFromSingleLink\n\n blockFromSingleLink (pathID, link, pathNodeInversionRate, pathNextNode)\n\nIdentify block from a single link It is the block that the link bounds, i.e.: If link if forward then it is inside the link + any side that is inverted If link is backward, then it is inside + any side that is normal direction.\n\n\n\ncheckIndividualLink\n\n checkIndividualLink (link, pathID, usedSecondInPairLink)\n\nFunction checks if this link is already second in pair. If it is, then it is not considered separately (return True?). Otherwise, it should be considered and block generated (using blockFromSingleLink) and associated with this link.\n\n\n\nprocessDoublePairedLinks\n\n processDoublePairedLinks (leftLink, rightLink, pathID, doublePairedLinks,\n                           pairedLinks, interconnectedLinks, linksBlocks,\n                           pathNextNode)\n\n\n\n\nprocessIndividualLink\n\n processIndividualLink (link, pathID, pathNodeInversionRate, pathNextNode,\n                        usedSecondInPairLink)\n\n\n\n\nrecordLinkBlockAssociation\n\n recordLinkBlockAssociation (link, blockList, linksBlocks)\n\n\n\n\nfindNextNode\n\n findNextNode (node, combinedArray)\n\n\n\n\nprocessPseudoPair\n\n processPseudoPair (breakPos, returnPos, pathID, pathNodeArray,\n                    combinedNodeDirArray, pathNextNode, nodeLengths,\n                    usedSecondInPairPath, pairedLinks, linksBlocks)\n\n\n\n\nprocessStartsEnds\n\n processStartsEnds (mainLink, linkStarts, linkEnds, interconnectedLinks,\n                    forwardLinks)\n\nCurrently not in use.\nTODO!!! Need to add checks for whether one link is intersecting the other or one is fully inside.\n\n\n\npostprocessLinksBlocks\n\n postprocessLinksBlocks (linksBlocks, interconnectedLinks)\n\n\n\n\nprocessPathBreaks\n\n processPathBreaks (pathBreakCoordPairs, pathNodeArray, pathNextNode,\n                    combinedNodeDirArray, pathNodeInversionRate,\n                    pathLengths, nodeLengths, forwardLinks)\n\n\n\nRearrangement blocks\n\n\n\naddBlockEdge\n\n addBlockEdge (edge, size, blockEdges)\n\n\n\n\nidentifyRearrangementBlocks\n\n identifyRearrangementBlocks (nodesStructure, nodeLengths)\n\nblock Edges is a dict with a structure: :  pointing to the node before (!) the break. In other words, if it is the start of the block, it will point to the node just before the block, and if it is the end of the block, it will point to the last node of the block.\n\n\nWrapper\n\n\n\ngetRemovableStructures\n\n getRemovableStructures (graph=None, nodeLengths=None, pathLengths=None,\n                         pathNodeArray=None, pathDirArray=None,\n                         pathNextNode=None, forwardLinks=None,\n                         inversionThreshold=0.5)\n\n\n\n\ngetBlockEdges\n\n getBlockEdges (graph=None, nodeLengths=None, pathLengths=None,\n                pathNodeArray=None, pathDirArray=None, pathNextNode=None,\n                forwardLinks=None, inversionThreshold=0.5)"
  },
  {
    "objectID": "export.html#generating-zoom-layer",
    "href": "export.html#generating-zoom-layer",
    "title": "Export module",
    "section": "Generating zoom layer",
    "text": "Generating zoom layer\nThis set of functions (with nextLayerZoom being main orchestration function) doing the job of generating next zoom level by collapsing columns and then components together after smaller non-linear links are removed (by different set of functions).\n\nFinalising bin and component\n\n\n\naddLink\n\n addLink (fromComp, fromStrand, toComp, toStrand, pathList,\n          fromComponentLinks, toComponentLinks)\n\n\ndef getOccInvChange(binColLengths,binBlockLength,binOcc,binInv,prevOcc,prevInv,inversionThreshold=0.5):\n    occChanged = False\n    invChanged = False\n    occ = {}\n    inv = {}\n    \n    for pathID in binOcc:\n        \n        # Averaging occupancy\n        occ[pathID] = sum([bl*bo for bl,bo in zip(binColLengths,binOcc[pathID])])/binBlockLength\n        # Do comparison through floor and then abs difference > 0\n        if np.abs(np.floor(occ[pathID]+0.5)-np.floor(prevOcc.get(pathID,occ[pathID])+0.5))>0 \\\n            and occ[pathID]>0.5 and prevOcc.get(pathID,occ[pathID])>0.5:\n            occChanged = True\n        prevOcc[pathID] = occ[pathID]\n        \n        # Averaging invertion\n        inv[pathID] = sum([bl*bo*bi for bl,bo,bi in zip(binColLengths,binOcc[pathID],binInv[pathID])])/(binBlockLength*occ[pathID])\n        if (inv[pathID]-inversionThreshold)*(prevInv.get(pathID,inv[pathID])-inversionThreshold)<0 or \\\n        (inv[pathID]-inversionThreshold)*(prevInv.get(pathID,inv[pathID])-inversionThreshold)==0 and \\ \n        inv[pathID]*prevInv.get(pathID,inv[pathID])>inversionThreshold*inversionThreshold:\n            # The second comdition after `or` is taking the case where one is equal to inversionThreshold\n            # and another is more than inversionThreshold.\n            invChanged = True\n        prevInv[pathID] = inv[pathID]\n        \n    return occChanged,invChanged,occ,inv,prevOcc,prevInv\n\n\n\n\ngetOccInv\n\n getOccInv (binColLengths, binBlockLength, binOcc, binInv,\n            inversionThreshold=0.5)\n\n\n\n\ncombineIntervals\n\n combineIntervals (posPath)\n\n\n\n\nrecordBinZoom\n\n recordBinZoom (occ, inv, binPosArray, nBins, nCols, binBlockLength,\n                binBlockLengths, binColLengths, binColStart, binColStarts,\n                binColEnd, binColEnds, matrix, inversionThreshold=0.5)\n\n\n\n\ngetAverageInv\n\n getAverageInv (binBlockLengths, matrixPathArray)\n\n\n\n\nfinaliseComponentZoom\n\n finaliseComponentZoom (component, components, componentLengths, nBins,\n                        nCols, occupants, binBlockLengths, binColStarts,\n                        binColEnds, matrix, starts, ends, forwardPaths,\n                        invertedPaths, compInvNum, compInvDen,\n                        inversionThreshold=0.5)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncomponent\n\n\n\n\n\ncomponents\n\n\n\n\n\ncomponentLengths\n\n\ncomponentNucleotides,\n\n\nnBins\n\n\n\n\n\nnCols\n\n\n\n\n\noccupants\n\n\n\n\n\nbinBlockLengths\n\n\n\n\n\nbinColStarts\n\n\n\n\n\nbinColEnds\n\n\n\n\n\nmatrix\n\n\n\n\n\nstarts\n\n\n\n\n\nends\n\n\n\n\n\nforwardPaths\n\n\n\n\n\ninvertedPaths\n\n\n\n\n\ncompInvNum\n\n\n\n\n\ncompInvDen\n\n\n\n\n\ninversionThreshold\nfloat\n0.5\n\n\n\n\n\n\n\nfinaliseBinZoom\n\n finaliseBinZoom (compNum, binOcc, binInv, binPosArray, nBins, nCols,\n                  binBlockLength, binBlockLengths, binColLengths,\n                  binColStart, binColStarts, binColEnd, binColEnds,\n                  matrix, newComponent, newComponents,\n                  newComponentLengths, newFromComponentLinks,\n                  newToComponentLinks, occupants, linkLengths, starts,\n                  ends, forwardPaths, invertedPaths, pathsToInversion,\n                  newToOldInd, oldToNewInd, inversionThreshold=0.5)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncompNum\n\n\n\n\n\nbinOcc\n\n\n\n\n\nbinInv\n\n\n\n\n\nbinPosArray\n\n\n\n\n\nnBins\n\n\n\n\n\nnCols\n\n\n\n\n\nbinBlockLength\n\n\n\n\n\nbinBlockLengths\n\n\n\n\n\nbinColLengths\n\n\n\n\n\nbinColStart\n\n\n\n\n\nbinColStarts\n\n\n\n\n\nbinColEnd\n\n\n\n\n\nbinColEnds\n\n\n\n\n\nmatrix\n\n\n\n\n\nnewComponent\n\n\n\n\n\nnewComponents\n\n\n\n\n\nnewComponentLengths\n\n\ncompAccDir,#newComponentNucleotides,\n\n\nnewFromComponentLinks\n\n\n\n\n\nnewToComponentLinks\n\n\n\n\n\noccupants\n\n\n\n\n\nlinkLengths\n\n\n\n\n\nstarts\n\n\n\n\n\nends\n\n\n\n\n\nforwardPaths\n\n\n\n\n\ninvertedPaths\n\n\n\n\n\npathsToInversion\n\n\n\n\n\nnewToOldInd\n\n\n\n\n\noldToNewInd\n\n\n\n\n\ninversionThreshold\nfloat\n0.5\n\n\n\n\n\n\nBreak component?\n\n\n\ngetMatrixPathElement\n\n getMatrixPathElement (matrix, pathID)\n\n\n\n\ncheckChange\n\n checkChange (compNum, components, zoomLevel, blockEdges)\n\n\n\n\njoinComponents\n\n joinComponents (leftComp, rightComp, maxLengthComponent,\n                 inversionThreshold=0.5)\n\n!!! ⚠️ Currently not used\nIf the joining was successful, the function will return a joined component.\nIf the joining was not successful and was aborted for one of the following reasons, it will return a list of original components. The reasons for aborting the joining can be the following: - In one of the paths the invertion is lower than threshold in one component and higher in the other. - Left component contains at least one end - Right component contains at least one start\nThe function will not check links for coming or going on the right of the left component and left of the right component. It will just get left links from left component and right links from right component and assign them to the new component.\n\n\n\ncheckLinksZoom\n\n checkLinksZoom (compNum, fromComponentLinks, toComponentLinks)\n\n\n\n\ncheckForBreaksZoom\n\n checkForBreaksZoom (zoomLevel, compNum, components, fromComponentLinks,\n                     toComponentLinks, blockEdges)\n\n\n\nUpdate links\n\n\n\nsplitPositiveNegative\n\n splitPositiveNegative (compID, accs, components)\n\nThis function simply pulls all accession presented in the component and split them into forward and inversed.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncompID\n\n\n\n\naccs\n\n\n\n\ncomponents\n\n\n\n\nReturns\nposAcc: list[int]. IDs of accession which has forward direction in given component.\n\n\n\n\n\n\n\nintersectAccLists\n\n intersectAccLists (accList, dirDict)\n\n\n\n\nupdateLinks\n\n updateLinks (newToOldInd, oldToNewInd, fromComponentLinks,\n              toComponentLinks, linkLengths, pairedLinks,\n              interconnectedLinks, blockEdges, accStarts, accEnds,\n              components, compAccDir, newFromComponentLinks={},\n              newToComponentLinks={})\n\nnewToOldInd and oldToNewInd: both index and values are 0-based numbers of components in previous and current zoomlayer.\n\n\nMain layer generation function + assistant function\n\n\n\nisStartEnd\n\n isStartEnd (compNum, components)\n\n\n\n\nnextLayerZoom\n\n nextLayerZoom (zoomLevel, components, componentLengths,\n                fromComponentLinks, toComponentLinks, graph, accStarts,\n                accEnds, maxLengthComponent, linkLengths, pairedLinks,\n                interconnectedLinks, blockEdges, inversionThreshold=0.5,\n                debug=False, debugTime=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nzoomLevel\n\n\n\n\n\ncomponents\n\n\n\n\n\ncomponentLengths\n\n\ncomponentNucleotides,\n\n\nfromComponentLinks\n\n\n\n\n\ntoComponentLinks\n\n\n\n\n\ngraph\n\n\n\n\n\naccStarts\n\n\n\n\n\naccEnds\n\n\n\n\n\nmaxLengthComponent\n\n\n\n\n\nlinkLengths\n\n\n\n\n\npairedLinks\n\n\n\n\n\ninterconnectedLinks\n\n\n\n\n\nblockEdges\n\n\n\n\n\ninversionThreshold\nfloat\n0.5\n\n\n\ndebug\nbool\nFalse\n\n\n\ndebugTime\nbool\nFalse"
  },
  {
    "objectID": "export.html#clear-elements-too-small-to-show",
    "href": "export.html#clear-elements-too-small-to-show",
    "title": "Export module",
    "section": "Clear elements too small to show",
    "text": "Clear elements too small to show\nThis set of functions (with the orchestrating function being clearInvisible) look at earlier identified non-linear link to size (or number of nucleotides) associations and if the next zoom level is larger than some sizes, then these links are removed (with reinstating of some of linear links instead).\nAfter that Isolation blocks are identified and removed. Isolation block is a contiguous block of components (columns) that are connected only to each other but not to any of components outside the block.\n\nRemoving links and rearrangement blocks associated to too small blocks\n\n\n\nremoveLink\n\n removeLink (fromComponentLinks, toComponentLinks, linkList, remLinks,\n             link, pairedLink=None, subLink=None, subLinks=None,\n             remLinkAccessions=None)\n\nThis function remove the main link.\nIf paired and substitute links are provided, the paired link will be checked (if it is not removed or in the queue to be removed), it will be added to the queue\nAfter that common accessions for the same strand (for each separately) for start of main link and and end of paired link are found and substitute link is established for all such accessions.\nIf the substitute link is not (k,k+1), but (k,k+p), then in componentLinks all links (k,k+1),(k+1,k+1),…,(k+p-1,k+p) are established.\n\n\n\nprocessCollapsibleBlocks\n\n processCollapsibleBlocks (zoomLevel, linkLengths, pairedLinks,\n                           interconnectedLinks, fromComponentLinks,\n                           toComponentLinks)\n\n\n\n\nclearRearrangementBlocks\n\n clearRearrangementBlocks (zoomLevel, blockEdges)\n\n\n\nFind isolated blocks\n\nIdentify empty edges\n\n\n\n\ntestStartEnd\n\n testStartEnd (compNum, isLeft, components, accStarts, accEnds)\n\n\n\n\nfindEmptyEdges\n\n findEmptyEdges (fromComponentLinks, toComponentLinks, accStarts, accEnds,\n                 components)\n\nIdentify all empty edges by simply finding components that do not appear either in toComponentLinks (left empty) or fromComponentLinks (right empty)\n\nIdentify isolated blocks\n\n\n\n\ncheckExternalLinks\n\n checkExternalLinks (blockStart, blockEnd, fromComponentLinks,\n                     toComponentLinks, components)\n\n\n\n\ncreateNewBoundaries\n\n createNewBoundaries (blockStart, blockEnd, externalLinksComps,\n                      leftEmptyList, rightEmptyList)\n\n\n# Test for `createNewBoundaries`\nimport numpy as np\n\nst = [2,5,6,8]\nend = [2,3,4,6,8,9,10,11]\n\nblocks = [[2,11],[2,3],[5,11],[8,11],[8,9],[8,11]]\nblockSplits = [[[2,3],[5,11]],[[2,2]],[[6,6],[8,11]],[[8,9]],[[8,8]],[]]\nexternals = [[4],[3],[5,7],[10],[9],[8,9,10,11]]\n\nfor bl,blSpl,ext in zip(blocks,blockSplits,externals):\n    blSplTT = createNewBoundaries(*bl,ext,st,end)\n    assert blSpl == blSplTT,f'Expected {blSpl}, but got {blSplTT}'\n\n\n# Another test for `createNewBoundaries`\nleftEmptyList = [2056, 3080, 3081, 2092, 2099, 1593, 3643, 2627, 1116, 2653, 2655, 3168, 2658, 613, 1637, 1638, 106, 1654, 2695, 2192, 1169, 1686, 2714, 3757, 2233, 3781, 723, 1240, 224, 1761, 1762, 1766, 3323, 1804, 786, 2331, 802, 2850, 807, 811, 1839, 1841, 3396, 3397, 1863, 3400, 843, 3423, 1898, 1899, 882, 884, 3463, 402, 2451, 3478, 408, 3482, 934, 426, 1962, 3504, 3516, 3519, 3520, 451, 1994, 1995, 972, 2506, 463, 3024, 1493, 1494, 3542, 1525]\nrightEmptyList = [402, 2451, 3478, 407, 280, 3482, 2848, 802, 934, 807, 426, 811, 2091, 2092, 3757, 1839, 3504, 1841, 3516, 3519, 3405, 463, 722, 1240, 1761, 1762, 1766, 1899]\nblockStart = 3396\nblockEnd = 3405\nexternalLinksComps = [3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405]\n\ncreateNewBoundaries(blockStart,blockEnd,externalLinksComps,leftEmptyList,rightEmptyList)\n\n[]\n\n\n\n\n\nidentifyIsolatedBlocks\n\n identifyIsolatedBlocks (leftEmptyList, rightEmptyList,\n                         fromComponentLinks, toComponentLinks, components)\n\n\n\nRemoving Isolated Blocks\n\n\n\nupdateLinksRemoveComp\n\n updateLinksRemoveComp (oldToNewInd, fromComponentLinks, toComponentLinks,\n                        linkLengths, pairedLinks, interconnectedLinks,\n                        blockEdges, accStarts, accEnds)\n\n\n\n\nremoveIsolatedBlocks\n\n removeIsolatedBlocks (isolatedBlockList, components, componentLengths,\n                       fromComponentLinks, toComponentLinks, accStarts,\n                       accEnds, linkLengths, pairedLinks,\n                       interconnectedLinks, blockEdges)\n\n\n\nClearing small element wrapping function\n\n\n\nclearInvisible\n\n clearInvisible (zoomLevel, linkLengths, pairedLinks, interconnectedLinks,\n                 blockEdges, fromComponentLinks, toComponentLinks,\n                 accStarts, accEnds, components, componentLengths)"
  },
  {
    "objectID": "export.html#exporting-layer",
    "href": "export.html#exporting-layer",
    "title": "Export module",
    "section": "Exporting layer",
    "text": "Exporting layer\nThese functions, with the main one being exportLayer, are exporting prepared zoom level (cleaned and collapsed by other functions) into Pantograph Visualisation tool data structures (JSON chunk files).\n\n\ncreateZoomLevelDir\n\n createZoomLevelDir (outputPath, outputName, zoomLevel)\n\nCreates a directory for zoom level chunks. The function will take care of correct directory level separator.\n\n\n\nfinaliseChunk\n\n finaliseChunk (rootStruct, zoomLevel, chunk, nucleotides, nBins,\n                chunkNum, curCompCols, prevTotalCols, outputPath,\n                outputName)\n\n\n\n\naddLinksToComp\n\n addLinksToComp (compNum, components, fromComponentLinks,\n                 toComponentLinks)\n\n\n\n\ncheckLinks\n\n checkLinks (leftComp, rightComp)\n\n\n\n\nsearchIndicesPosRecord\n\n searchIndicesPosRecord (redisConn, redisCaseID, zoomLevel, accessions,\n                         posMapping)\n\n\n\n\nexportLayer\n\n exportLayer (zoomLevel, components, componentNucleotides,\n              fromComponentLinks, toComponentLinks, rootStruct,\n              outputPath, outputName, maxLengthComponent, maxLengthChunk,\n              inversionThreshold=0.5, redisConn=None, redisCaseID=None,\n              accessions=None, debug=False)"
  },
  {
    "objectID": "export.html#main-exporter-wrapper-with-its-helper-functions",
    "href": "export.html#main-exporter-wrapper-with-its-helper-functions",
    "title": "Export module",
    "section": "Main exporter wrapper with its helper functions",
    "text": "Main exporter wrapper with its helper functions\nThis is the main orchestrating function that export a single graph to Pantograph Visualisation tool with a couple of auxiliary functions.\n\n\ncompLinksToAccCompLinks\n\n compLinksToAccCompLinks (compLinks, doCompDir=False)\n\n\n\n\nrecordZoomLevelForDebug\n\n recordZoomLevelForDebug (zoomNodeToComponent, zoomComponentToNodes,\n                          zoomComponents, nodeToComponent,\n                          componentToNodes, components, zoomLevel)\n\nA function which records result of segmentation to dictionaries, which holds results for all zoom levels. It is currently used only for debugging purposes and in normal operation all zoom level dictionaries are not created and used.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nzoomNodeToComponent\n\n\n\n\nzoomComponentToNodes\n\n\n\n\nzoomComponents\n\n\n\n\nnodeToComponent\n\n\n\n\ncomponentToNodes\n\n\n\n\ncomponents\n\n\n\n\nzoomLevel\n\n\n\n\nReturns\nReturns modified dictionaries with zoom in the beginning of the names. Theoretically,\n\n\n\n\n\n\n\nsearchIndicesGeneRecord\n\n searchIndicesGeneRecord (redisConn, redisCaseID, geneMapping,\n                          genPosMapping, altChrGenPosMapping,\n                          genPosSearchMapping, pangenPosSearchMapping)\n\nRecording prepared metadata structures into Redis DB\n\n\n\nexportToPantograph\n\n exportToPantograph (graph=None, inputPath=None, GenomeGraphParams={},\n                     outputPath=None, outputName=None, outputSuffix=None,\n                     isSeq=True, nodeLengths=None, redisConn=None,\n                     zoomLevels=[1], fillZoomLevels=True,\n                     maxLengthComponent=100, maxLengthChunk=20,\n                     inversionThreshold=0.5, debug=False,\n                     returnDebugData=False)\n\nThis function is used by exportProject function and should not normally be used independently now."
  },
  {
    "objectID": "export.html#project-generation",
    "href": "export.html#project-generation",
    "title": "Export module",
    "section": "Project generation",
    "text": "Project generation\n\n\nexportProject\n\n exportProject (projectID, projectName, caseDict, pathToIndex,\n                pathToGraphs, redisHost=None, redisPort=6379, redisDB=0,\n                suffix='', maxLengthComponent=100, maxLengthChunk=6,\n                inversionThreshold=0.5, isSeq=False, zoomLevels=[1],\n                fillZoomLevel=True)\n\nThis is the only function that should normally be used to export a set of graphs (e.g. a graph per chromosome) to Pantograph Visualisation tool as a project (or interconnected structure).\nExporting of each graph creates a case directory _\n\n with bin2file.json file which describes the case overall and each zoom level. At the same time, each zoom level is contained in multiple chunk JSON files, each zoom level n is in the directory n inside the case directory. Each JSON chunk files contains all required information to visualise up to maxLengthChunk components at a given zoom level.\nALl case directories are in project directory together with <projectID>_project.json, which is simply provides association between case names and and corresponding directory name.\nFinally, information about the project will be recorded to Pantograph Visualisation tool data index to make it discoverable by the tool.\nIn addition, no metadata is recorded into these files as it inflates it very quickly. Instead, a very simple (optional) API works alongside main Pantograph Visualisation tool which provides a lot of various metadata on request if API available or do nothing if not. This API uses Redis DB with special DB schema.\nWhen graphs are exported some metadata (annotations, genome and pangenome positions) can be recorded to Redis DB. If Redis DB is not available or recording of metadata is not needed, then parameter redisHost should be omitted. Otherwise, if Redis DB is available and metadata should be recorded, then redisHost should be set to the hostname (or IP address) of the Redis DB server"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pygengraph",
    "section": "",
    "text": "Enter the directory of the library and enter:\npip install .\nand for development use\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "pygengraph",
    "section": "How to use",
    "text": "How to use\n\nfrom nbdev import nbdev_export\nnbdev_export()\n\n\nimport os\nimport glob\nimport re\nimport time\n\nfrom pygengraph.graph import GenomeGraph\nfrom pygengraph.utils import pathFileToPathDict\nfrom pygengraph.export import exportProject"
  },
  {
    "objectID": "index.html#generating-from-annotation",
    "href": "index.html#generating-from-annotation",
    "title": "pygengraph",
    "section": "Generating from annotation",
    "text": "Generating from annotation\n\nPreparing list of files\n\nrefdir = '/path/to/reference/'\nannotationdir = '/path/to/annotation'\ngfadir = '/path/to/graphs'\n\n\nannotationFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}*.gff'))\npangenomeFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}*pangen.gff'))\n# If you want to include sequences instead of simple notion of genes.\n# It should also be converted to sequenceFileDict, see details in documentation for GenomeGraph Class constructor.\n# sequenceFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}sequences{os.path.sep}*.fasta'))\nrefAnnotationFile = f'{refdir}{os.path.sep}reference.gff'\n# If you want to include sequences instead of simple notion of genes\n# refSequenceFile = f'{refdir}{os.path.sep}reference.fasta'\n\n\nrefdir = '../../1001G/annotations/freeze2.1/outgroups'\nannotationdir = '../../1001G/annotations/freeze2.1'\ngfadir = '../../1001G/annotations/graphs'\n\n\nannotationFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}*.gff'))\n# pangenomeFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}*pangen.gff'))\n# If you want to include sequences instead of simple notion of genes.\n# It should also be converted to sequenceFileDict, see details in documentation for GenomeGraph Class constructor.\n# sequenceFiles = sorted(glob.glob(f'{annotationdir}{os.path.sep}sequences{os.path.sep}*.fasta'))\nrefAnnotationFile = f'{refdir}{os.path.sep}araport.gff'\n# If you want to include sequences instead of simple notion of genes\n# refSequenceFile = f'{refdir}{os.path.sep}reference.fasta'\n\n\nannotationFiles\n\n['../../1001G/annotations/freeze2.1/10002.gff',\n '../../1001G/annotations/freeze2.1/10015.gff',\n '../../1001G/annotations/freeze2.1/10024.gff',\n '../../1001G/annotations/freeze2.1/1741.gff',\n '../../1001G/annotations/freeze2.1/22001.gff',\n '../../1001G/annotations/freeze2.1/22002.gff',\n '../../1001G/annotations/freeze2.1/22003.gff',\n '../../1001G/annotations/freeze2.1/22004.gff',\n '../../1001G/annotations/freeze2.1/22005.gff',\n '../../1001G/annotations/freeze2.1/22006.gff',\n '../../1001G/annotations/freeze2.1/22007.gff',\n '../../1001G/annotations/freeze2.1/6024.gff',\n '../../1001G/annotations/freeze2.1/6069.gff',\n '../../1001G/annotations/freeze2.1/6124.gff',\n '../../1001G/annotations/freeze2.1/6244.gff',\n '../../1001G/annotations/freeze2.1/6909.gff',\n '../../1001G/annotations/freeze2.1/6966.gff',\n '../../1001G/annotations/freeze2.1/8236.gff',\n '../../1001G/annotations/freeze2.1/9075.gff',\n '../../1001G/annotations/freeze2.1/9537.gff',\n '../../1001G/annotations/freeze2.1/9543.gff',\n '../../1001G/annotations/freeze2.1/9638.gff',\n '../../1001G/annotations/freeze2.1/9728.gff',\n '../../1001G/annotations/freeze2.1/9764.gff',\n '../../1001G/annotations/freeze2.1/9888.gff',\n '../../1001G/annotations/freeze2.1/9905.gff',\n '../../1001G/annotations/freeze2.1/9981.gff']\n\n\n\n\nGeneraton of gene graph\n\ndoUS = False\nn = 1\nfor chrnum in range(1,n+1): # here n is number of chromosomes.\n    chromosome = f'Chr{chrnum}'\n\n    print(f'\\nProcessing {chromosome}\\n============')\n\n    curtst = time.time()\n    \n    graph = GenomeGraph(annotationFiles = annotationFiles,\n                        pangenomeFiles = None,\n                        sequenceFilesDict = None,\n                        doUS = doUS,\n                        chromosome = chromosome,\n                        refAnnotationFile=refAnnotationFile,\n                        refAccession='TAIR10')\n    \n    print(f'Generating graph for {chromosome} took {time.time() - curtst} seconds')\n    \n    curtst = time.time()\n    graph.treeSort()\n    print(f'Sorting graph for {chromosome} took {time.time() - curtst} seconds')\n    if len(graph.nodes)!=len(graph.order):\n            print('Sorting failed and not all nodes were sorted. Saving unsorted graph')\n            gfaFilename = f'Gene_{chromosome}_simOnly_unordered.gfa'\n            graph.order = list(range(1,len(graph.nodes)+1))\n    else:\n        gfaFilename = f'Gene_{chromosome}_simOnly.gfa'\n    \n    graph.toGFA(f'{gfadir}{os.path.sep}{gfaFilename}',doSeq=False)\n\n\nq = [1,2,3,4]\n\n\nq.remove(3)\n\n\nq\n\n[1, 2, 4]"
  },
  {
    "objectID": "index.html#loading-pathfile-to-graph",
    "href": "index.html#loading-pathfile-to-graph",
    "title": "pygengraph",
    "section": "Loading Pathfile to graph",
    "text": "Loading Pathfile to graph\n\n# For path file v1\npathfileDir = 'examples/gene_graph'\n\npathsfile = 'paths_genegraph.txt'\n\npaths = pathFileToPathDict(f'{pathfileDir}{os.path.sep}{pathsfile}', True, True)\n\ngraph = GenomeGraph(pathsDict=paths)\n\ngraph.treeSort()\n\nif len(graph.nodes)!=len(graph.order):\n    print('Sorting failed and not all nodes were sorted. Saving unsorted graph')\n    output = 'paths_genegraph_unordered.gfa'\n    graph.order = list(range(1,len(graph.nodes)+1))\n    graph.toGFA(output,doSeq=False)\nelse:\n    coreGFApath = f'paths_genegraph.gfa'\n    coregraph.toGFA(coreGFApath,doSeq=False)\n\n\n# For v2\n# This is example, no v2 file currently available for demonstration.\npathfileDir = '/path/to/file'\n\npathsfile = f'paths.txt'\n\npaths = pathFileToPathDict(f'{pathfileDir}{os.path.sep}{pathsfile}',True,'reference',True)\n\nfor seqNum in paths.keys():\n\n    graph = GenomeGraph(pathsDict=paths[seqNum])\n\n    # On undirected coregraph sorting is not optimal! Check sorting!!!\n\n    graph.treeSort()\n\n    if len(graph.nodes)!=len(graph.order):\n        print('Sorting failed and not all nodes were sorted. Saving unsorted graph')\n        output = f'{pathfileDir}{os.path.sep}graph_Chr{seqNum}_unordered.gfa'\n        graph.order = list(range(1,len(graph.nodes)+1))\n        graph.toGFA(output,doSeq=False)\n    else:\n        coreGFApath = f'{pathfileDir}{os.path.sep}graph_Chr{seqNum}.gfa'\n        graph.toGFA(coreGFApath,doSeq=False)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils module",
    "section": "",
    "text": "generateComplementDict (seqType='DNA', isDict=True)\n\nThe function generateComplementDict generates a dictionary for complementing the DNA sequence. It can be applied to RNA to identify inverted sequences.\nseqType: str, Can be either ‘DNA’ or ‘RNA’ at the moment. If ‘DNA’, then the complement to four known nucleotide (A, C, G, T) will be provided. All other letters (B, D, H, U, N and all others) will be translated to N.\n\n\n\n\n\n complementSequence (seq, complementDict='DNA')\n\n\n\n\n\n\n reverseSequence (seq)\n\n\n\n\n\n\n inverseSequence (seq, complementDict='DNA')"
  },
  {
    "objectID": "utils.html#other-file-operations",
    "href": "utils.html#other-file-operations",
    "title": "Utils module",
    "section": "Other file operations",
    "text": "Other file operations\n\n\ncheckNodeLengthsFile\n\n checkNodeLengthsFile (GFAPath)"
  },
  {
    "objectID": "utils.html#path-files-operations",
    "href": "utils.html#path-files-operations",
    "title": "Utils module",
    "section": "Path files operations",
    "text": "Path files operations\n\n\nsortAccessions\n\n sortAccessions (sort, _paths)\n\n/home/pigrenok/.pyenv/versions/3.10.9/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Return\n  else: warn(msg)\n\n\n\npathFileToPathDict\n\n pathFileToPathDict (filePath, directional=True, sort=True, v2=True)\n\nReads path file (ASCII file) and translates it to path dictionary for GenGraph class constructor.\nPath file has a path on each line in the following format: : <nodeID[+|-]>[,<nodeID[+,-]>]"
  },
  {
    "objectID": "utils.html#export-parameters-processing-and-validating",
    "href": "utils.html#export-parameters-processing-and-validating",
    "title": "Utils module",
    "section": "Export parameters processing and validating",
    "text": "Export parameters processing and validating\n\n\npathConvert\n\n pathConvert (inputPath, suffix='')\n\n\n\n\ncheckZoomLevels\n\n checkZoomLevels (zoomLevels)\n\nCheck that each previous zoom level is factor of next one\n\n\n\nadjustZoomLevels\n\n adjustZoomLevels (zoomLevels)\n\nIf there is no zoom level 1, adds it to the list."
  },
  {
    "objectID": "utils.html#utility-classes",
    "href": "utils.html#utility-classes",
    "title": "Utils module",
    "section": "Utility classes",
    "text": "Utility classes\n\nNumpy to JSON encoder\n\n\n\nNpEncoder\n\n NpEncoder (skipkeys=False, ensure_ascii=True, check_circular=True,\n            allow_nan=True, sort_keys=False, indent=None, separators=None,\n            default=None)\n\nExtensible JSON https://json.org encoder for Python data structures.\nSupports the following objects and types by default:\n\n\n\n\n\n\n\nPython\nJSON\n\n\n\n\ndict\nobject\n\n\nlist, tuple\narray\n\n\nstr\nstring\n\n\nint, float\nnumber\n\n\nTrue\ntrue\n\n\nFalse\nfalse\n\n\nNone\nnull\n\n\n\nTo extend this to recognize other objects, subclass and implement a .default() method with another method that returns a serializable object for o if possible, otherwise it should call the superclass implementation (to raise TypeError).\n\n\nBidirectional dict structure\n\n\n\nbidict\n\n bidict (*args, **kwargs)\n\nHere is a class for a bidirectional dict, inspired by Finding key from value in Python dictionary and modified to allow the following 2) and 3).\nNote that :\n\nThe inverse directory bd.inverse auto-updates itself when the standard dict bd is modified.\nThe inverse directory bd.inverse[value] is always a list of keys such that value in bd[key] for each key.\nUnlike the bidict module from https://pypi.python.org/pypi/bidict, here we can have 2 keys having same value, this is very important.\nAfter modification, values in the “forward” (not inversed) dict can be lists (or any iterables theoretically, but only list was tested).\n\nFor implementing 4), new method add was introduced. If d[key].append(value) attempted, the link between main and inversed dict will be broken. Method add can accept both\nCredit: Implemented as an answer to https://stackoverflow.com/questions/3318625/how-to-implement-an-efficient-bidirectional-hash-table by Basj (https://stackoverflow.com/users/1422096/basj)."
  },
  {
    "objectID": "utils.html#redis-utility",
    "href": "utils.html#redis-utility",
    "title": "Utils module",
    "section": "Redis utility",
    "text": "Redis utility\n\nDB cleaning and maintenance\n\n\n\nresetDB\n\n resetDB (redisServer='redis', port=6379)\n\nReset the whole database. Be careful, it is impossible re restore DB once it was flushed.\n\n\nFunctions implementing secondary interval set in Redis database\n\n\n\niset_add\n\n iset_add (r, name, intervalMapping)\n\nAdd members with intervals to interval set. If interval set does not exist, it will be created. In reality, it will create two Redis Sorted Sets for starts and ends of the intervals. The rest of the functions iset_ will know what to do with them.\nr: Redis object. Redis client. name: string. Name of the interval set. intervalMapping: dict. Dictionary with names of intervals as keys and tuples with start and end of intervals.\nReturn number of added intervals. In reality, it adds equal number of elements to two sorted sets, if number of added elements are not equal, DataError is raised.\n\n\n\niset_get\n\n iset_get (r, name, member=None)\n\nReturn either the whole interval set or specific name(s) with its interval.\nr: Redis object. Redis client. name: string. Name of the interval set. member: string, list, tuple or None. If None, function return all members with their respective intervals. If string, returns a single member with its interval, if list or tuple, returns all requested members with their respecitve intervals.\nReturn a dictionary with member names as keys and tuples with interval starts and ends as values. For member names not found in interval set, the value for the given key will be a tuple (None,None).\n\n\n\niset_score\n\n iset_score (r, name, start, end=None)\n\nReturns all member names whose interval contains a given value or intersects with the given interval\nr: Redis object. Redis client. name: string. Name of the interval set start: int. Query value or the start of query interval. end: int or None. If None, start is treated as a single query value. If int, then start is the start of the query interval, end is the end of the query interval.\nReturns a list of members whose intervals either contain query value or intersects with query interval.\n\n\n\niset_not_score\n\n iset_not_score (r, name, start, end=None)\n\nReturns all intervals (member names only) where query value is not contained or query interval is not intersecting. Inverison of iset_score() function\nr: Redis object. Redis client. name: string. Name of the interval set start: int. Query value or the start of query interval. end: int or None. If None, start is treated as a single query value. If int, then start is the start of the query interval, end is the end of the query interval.\nReturns a list of members whose intervals either does not contain query value or does not intersect with query interval.\n\n\n\niset_del\n\n iset_del (r, name, member=None)\n\nReturn either the whole interval set or specific name(s) with its interval.\nr: Redis object. Redis client. name: string. Name of the interval set. member: string, list, tuple or None. If None, function return all members with their respective intervals. If string, returns a single member with its interval, if list or tuple, returns all requested members with their respecitve intervals.\nReturn number of removed intervals. In reality, it removes equal number of elements from two sorted sets, if number of added elements are not equal, DataError is raised."
  },
  {
    "objectID": "debruijngraphprocessing.html",
    "href": "debruijngraphprocessing.html",
    "title": "Processing GFA produced by CuttleFish compacted de Bruijn graph constructor",
    "section": "",
    "text": "from nbdev import nbdev_export\nnbdev_export()"
  },
  {
    "objectID": "debruijngraphprocessing.html#loading-graph",
    "href": "debruijngraphprocessing.html#loading-graph",
    "title": "Processing GFA produced by CuttleFish compacted de Bruijn graph constructor",
    "section": "Loading graph",
    "text": "Loading graph\n\nnotebook2script()\n\nConverted 00_init.ipynb.\nConverted 01_graph.ipynb.\nConverted 02_tree.ipynb.\nConverted 03_synteny.ipynb.\nConverted 04_utils.ipynb.\nConverted 05_export.ipynb.\nConverted deBruijnGraphProcessing.ipynb.\nConverted index.ipynb.\n\n\n\npaths = '../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/nextStageTest.gfa'\n# paths = '../../1001G/pantograph/data/AT_Chr1_OGOnly_new.gfa'\n\n\ndirpath = '../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out'\n# paths = '../../1001G/pantograph/data/AT_Chr1_OGOnly_new.gfa'\n\n\nfor path in glob.glob(f'{dirpath}{os.path.sep}*.gfa'):\n    graph = GenomeGraph(gfaPath=path)\n#     dirpath = os.path.dirname(path)\n    fileList = os.path.splitext(os.path.basename(path))\n    suffix = 'clean'\n    graph.toGFA(f'{dirpath}{os.path.sep}{fileList[0]}_{suffix}{fileList[1]}')\n\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_transpose_common_mid.gfa\nLoading segment 7/7\nLoading segments finished.\nLoading link 8/8\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nNode 3 inverted\nNode 4 inverted\nNode 7 inverted\nConstructing Tremaux tree\nNode: 1\nGraph edges: [(7, 1)]\nTree edges: []\nEdge to keep: (7, 1)\nCycle found!\nPath to break: [1, 2, 3, 7]\nUnbreakable path!\nNode: 1\nGraph edges: [(7, 1)]\nTree edges: []\nEdge to keep: (7, 1)\nCycle found!\nPath to break: [1, 2, 3, 7]\nUnbreakable path!\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 3\n1 - 2 - 3 - 3 - 7\n0 - 3 - 4 - 3 - 5\n0 - 4 - 5 - 5 - 6\n0 - 5 - 6 - 6 - 4\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/fully_inverted.gfa\nLoading segment 1/1\nLoading segments finished.\n\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 1/1\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/mid_block_inverted.gfa\nLoading segment 7/7\nLoading segments finished.\nLoading link 8/8\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 4\n0 - 1 - 2 - 1 - 2\n0 - 2 - 3 - 2 - 3\n1 - 3 - 4 - 3 - 7\n0 - 4 - 5 - 3 - 5\n0 - 5 - 6 - 5 - 6\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/end_block_inverted.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 4\n0 - 1 - 2 - 1 - 2\n0 - 2 - 3 - 2 - 3\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/substitution_mid.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 3\n0 - 1 - 2 - 1 - 2\n0 - 2 - 3 - 2 - 4\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/end_block_indel.gfa\nLoading segment 2/2\nLoading segments finished.\nLoading link 1/1\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 2/2\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\nCalculating nodes length...\nProcessing node 2/2\nFinished calculating nodes lengths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/full_copy.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 5/5\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nNode 3 inverted\nConstructing Tremaux tree\nNode: 2\nGraph edges: [(1, 2), (3, 2)]\nTree edges: [(3, 2)]\nEdge to keep: (1, 2)\nCycle found!\nPath to break: [2, 4, 1]\nUnbreakable path!\nNode: 2\nGraph edges: [(1, 2), (3, 2)]\nTree edges: [(3, 2)]\nEdge to keep: (1, 2)\nCycle found!\nPath to break: [2, 4, 1]\nUnbreakable path!\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 3 - 2\n0 - 1 - 2 - 2 - 4\n0 - 2 - 3 - 4 - 1\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_copy_common_mid.gfa\nLoading segment 3/3\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nConstructing Tremaux tree\nNode: 1\nGraph edges: [(3, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 2, 3]\nUnbreakable path!\nNode: 1\nGraph edges: [(3, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 2, 3]\nUnbreakable path!\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 3\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_transpose_common_end.gfa\nLoading segment 7/7\nLoading segments finished.\nLoading link 8/8\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nNode 3 inverted\nNode 7 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 4\n1 - 1 - 2 - 4 - 6\n1 - 1 - 2 - 1 - 2\n1 - 2 - 3 - 2 - 3\n1 - 3 - 4 - 3 - 5\n0 - 4 - 5 - 4 - 6\n0 - 5 - 6 - 6 - 7\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 7/7\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_copy_common_end.gfa\nLoading segment 3/3\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 2 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 3\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_transpose_common_both_ends.gfa\nLoading segment 10/10\nLoading segments finished.\nLoading link 12/12\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 10/10\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 8 inverted\nNode 9 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 3\n1 - 1 - 2 - 3 - 7\n2 - 2 - 3 - 7 - 6\n1 - 3 - 4 - 7 - 8\n1 - 4 - 5 - 8 - 10\n1 - 4 - 5 - 1 - 2\n1 - 5 - 6 - 2 - 4\n2 - 6 - 7 - 4 - 5\n1 - 7 - 8 - 4 - 9\n0 - 8 - 9 - 8 - 10\nCalculating nodes length...\nProcessing node 10/10\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 10/10\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/equal.gfa\nLoading segment 1/1\nLoading segments finished.\n\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 1/1\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/mid_block_indel.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 3\n0 - 1 - 2 - 1 - 2\n0 - 2 - 3 - 2 - 4\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/full_transpose.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 4/4\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nNode 3 inverted\nNode 4 inverted\nConstructing Tremaux tree\nNode: 1\nGraph edges: [(3, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 2, 4, 3]\nUnbreakable path!\nNode: 1\nGraph edges: [(3, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 2, 4, 3]\nUnbreakable path!\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 4\n0 - 2 - 3 - 4 - 3\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/substitution_end.gfa\nLoading segment 3/3\nLoading segments finished.\nLoading link 2/2\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 1 inverted\nNode 2 inverted\nNode 3 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n1 - 0 - 1 - 1 - 3\n0 - 1 - 2 - 1 - 2\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/partial_copy_common_both_ends.gfa\nLoading segment 4/4\nLoading segments finished.\nLoading link 6/6\nLoading links finished\nLoading path 2/2\nLoading paths finished. 2 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\nNode 2 inverted\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n1 - 1 - 2 - 2 - 4\n0 - 2 - 3 - 2 - 3\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nCalculating nodes length...\nProcessing node 4/4\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\n\n\n\ngraph = GenomeGraph(gfaPath=paths)\n\nLoading graph from ../../1001G/GraphGeneration/TestingPipelines/data/cuttlefish/out/nextStageTest.gfa\nLoading segment 29/29\nLoading segments finished.\nLoading link 98/98\nLoading links finished\nLoading path 7/7\nLoading paths finished. 7 paths added, 0 paths ignored.\nCalculating nodes length...\nProcessing node 29/29\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 7/7\nFinished preprocessing paths\nNode 2 inverted\nNode 3 inverted\nNode 4 inverted\nNode 6 inverted\nNode 10 inverted\nNode 11 inverted\nNode 14 inverted\nNode 16 inverted\nNode 21 inverted\nNode 22 inverted\nNode 23 inverted\nNode 24 inverted\nNode 28 inverted\nConstructing Tremaux tree\nNode: 1\nGraph edges: [(3, 1), (7, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 2, 4, 19, 20, 12, 28, 15, 17, 3]\nUnbreakable path!\nNode: 10\nGraph edges: [(9, 10), (13, 10), (14, 10), (21, 10)]\nTree edges: [(13, 10)]\nEdge to keep: (14, 10)\nCycle found!\nPath to break: [10, 9, 29, 25, 8, 22, 23, 14]\nEdge substituted!\nEdge (20, 22) substitute edge (8, 22)\nNode: 1\nGraph edges: [(3, 1), (7, 1)]\nTree edges: []\nEdge to keep: (3, 1)\nCycle found!\nPath to break: [1, 3]\nUnbreakable path!\nNode: 22\nGraph edges: [(8, 22), (11, 22), (18, 22), (20, 22)]\nTree edges: [(20, 22)]\nEdge to keep: (8, 22)\nCycle found!\nPath to break: [22, 23, 14, 10, 9, 29, 25, 8]\nEdge substituted!\nEdge (21, 10) substitute edge (14, 10)\nNode: 10\nGraph edges: [(9, 10), (13, 10), (14, 10), (21, 10)]\nTree edges: [(21, 10)]\nEdge to keep: (14, 10)\nCycle found!\nPath to break: [10, 9, 29, 25, 8, 22, 23, 14]\nEdge substituted!\nEdge (20, 22) substitute edge (8, 22)\nDone!\nGetting root nodes\nStart Loop...\n2 - 0 - 1 - 1 - 3\n2 - 1 - 2 - 3 - 17\n2 - 1 - 2 - 1 - 2\n2 - 2 - 3 - 2 - 4\n3 - 3 - 4 - 4 - 19\n3 - 4 - 5 - 19 - 20\n4 - 5 - 6 - 20 - 22\n4 - 6 - 7 - 22 - 23\n5 - 7 - 8 - 23 - 14\n5 - 8 - 9 - 14 - 10\n5 - 8 - 9 - 1 - 21\n4 - 9 - 10 - 14 - 10\n5 - 10 - 11 - 10 - 9\n5 - 11 - 12 - 9 - 29\n5 - 12 - 13 - 29 - 25\n5 - 12 - 13 - 23 - 24\n4 - 13 - 14 - 29 - 25\n5 - 14 - 15 - 25 - 26\n6 - 15 - 16 - 26 - 7\n5 - 16 - 17 - 26 - 27\n4 - 17 - 18 - 25 - 8\n3 - 18 - 19 - 10 - 13\n2 - 19 - 20 - 20 - 12\n2 - 20 - 21 - 12 - 28\n2 - 21 - 22 - 28 - 15\n1 - 22 - 23 - 4 - 5\n0 - 23 - 24 - 3 - 17\n2 - 24 - 25 - 17 - 11\n1 - 25 - 26 - 17 - 16\n1 - 26 - 27 - 16 - 6\n0 - 27 - 28 - 17 - 18\n> /data/YandexDisk/Kew/src/graphConstruction/pangraph_constructor/graph.py(159)__init__()\n    157             self.treeSort()\n    158             pdb.set_trace()\n--> 159             self.removeOverlaps()\n    160 \n    161     # check edgePaths calculation. Incorrectly calculates the numbers\n\n\n\nipdb>  self.nodes\n\n\n['24290', '8931', '1137', '14693', '1099', '20952', '13261', '10453', '1614', '9478', '22619', '16558', '19628', '22113', '10684', '20353', '21183', '24207', '4434', '9923', '1416', '16345', '10152', '5985', '2318', '11400', '3056', '20959', '1015']\n\n\nipdb>  [f'i:{self.nodes[el]}' for i,el in self.order]\n\n\n*** TypeError: cannot unpack non-iterable int object\n\n\nipdb>  [f'i:{self.nodes[el]}' for i,el in enumerate(self.order)]\n\n\n*** NameError: name 'self' is not defined\n\n\nipdb>  self.order\n\n\n[1, 3, 2, 4, 19, 20, 22, 23, 14, 21, 10, 9, 29, 24, 25, 26, 7, 27, 8, 13, 12, 28, 15, 5, 17, 11, 16, 6, 18]\n\n\nipdb>  self.forwardLinks\n\n\n{1: {'+': [(2, '+'), (3, '+')], '-': [(21, '+')]}, 2: {'+': [(4, '-')]}, 3: {'-': [(1, '-')], '+': [(17, '+')]}, 17: {'+': [(11, '+'), (16, '+')], '-': [(18, '+'), (3, '-')]}, 9: {'+': [(29, '+'), (5, '+')], '-': [(10, '-')]}, 29: {'+': [(25, '+')]}, 25: {'+': [(26, '+'), (8, '+')]}, 4: {'+': [(19, '+')], '-': [(5, '-')]}, 5: {'-': [(9, '-')], '+': [(4, '+')]}, 16: {'-': [(17, '-')], '+': [(13, '-'), (6, '+')]}, 13: {'-': [(10, '+')], '+': [(16, '-')]}, 10: {'-': [(13, '+')], '+': [(9, '+')]}, 6: {'+': [(4, '+')]}, 26: {'+': [(7, '+'), (27, '+')]}, 7: {'+': [(1, '+')]}, 8: {'+': [(22, '-')]}, 22: {'+': [(23, '+')], '-': [(20, '-'), (11, '-')]}, 19: {'+': [(20, '+')]}, 11: {'-': [(17, '-')], '+': [(22, '+')]}, 20: {'-': [(12, '+')], '+': [(22, '+')]}, 12: {'+': [(28, '+')]}, 28: {'+': [(15, '+')]}, 23: {'+': [(14, '+'), (24, '+')]}, 14: {'+': [(10, '+')]}, 15: {'+': [(17, '-')]}, 18: {'+': [(22, '+')]}, 21: {'+': [(10, '+')]}, 24: {'+': [(25, '+')]}, 27: {'+': [(28, '+')]}}\n\n\nipdb>  calcNodeLengths(self)\n\n\nCalculating nodes length...\nProcessing node 29/29\nFinished calculating nodes lengths\n[3000, 59, 59, 3001, 59, 59, 59, 60, 3001, 3030, 60, 58, 59, 58, 60, 31, 3001, 59, 57, 32, 60, 3000, 32, 58, 3000, 31, 59, 3000, 59]\n\n\nipdb>  q\n\n\nBdbQuit: \n\n\n\nimport pdb\npdb.pm()\n\n> /data/YandexDisk/Kew/src/graphConstruction/pangraph_constructor/graph.py(813)<listcomp>()\n    811 \n    812         for pathID in range(len(self.paths)):\n--> 813             self.paths[pathID] = [f'{offsetDict[int(el[:-1])-1]+1}{el[-1]}' for el in self.paths[pathID]]\n    814 \n    815         print('paths')\n\n\n\nipdb>  pathID\n\n\n*** NameError: name 'pathID' is not defined\n\n\nipdb>  u\n\n\n> /data/YandexDisk/Kew/src/graphConstruction/pangraph_constructor/graph.py(813)_clearNodes()\n    811 \n    812         for pathID in range(len(self.paths)):\n--> 813             self.paths[pathID] = [f'{offsetDict[int(el[:-1])-1]+1}{el[-1]}' for el in self.paths[pathID]]\n    814 \n    815         print('paths')\n\n\n\nipdb>  pathID\n\n\n1\n\n\nipdb>  q\n\n\n\nnodeLengths = calcNodeLengths(graph)\n\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\n\n\n\ngraph.nodes,graph.nodeNameToID,graph.forwardLinks,nodeLengths,graph.order,graph.paths\n\n(['2887', '4954', '3502'],\n {'2887': 1, '4954': 2, '3502': 3},\n {1: {'+': [(2, '+')]}, 2: {'+': [(3, '+'), (2, '+')]}},\n [3000, 3000, 3000],\n [1, 2, 3],\n [['1+', '2+', '3+'], ['1+', '2+', '2+', '3+']])\n\n\n\ndirpath = os.path.dirname(paths)\nfileList = os.path.splitext(os.path.basename(paths))\nsuffix = 'clean'\ngraph.toGFA(f'{dirpath}{os.path.sep}{fileList[0]}_{suffix}{fileList[1]}')"
  },
  {
    "objectID": "debruijngraphprocessing.html#inverting-necessary-nodes",
    "href": "debruijngraphprocessing.html#inverting-necessary-nodes",
    "title": "Processing GFA produced by CuttleFish compacted de Bruijn graph constructor",
    "section": "Inverting necessary nodes",
    "text": "Inverting necessary nodes\n\ndef invertNode(nodeID,graph,pathNodeArray):\n    \n    pathIDs,positions = np.where(pathNodeArray==nodeID+1)\n    strandreversal = {'+':'-','-':'+'}\n    for pathID,pos in zip(pathIDs,positions):\n        nodeStrand = graph.paths[pathID][pos]\n        graph.paths[pathID][pos] = nodeStrand[:-1]+strandreversal[nodeStrand[-1]]\n\n    nodeFromLink = graph.forwardLinks.get(nodeID+1,{})\n    positiveStrand = nodeFromLink.get('+',[])\n    negativeStrand = nodeFromLink.get('-',[])\n\n    if len(positiveStrand)>0:\n        graph.forwardLinks[nodeID+1]['-'] = positiveStrand\n    else:\n        if len(negativeStrand)>0:\n            del graph.forwardLinks[nodeID+1]['-']\n\n    if len(negativeStrand)>0:\n        graph.forwardLinks[nodeID+1]['+'] = negativeStrand\n    else:\n        if len(positiveStrand)>0:\n            del graph.forwardLinks[nodeID+1]['+']\n\n    revertLinks = graph._revertLinks(graph.forwardLinks)        \n    nodeToLink = revertLinks.get(nodeID+1,{})\n    positiveStrand = nodeToLink.get('+',[])\n    negativeStrand = nodeToLink.get('-',[])\n\n    for fromNode,fromStrand in positiveStrand:\n        graph.forwardLinks[fromNode][fromStrand].remove((nodeID+1,'+'))\n        graph.forwardLinks[fromNode][fromStrand].append((nodeID+1,'-'))\n\n    for fromNode,fromStrand in negativeStrand:\n        graph.forwardLinks[fromNode][fromStrand].remove((nodeID+1,'-'))\n        graph.forwardLinks[fromNode][fromStrand].append((nodeID+1,'+'))\n\n\nnodeLengths = calcNodeLengths(graph)\npathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n\nfor nodeID in range(len(graph.nodes)):\n    if graph.nodeStrandPaths[nodeID][1]>graph.nodeStrandPaths[nodeID][0]:\n        print(f'Node {nodeID+1} inverted')\n        invertNode(nodeID,graph,pathNodeArray)\n\ngraph._pathCount()\n\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths"
  },
  {
    "objectID": "debruijngraphprocessing.html#removing-overlaps",
    "href": "debruijngraphprocessing.html#removing-overlaps",
    "title": "Processing GFA produced by CuttleFish compacted de Bruijn graph constructor",
    "section": "Removing overlaps",
    "text": "Removing overlaps\n\nRemoving nodes functionality\n\n\nFinding and clearing overlaps\n\ngraph.treeSort()\n\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 3\n\n\n\nrevertLinks = graph._revertLinks(graph.forwardLinks)\n\n\n# Do we need it here or when we are going to remove nodes?\nnodeLengths = calcNodeLengths(graph)\npathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\n\n\n\nkmerOverlap = 30\n\n\nnodeIDsToRemove = []\nleftCut = {}\nrightCut = {}\n\n\ngraph.nodes,graph.nodeNameToID,graph.forwardLinks,revertLinks,nodeLengths,graph.order\n\n(['2887', '4954', '3502'],\n {'2887': 1, '4954': 2, '3502': 3},\n {1: {'+': [(2, '+')]}, 2: {'+': [(2, '+'), (3, '+')]}},\n {2: {'+': [(1, '+'), (2, '+')]}, 3: {'+': [(2, '+')]}},\n [3000, 3000, 3000],\n [1, 2, 3])\n\n\n\ndef linkBounce(fromNodeStart,fromStrandStart,forwardLinks,revertLinks,nodeLengths,kmerOverlap,cutOffsetRight,leftCut,rightCut):\n    cutOffsetLeft = 0\n    leftToCut = [(fromNodeStart,fromStrandStart)]\n    rightToCut = []\n    leftToProcess = [(fromNodeStart,fromStrandStart)]\n    rightToProcess = []\n#     pdb.set_trace()\n    while (len(leftToProcess)>0 or len(rightToProcess)>0):\n        for node,strand in leftToProcess:\n            rightSide = graph.forwardLinks.get(node,{}).get(strand,[])\n            for toNode,toStrand in rightSide:\n                if toStrand=='+':\n                    cutOffsetLeft = max(leftCut.get(toNode,0),cutOffsetLeft)\n                else:\n                    cutOffsetLeft = max(rightCut.get(toNode,0),cutOffsetLeft)\n\n                if cutOffsetLeft<kmerOverlap-cutOffsetRight and (toNode,toStrand) not in rightToCut and nodeLengths[toNode-1]>0:\n                    rightToCut.append((toNode,toStrand))\n                    rightToProcess.append((toNode,toStrand))\n        leftToProcess = []\n\n        for node,strand in rightToProcess:\n            leftSide = revertLinks.get(node,{}).get(strand,[])\n            for fromNode,fromStrand in leftSide:\n                if fromStrand=='+':\n                    cutOffsetRight = max(rightCut.get(fromNode,0),cutOffsetRight)\n                else:\n                    cutOffsetRight = max(leftCut.get(fromNode,0),cutOffsetRight)\n\n                if cutOffsetRight<kmerOverlap-cutOffsetLeft and (fromNode,fromStrand) not in leftToCut and nodeLengths[fromNode-1]>0:\n                    leftToCut.append((fromNode,fromStrand))\n                    leftToProcess.append((fromNode,fromStrand))\n        rightToProcess = []\n        \n    return leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight\n\n\ndef processBouncedLink(leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight,graph,nodeLengths,leftCut,rightCut,):\n    toCut = kmerOverlap - cutOffsetLeft - cutOffsetRight\n    minLengthRight = min([nodeLengths[node-1] for node,strand in rightToCut])\n    minLengthLeft = min([nodeLengths[node-1] for node,strand in leftToCut])\n    if minLengthRight<minLengthLeft:\n        sideToCut = rightToCut\n        sideToKeep = leftToCut\n        globalPositiveSide = leftCut\n        globalNegativeSide = rightCut\n        side = 'right'\n    else:\n        sideToCut = leftToCut\n        sideToKeep = rightToCut\n        globalPositiveSide = rightCut\n        globalNegativeSide = leftCut\n        side = 'left'\n\n    maxCutAdjustment = max([max(toCut - nodeLengths[node-1],0) for node,strand in sideToCut])\n    for node,strand in sideToCut:\n        if strand=='+':\n            if side =='right':\n                graph.nodesData[node-1] = graph.nodesData[node-1][toCut-maxCutAdjustment:]\n            else:\n                graph.nodesData[node-1] = graph.nodesData[node-1][:-(toCut-maxCutAdjustment)]\n            globalPositiveSide[node] = globalPositiveSide.get(node,0) + toCut - maxCutAdjustment\n        else:\n            if side =='left':\n                graph.nodesData[node-1] = graph.nodesData[node-1][toCut-maxCutAdjustment:]\n            else:\n                graph.nodesData[node-1] = graph.nodesData[node-1][:-(toCut-maxCutAdjustment)]\n            globalNegativeSide[node] = globalNegativeSide.get(node,0) + toCut - maxCutAdjustment\n        nodeLengths[node-1] -= toCut - maxCutAdjustment\n\n    if maxCutAdjustment>0:\n        # adjust the other side as well.\n        for node,strand in sideToKeep:\n            if strand=='+':\n                if side =='right':\n                    graph.nodesData[node-1] = graph.nodesData[node-1][:-(maxCutAdjustment)]\n                else:\n                    graph.nodesData[node-1] = graph.nodesData[node-1][maxCutAdjustment:]\n                globalNegativeSide[node] = globalNegativeSide.get(node,0) + maxCutAdjustment\n            else:\n                if side =='left':\n                    graph.nodesData[node-1] = graph.nodesData[node-1][:-(maxCutAdjustment)]\n                else:\n                    graph.nodesData[node-1] = graph.nodesData[node-1][maxCutAdjustment:]\n                globalPositiveSide[node] = globalPositiveSide.get(node,0) + maxCutAdjustment\n            nodeLengths[node-1] -= maxCutAdjustment\n\n\nfor fromNodeStart in graph.order:\n    print('------')\n    print(f'Procesing node {fromNodeStart}')\n    if fromNodeStart not in graph.forwardLinks:\n        print(f'No links originates from node {fromNodeStart}')\n        continue\n    cutOffsetRight = rightCut.get(fromNodeStart,0)\n    cutOffsetLeft = 0\n    if cutOffsetRight<kmerOverlap and '+' in graph.forwardLinks[fromNodeStart] and nodeLengths[fromNodeStart-1]>0:\n        leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight = linkBounce(fromNodeStart,'+',graph.forwardLinks,revertLinks,nodeLengths,kmerOverlap,cutOffsetRight,leftCut,rightCut)\n        \n        print(f'Processing positive \"+\" strand of node {fromNodeStart}')\n        print('leftToCut')\n        print(leftToCut)\n        print('rightToCut')\n        print(rightToCut)\n        print(f'cutOffsetLeft: {cutOffsetLeft}; cutOffsetRight: {cutOffsetRight}')\n\n        processBouncedLink(leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight,graph,nodeLengths,leftCut,rightCut,)\n        print('leftCut')\n        print(leftCut)\n        print('rightCut')\n        print(rightCut)\n        \n    cutOffsetRight = leftCut.get(fromNodeStart,0)\n    if cutOffsetRight<kmerOverlap and '-' in graph.forwardLinks[fromNodeStart] and nodeLengths[fromNodeStart-1]>0:\n        leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight = linkBounce(fromNodeStart,'-',graph.forwardLinks,revertLinks,nodeLengths,kmerOverlap,cutOffsetRight,leftCut,rightCut)\n    \n        print(f'Processing positive \"-\" strand of node {fromNodeStart}')\n\n        print('leftToCut')\n        print(leftToCut)\n        print('rightToCut')\n        print(rightToCut)\n        print(f'cutOffsetLeft: {cutOffsetLeft}; cutOffsetRight: {cutOffsetRight}')\n\n        processBouncedLink(leftToCut,rightToCut,cutOffsetLeft,cutOffsetRight,graph,nodeLengths,leftCut,rightCut,)\n        print('leftCut')\n        print(leftCut)\n        print('rightCut')\n        print(rightCut)\n\n------\nProcesing node 1\nProcessing positive \"+\" strand of node 1\nleftToCut\n[(1, '+')]\nrightToCut\n[(4, '+'), (2, '+')]\ncutOffsetLeft: 0; cutOffsetRight: 0\nleftCut\n{4: 30, 2: 30}\nrightCut\n{}\n------\nProcesing node 4\nProcessing positive \"+\" strand of node 4\nleftToCut\n[(4, '+')]\nrightToCut\n[(3, '+')]\ncutOffsetLeft: 0; cutOffsetRight: 0\nleftCut\n{4: 30, 2: 30, 3: 1}\nrightCut\n{4: 29}\n------\nProcesing node 2\nProcessing positive \"+\" strand of node 2\nleftToCut\n[(2, '+')]\nrightToCut\n[(3, '-')]\ncutOffsetLeft: 0; cutOffsetRight: 0\nleftCut\n{4: 30, 2: 30, 3: 1}\nrightCut\n{4: 29, 2: 29, 3: 1}\n------\nProcesing node 3\nProcessing positive \"+\" strand of node 3\nleftToCut\n[(3, '+')]\nrightToCut\n[(7, '+')]\ncutOffsetLeft: 0; cutOffsetRight: 1\nleftCut\n{4: 30, 2: 30, 3: 1, 7: 29}\nrightCut\n{4: 29, 2: 29, 3: 1}\nProcessing positive \"-\" strand of node 3\nleftToCut\n[(3, '-')]\nrightToCut\n[(5, '+')]\ncutOffsetLeft: 0; cutOffsetRight: 1\nleftCut\n{4: 30, 2: 30, 3: 1, 7: 29, 5: 29}\nrightCut\n{4: 29, 2: 29, 3: 1}\n------\nProcesing node 7\nProcessing positive \"+\" strand of node 7\nleftToCut\n[(7, '+'), (5, '+')]\nrightToCut\n[(6, '+')]\ncutOffsetLeft: 0; cutOffsetRight: 0\nleftCut\n{4: 30, 2: 30, 3: 1, 7: 29, 5: 29}\nrightCut\n{4: 29, 2: 29, 3: 1, 7: 30, 5: 30}\n------\nProcesing node 5\n------\nProcesing node 6\nNo links originates from node 6\n\n\n\nnodeIDsToRemove = [i for i,l in enumerate(nodeLengths) if l==0]\n\n\nleftCut,rightCut,graph.forwardLinks,nodeLengths,nodeIDsToRemove\n\n({4: 30, 2: 30, 3: 1, 7: 29, 5: 29},\n {4: 29, 2: 29, 3: 1, 7: 30, 5: 30},\n {1: {'+': [(4, '+'), (2, '+')]},\n  2: {'+': [(3, '-')]},\n  3: {'-': [(5, '+')], '+': [(7, '+')]},\n  4: {'+': [(3, '+')]},\n  5: {'+': [(6, '+')]},\n  7: {'+': [(6, '+')]}},\n [3000, 0, 3000, 0, 0, 3000, 0],\n [1, 3, 4, 6])\n\n\n\n\nRemoving empty nodes\n\ndef removePositionsFromPaths(graph,pathIDs,positions):\n    offsetList = np.zeros(len(graph.paths),dtype=int)\n    for pathID,pos in zip(pathIDs,positions):\n        del graph.paths[pathID][pos-offsetList[pathID]]\n        offsetList[pathID] += 1\n\n\ndef updateLinkList(strandList,offsetDict):\n    newStrand = []\n    for toNode,toStrand in strandList:\n        newStrand.append((offsetDict[toNode-1]+1,toStrand))\n    return newStrand\n\n\ndef clearNodes(graph,nodeIDs):\n    offsetDict = {node:node for node in range(len(graph.nodes))}\n    for nodeID in nodeIDs:\n        updatedNodeID = offsetDict[nodeID]\n        nodeName = graph.nodes[updatedNodeID]\n        del graph.nodeNameToID[nodeName]\n        del graph.nodes[updatedNodeID]\n        del graph.nodesData[updatedNodeID]\n        del offsetDict[nodeID]\n        offsetDict.update({k:v-1 for k,v in offsetDict.items() if k>nodeID})\n    \n    graph.nodeNameToID.update({k:offsetDict[v-1]+1 for k,v in graph.nodeNameToID.items()})\n    \n    for pathID in range(len(graph.paths)):\n        graph.paths[pathID] = [f'{offsetDict[int(el[:-1])-1]+1}{el[-1]}' for el in graph.paths[pathID]]\n    \n    nodesInForwardLinks = list(graph.forwardLinks.keys())\n    for node in nodesInForwardLinks:\n        nodeDict = graph.forwardLinks[node]\n        newNode = offsetDict[node-1]+1\n        if node!=newNode:\n            del graph.forwardLinks[node]\n        forwardStrand = updateLinkList(nodeDict.get('+',[]),offsetDict)\n        inverseStrand = updateLinkList(nodeDict.get('-',[]),offsetDict)\n\n        if len(forwardStrand)>0:\n            graph.forwardLinks.setdefault(newNode,{})['+'] = forwardStrand\n        if len(inverseStrand)>0:\n            graph.forwardLinks.setdefault(newNode,{})['-'] = inverseStrand\n\n\ndef removeNode(nodeID,graph,revertLinks,pathNodeArray):\n    pathIDs,positions = np.where(pathNodeArray==nodeID+1)\n    \n#     pdb.set_trace()\n    \n    excludeTuples = [(nodeID+1,'+'),(nodeID+1,'-')]\n    \n    nodeToLink = revertLinks.get(nodeID+1,{})\n    toPositiveStrand = nodeToLink.get('+',[])\n    toNegativeStrand = nodeToLink.get('-',[])\n    \n    nodeFromLink = graph.forwardLinks.get(nodeID+1,{})\n    fromPositiveStrand = nodeFromLink.get('+',[])\n    fromNegativeStrand = nodeFromLink.get('-',[])\n\n    for fromNode,fromStrand in toPositiveStrand:\n        # Amending forwardLinks for positive strand of current node\n        forwardNodeStrand = graph.forwardLinks.get(fromNode,{}).get(fromStrand,[])\n        forwardNodeStrand = list(set(forwardNodeStrand).union(fromPositiveStrand).difference(excludeTuples))\n        graph.forwardLinks[fromNode][fromStrand] = forwardNodeStrand\n    \n    for fromNode,fromStrand in toNegativeStrand:\n        # Amend forwardLinks for negative strand of current node\n        forwardNodeStrand = graph.forwardLinks.get(fromNode,{}).get(fromStrand,[])\n        forwardNodeStrand = list(set(forwardNodeStrand).union(fromNegativeStrand).difference(excludeTuples))\n        graph.forwardLinks[fromNode][fromStrand] = forwardNodeStrand\n        \n    for toNode,toStrand in fromPositiveStrand:\n        # Amending revertLinks for positive strand of current node\n        revertNodeStrand = revertLinks.get(toNode,{}).get(toStrand,[])\n        revertNodeStrand = list(set(revertNodeStrand).union(toPositiveStrand).difference(excludeTuples))\n        revertLinks[toNode][toStrand] = revertNodeStrand\n    \n    for toNode,toStrand in fromNegativeStrand:\n        # Amend revertLinks for negative strand of current node\n        revertNodeStrand = revertLinks.get(toNode,{}).get(toStrand,[])\n        revertNodeStrand = list(set(revertNodeStrand).union(toNegativeStrand).difference(excludeTuples))\n        revertLinks[toNode][toStrand] = revertNodeStrand\n    \n    del revertLinks[nodeID+1]\n    del graph.forwardLinks[nodeID+1]\n    \n    return pathIDs,positions\n\n\ndef _removeNodes(self,nodeIDsToRemove)\n    # That is how deleting of nodes should happen.\n    pathIDs = []\n    positions = []\n    for nodeID in nodeIDsToRemove:\n        pID,pos = removeNode(nodeID,self,revertLinks,pathNodeArray)\n        pathIDs.extend(pID)\n        positions.extend(pos)\n\n    removePositionsFromPaths(self,pathIDs,positions)\n    clearNodes(self,nodeIDsToRemove)\n\n    self._pathCount()\n\n\ngraph.treeSort()\n\nConstructing Tremaux tree\nDone!\nGetting root nodes\nStart Loop...\n0 - 0 - 1 - 1 - 2\n0 - 1 - 2 - 2 - 3\n\n\n\n# Do we need it here or when we are going to remove nodes?\nnodeLengths = calcNodeLengths(graph)\npathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n\nCalculating nodes length...\nProcessing node 3/3\nFinished calculating nodes lengths\nPreprocessing paths...\nProcessing path 2/2\nFinished preprocessing paths\n\n\n\ngraph.nodes,graph.nodeNameToID,graph.forwardLinks,revertLinks,nodeLengths,graph.order\n\n(['1612', '1087', '2264'],\n {'1612': 1, '1087': 2, '2264': 3},\n {1: {'+': [(2, '-'), (2, '+')]}, 2: {'+': [(3, '+')], '-': [(3, '+')]}},\n {3: {'-': [(1, '+')], '+': [(1, '+')]}, 6: {'+': [(3, '-'), (3, '+')]}},\n [3000, 3000, 3000],\n [1, 2, 3])\n\n\n\ngraph.nodes,graph.nodeNameToID,graph.forwardLinks,revertLinks,nodeLengths,graph.order\n\n(['1612', '1087', '2264'],\n {'1612': 1, '1087': 2, '2264': 3},\n {1: {'+': [(2, '-'), (2, '+')]}, 2: {'+': [(3, '+')], '-': [(3, '+')]}},\n {3: {'-': [(1, '+')], '+': [(1, '+')]}, 6: {'+': [(3, '-'), (3, '+')]}},\n [3000, 3000, 3000],\n [1, 2, 3])"
  }
]