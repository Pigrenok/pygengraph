{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7b547a9d",
   "metadata": {},
   "source": [
    "---\n",
    "description: Provides functionality to export graph to Pantograph data storage.\n",
    "output-file: export.html\n",
    "title: Export module\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3abd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa6934",
   "metadata": {},
   "source": [
    "## Imports and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "from redis import ResponseError, Redis\n",
    "\n",
    "from pygengraph.utils import pathConvert,NpEncoder,adjustZoomLevels\n",
    "from pygengraph.utils import iset_add\n",
    "from pygengraph.graph import GenomeGraph\n",
    "from pygengraph.graph import initialPathAnalysis,calcNodeLengths\n",
    "from pygengraph.graph import getNodesStructurePathNodeInversionRate,getNextNodePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc714621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Here are three templates: for component, chunk and main case data structure. It is not a full possible structure, some extra attributes\n",
    "# can be added during exporting graph.\n",
    "componentTemplate = {\n",
    "    \"first_bin\": 1,\n",
    "    \"last_bin\": 1,\n",
    "    \"firstCol\": 1,\n",
    "    \"lastCol\": 1,\n",
    "    \"occupants\": [],\n",
    "    \"matrix\": [],\n",
    "    \"larrivals\": [],\n",
    "    \"rarrivals\": [],\n",
    "    \"ldepartures\": [],\n",
    "    \"rdepartures\": [],\n",
    "    \"x\": 0\n",
    "}\n",
    "\n",
    "chunkTemplate = {\n",
    "    \"json_version\":19,\n",
    "    \"first_bin\": 1,\n",
    "    \"first_col\": 1,\n",
    "    \"last_bin\": 1,\n",
    "    \"last_col\": 1,\n",
    "    \"includes_connectors\": True,\n",
    "    \"components\": [],\n",
    "    \"compVisCol\": {}\n",
    "}\n",
    "\n",
    "rootStructTemplate = {\n",
    "    \"json_version\": 19,\n",
    "    \"pangenome_length\": 0,#pangenomeLength, #total number of bp in pangenome\n",
    "    \"includes_connectors\": True,\n",
    "    \"zoom_levels\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba18c2",
   "metadata": {},
   "source": [
    "## Functions intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf3007",
   "metadata": {},
   "source": [
    "**Notation and terminology**\n",
    "\n",
    "In documentation, we refer to graph nucleotides, columns and components. Components contain columns and columns contain nucleotides.\n",
    "\n",
    "In the code variable names and comments use slightly different notation. Columns in documentation are bins in code and comments, whereas graph nucleotides in documentation are called columns in the code and comments. This happened for the legacy reasons, i.e. originally there was no nucleotide numbers (columns) in the visualised graph structure and components were split into bins (literally, equal sized bins). It is not true anymore, but old terminology left here.\n",
    "\n",
    "Ideally all variable names and comments should be changes in line with documentation notation, but I have no idea when this can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95390ac7",
   "metadata": {},
   "source": [
    "For various operational or legacy reasons, some of the data structures (usually, lists/array) use 0-based indexing, whereas some others\n",
    "(usually dicts) can be 0-based or 1-based. Here are the main structures with numerical indexing and their index bases:\n",
    "\n",
    "- components: keys: 0-based, values: occupants: 0-based, binNumbers: 0-based  \n",
    "- componentToNode: keys: 0-based, values: 1-based  \n",
    "- nodeToComponent: keys: 0-based, values: 1-based  \n",
    "- newToOldInd and oldToNewInd: both index and values are 0-based numbers of components in previous and current zoomlayer.  \n",
    "- fromLinks: top level keys (from nodes): 1-based, bottom level keys (to nodes): 1-based, values (list of participants): 0-based  \n",
    "- toLinks: top level keys (to nodes): 1-based, bottom level keys (from nodes): 1-based, values (list of participants): 0-based  \n",
    "- fromComponentLinks: top level keys (from components): 1-based, bottom level keys (to components): 1-based, values (set of participants): 0-based  \n",
    "- toComponentLinks: top level keys (to components): 1-based, bottom level keys (from components): 1-based, values (set of participants): 0-based  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0161a0",
   "metadata": {},
   "source": [
    "## Generating base layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bbb30-531f-4794-a2d0-7f9cf7563568",
   "metadata": {},
   "source": [
    "This set of functions generate the data structures for initial, lowest level zoom (nucleotide or minimum unit resolution). The main orchestration \n",
    "function is `baseLayerZoom`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e5f62-c237-4e39-8ca3-a23c5e8867c1",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154886e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def outLeftRight(nodeInversionInPath,leftFarLink,rightFarLink,reason,debug=False,inversionThreshold=0.5):\n",
    "    \n",
    "    if (nodeInversionInPath<=inversionThreshold):\n",
    "        rightFarLink = True\n",
    "    else:\n",
    "        leftFarLink = True\n",
    "        \n",
    "    return leftFarLink,rightFarLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recordLinks(nodeIdx,nextNode,pathID,step,nodeInversionInPath,nonLinearCond,pathNodeArray,fromLinks,toLinks,debug=False,inversionThreshold=0.5):\n",
    "    leftFarLink = False\n",
    "    rightFarLink = False\n",
    "    if nonLinearCond:\n",
    "        if debug:\n",
    "            print(f'Non-linear link for path {pathID} from node {nodeIdx} to node {nextNode}')\n",
    "\n",
    "        fromLinks.setdefault(nodeIdx,{}).setdefault(nextNode,[]).append(pathID)\n",
    "        toLinks.setdefault(nextNode,{}).setdefault(nodeIdx,[]).append(pathID)\n",
    "        leftFarLink,rightFarLink = outLeftRight(nodeInversionInPath,leftFarLink,rightFarLink,'backward link',debug=debug,inversionThreshold=0.5)\n",
    "            \n",
    "    else:\n",
    "        if np.any([node in pathNodeArray[pathID,:] for node in range(nodeIdx+1*step,nextNode,step)]):\n",
    "            if debug:\n",
    "                print(f'Jump link for path {pathID} from node {nodeIdx} to node {nextNode}')\n",
    "\n",
    "            fromLinks.setdefault(nodeIdx,{}).setdefault(nextNode,[]).append(pathID)\n",
    "            toLinks.setdefault(nextNode,{}).setdefault(nodeIdx,[]).append(pathID)\n",
    "            \n",
    "            leftFarLink,rightFarLink = outLeftRight(nodeInversionInPath,leftFarLink,rightFarLink,'forward jumping link',debug=debug,inversionThreshold=0.5)\n",
    "        else:\n",
    "            startNode = None\n",
    "            endNode = None\n",
    "\n",
    "            if step==-1:\n",
    "                startNode = nodeIdx\n",
    "                endNode = nextNode\n",
    "                if debug:\n",
    "                    print(f'Intermediate nodes for path {pathID} from node {nodeIdx} to node {nextNode}')\n",
    "\n",
    "            elif step==1:\n",
    "                startNode = nodeIdx\n",
    "                endNode = nextNode\n",
    "                if debug:\n",
    "                    print(f'Intermediate nodes for path {pathID} from node {nodeIdx} to node {nextNode}')\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f'Jump link for path {pathID} from node {nodeIdx} to node {nextNode}')\n",
    "\n",
    "                fromLinks.setdefault(nodeIdx,{}).setdefault(nextNode,[]).append(pathID)\n",
    "                toLinks.setdefault(nextNode,{}).setdefault(nodeIdx,[]).append(pathID)\n",
    "                leftFarLink,rightFarLink = outLeftRight(nodeInversionInPath,leftFarLink,rightFarLink,'forward jumping link',debug=debug,inversionThreshold=0.5)\n",
    "\n",
    "            if startNode is not None and endNode is not None and step is not None:\n",
    "                for intermediateNodeIdx in range(startNode,endNode,step):\n",
    "                    if debug:\n",
    "                        print(f'Adding link from node {intermediateNodeIdx} to node {intermediateNodeIdx+1*step} for path {pathID}')\n",
    "                    fromLinks.setdefault(intermediateNodeIdx,{}).setdefault(intermediateNodeIdx+1*step,[]).append(pathID)\n",
    "                    toLinks.setdefault(intermediateNodeIdx+1*step,{}).setdefault(intermediateNodeIdx,[]).append(pathID)\n",
    "    return leftFarLink,rightFarLink,fromLinks,toLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214452b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkForBreak(nodeIdx,nodeLen,\n",
    "                  nodePathsIdx,nodeSeqInPath,\n",
    "                  uniqueNodePathsIDs,pathNodeCount,\n",
    "                  pathLengths,\n",
    "                  pathNodeArray,pathDirArray,\n",
    "                  occupancy,inversion,\n",
    "                  fromLinks,toLinks,\n",
    "                  nBins,maxLengthComponent,\n",
    "                  blockEdges,\n",
    "                  inversionThreshold=0.5,\n",
    "                  debug=False):\n",
    "    '''\n",
    "    Function to check whether the component should be broken before (left) and/or after (right) it.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `nodeIdx`: int. Numerical (1 based) node ID within the graph (consecutive number of the node)\n",
    "               in the graph order.\n",
    "    \n",
    "    #######\n",
    "    The following 5 parameters are calculated in `nodeStat` function externally and passed here:\n",
    "    `nodeLen`: int. Length of the node in nucleotides\n",
    "    `nodePathsIdx`: numpy.array. A 1D numpy.array which contain ids of the paths \n",
    "                    for each appearance of the node ID in each path.\n",
    "    `nodeSeqInPath`: numpy.array. A 1D numpy.array with positions of given node in every path. \n",
    "                     The path to which this position is related to can be found from `nodePathsIdx`, \n",
    "                     e.g. if `nodeSeqInPath[i]` is `k`, then node with node ID `nodeIdx`\n",
    "                     appear in path `nodePathsIdx[i]` at position `k`.\n",
    "    `uniqueNodePathsIDs`: Iterable[int]. Usually a list or 1D numpy.array holding unique IDs of paths \n",
    "                          which pass this node at least once.\n",
    "    `pathNodeCount`: numpy.array.: A 1D array (the same length as `uniqueNodePathsIDs`) which \n",
    "                     contains number of times the node appear in each path from `uniqueNodePathsIDs`,\n",
    "                     e.g. if `pathNodeCount[i]` is `k` then node with ID nodeIdx appears in path\n",
    "                     `uniqueNodePathsIds[i]` `k` times.\n",
    "    #######\n",
    "    \n",
    "    `occupancy`: dict. A dictionary containing occupancy (number of times given path passed the given \n",
    "                 node) of the previous node (in terms of graph order) for each path than passed \n",
    "                 through previous node. The dictionary has path IDs as keys.\n",
    "    `inversion`: dict. A dictionary containing inversion (fraction of copies of the node in the path \n",
    "                 that are inverted) of the previous node (in terms of graph order) for each path than \n",
    "                 passed through previous node. The dictionary has path IDs as keys.\n",
    "    `fromLinks`: dict[dict[list]]. Dictionary which holds links from each processed node.\n",
    "    `toLinks`: dict[dict[list]]. Dictionary which holds links to each processed node.\n",
    "    `nBins`: int. Number of bins recorded in the current component.\n",
    "    `maxLengthComponent`: int. Maximum number of bins allowed in a component. Used to break too \n",
    "                          long components into smaller more manageable blocks.\n",
    "    `pathDirArray`: 2D numpy.array. Each row (corresponding to specific path in graph) contains directionality\n",
    "                    of each node in the path. See `initialPathAnalysis` for details.\n",
    "    `inversionThreshold`: float (default: 0.5). A threshold after which the node is considered inverted.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    `leftFarLink`: bool. Shows whether there is a far link on the left that will require component break.\n",
    "    `rightFarLink`: bool. Shows whether there is a far link on the right that will require component break.\n",
    "    `leftPathStartEnd`: bool. Shows whether there is a start or end of the path on the left of the node.\n",
    "    `rightPathStartEnd`: bool. Shows whether there is a start or end of the path on the right of the node.\n",
    "    `isChangedOccupancy`: bool. Shows whether the occupancy of at least one path significantly changed \n",
    "                          from previous node. If previous node was missing, and current node has any \n",
    "                          occupancy, it will be False as it does not constitute a component break.\n",
    "    `isChangeInversion`: bool. Shows whether the inversion of at least one path significantly changed \n",
    "                          from previous node.\n",
    "    `isNotFitIntoComponent`: bool. Shows whether this node won't fit into the currently forming component\n",
    "                             and it should be broken before the current node.\n",
    "    `pathStarts`: list. Contains list of path IDs which start from this node.\n",
    "    `pathEnds`: list. Contains list of path IDs which end on this node.\n",
    "    `fromLinks`: dict[dict[list]]\n",
    "    `toLinks`: \n",
    "    `occupancy`: dict. A dictionary containing occupancy (number of times given path passed the given \n",
    "                 node) of the current node (in terms of graph order) for each path than passed \n",
    "                 through previous node. The dictionary has path IDs as keys.\n",
    "    `inversion`: dict. A dictionary containing inversion (fraction of copies of the node in the path \n",
    "                 that are inverted) of the current node (in terms of graph order) for each path than \n",
    "                 passed through previous node. The dictionary has path IDs as keys.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Boolean indicator of far incoming link on the left and right\n",
    "    leftFarLink = False\n",
    "    rightFarLink = False\n",
    "\n",
    "    # Boolean if at least one path starts from this node and start is on the left or right\n",
    "    leftPathStart = False\n",
    "    rightPathStart = False\n",
    "    # Boolean if at least one path ends on this node and end is on the left or right\n",
    "    leftPathEnd = False\n",
    "    rightPathEnd = False\n",
    "\n",
    "    # Indicator whether structure (inversion and occupancy combination over all paths) changed from previous node\n",
    "    isChangedStructure = nodeIdx-1 in blockEdges\n",
    "    \n",
    "    # Indicator whether this node does not fit into current component.\n",
    "    isNotFitIntoComponent = False\n",
    "\n",
    "    pathEnds = np.sort(np.unique([pathIdx for pathIdx,nodeInSeq in zip(nodePathsIdx,nodeSeqInPath) if nodeInSeq==pathLengths[pathIdx]-1]))\n",
    "    pathStarts = np.sort(np.unique([pathIdx for pathIdx,nodeInSeq in zip(nodePathsIdx,nodeSeqInPath) if nodeInSeq==0]))\n",
    "    \n",
    "    invertedEnds = set()\n",
    "    invertedStarts = set()\n",
    "    \n",
    "    # Check whether adding this node to the current component will violate the limitation\n",
    "    # for maximum component length. Is it still needed with proper treatment of partially\n",
    "    # visible components in the front-end???\n",
    "    if nBins+nodeLen>maxLengthComponent:\n",
    "        isNotFitIntoComponent = True\n",
    "    \n",
    "    # Check whether component should be broken before current node.\n",
    "    for j, pathID in enumerate(uniqueNodePathsIDs):\n",
    "        \n",
    "        # Calculate occupancy of current node for path\n",
    "        # It is simply how many times this node is passed by current path.\n",
    "        occupancy[pathID] = pathNodeCount[j]\n",
    "        \n",
    "        # Calculate inversion rate of current node for path\n",
    "        # It is ratio of how many times the node is passed in reversed direction\n",
    "        # by the path to overall occupancy of the node by the path.\n",
    "        nodePositions = np.where(nodePathsIdx==pathID)[0]\n",
    "        \n",
    "        nodeInversionInPath = np.sum([int(pathDirArray[pathID,nodeNumInPath]) \\\n",
    "                          for nodeNumInPath in nodeSeqInPath[nodePositions]])\\\n",
    "                          /occupancy[pathID]\n",
    "        \n",
    "        # Record current inversion for current pathID for bins generation.\n",
    "        inversion[pathID] = nodeInversionInPath\n",
    "\n",
    "        # Get indexes of positions of the node in the path\n",
    "        \n",
    "        \n",
    "        # Check whether the node \n",
    "        if nodeInversionInPath<=inversionThreshold:\n",
    "            # not inverted node\n",
    "            leftPathStart = leftPathStart or (pathID in pathStarts)\n",
    "            rightPathEnd = rightPathEnd or (pathID in pathEnds)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f'Node {nodeIdx}, accession {graph.accessions[pathID]}, inversion {nodeInversionInPath}')\n",
    "            # inverted node\n",
    "            rightPathStart = rightPathStart or (pathID in pathStarts)\n",
    "            \n",
    "            if pathID in pathStarts:\n",
    "                invertedStarts.add(pathID)\n",
    "            \n",
    "            leftPathEnd = leftPathEnd or (pathID in pathEnds)\n",
    "            \n",
    "            if pathID in pathEnds:\n",
    "                invertedEnds.add(pathID)\n",
    "        \n",
    "        for nodePos in nodePositions:\n",
    "            # Boolean flags to whether check and record incoming and outgoing links\n",
    "            checkIn = False\n",
    "            checkOut = False\n",
    "            # Get the sequencial number of node in the path (for current passing), can be several for duplicated nodes\n",
    "            nodePositionInPath = nodeSeqInPath[nodePos]\n",
    "            \n",
    "            if nodePositionInPath<(pathLengths[pathID]-1):\n",
    "                # if not last node in the path\n",
    "                checkOut = True\n",
    "                nextNode = pathNodeArray[pathID,nodePositionInPath+1]\n",
    "                nextNodePositions = np.where(pathNodeArray[pathID,:]==nextNode)[0]\n",
    "                nextNodeInversionInPath = np.sum([int(pathDirArray[pathID,nodeInPathPos]) for nodeInPathPos in nextNodePositions])/len(nextNodePositions)\n",
    "                if debug:\n",
    "                    print(f'Node {nodeIdx}, next node {nextNode}, path {pathID}, number in path {nodePositionInPath}, length of path {pathLengths[pathID]}')\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"Last node in path\")\n",
    "            \n",
    "            if nodePositionInPath>0:\n",
    "                checkIn = True\n",
    "                prevNode = pathNodeArray[pathID,nodePositionInPath-1]\n",
    "                prevNodePositions = np.where(pathNodeArray[pathID,:]==prevNode)[0]\n",
    "                prevNodeInversionInPath = np.sum([int(pathDirArray[pathID,nodeInPathPos]) for nodeInPathPos in prevNodePositions])/len(prevNodePositions)\n",
    "                if debug:\n",
    "                    print(f'Node {nodeIdx}, previous node {nextNode}, path {pathID}, number in path {nodePositionInPath}, length of path {pathLengths[pathID]}')\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"First node in path\")\n",
    "            \n",
    "            \n",
    "            if nodeInversionInPath<=inversionThreshold:\n",
    "            # Not inverted node\n",
    "            # For the node find the links going to the left of the node\n",
    "                if checkIn:\n",
    "                    # node is not the first\n",
    "                    if debug:\n",
    "                        print(f\"IncomingLinks: {prevNode} -> {nodeIdx}\")\n",
    "                        print(f'Existing known links: {fromLinks.get(prevNode,{})}')\n",
    "\n",
    "                    if prevNode+1!=nodeIdx:\n",
    "                        # Link in the path does not go between consecutive nodes (from previous to current)\n",
    "                        # This means that there is a links coming to the left of the node\n",
    "                        if (prevNode<nodeIdx and nodeIdx in fromLinks.get(prevNode,{})) or (prevNode>=nodeIdx):\n",
    "                            # If we are not picking up passing through empty blocks as far link\n",
    "                            # Tandem duplicates are included as they cause an arrow to the left of the node.\n",
    "                            leftFarLink = True\n",
    "                if checkOut:\n",
    "                    # node is not the last\n",
    "                    \n",
    "                    # Find out whether it is a non-linear (e.g. reversed link or link from inversed to non-inversed node).\n",
    "                    if nextNodeInversionInPath<=inversionThreshold:\n",
    "                        # The inversion is the same for two consecutive (in path) nodes\n",
    "                        # Link will be non-linear if previous node is staying further in linearised graph\n",
    "                        nonLinearCond = (nodeIdx>=nextNode)\n",
    "                        # Step for checking for jumping over link\n",
    "                        step = 1\n",
    "                    else:\n",
    "                        # The inversion is not the same for two consecutive nodes.\n",
    "                        nonLinearCond = True\n",
    "                        # Step is not used and assigned for consistency\n",
    "                        step =0\n",
    "                        \n",
    "                    # Process links from (!) this node.\n",
    "                    _leftFarLink,_rightFarLink,fromLinks,toLinks = \\\n",
    "                    recordLinks(nodeIdx,nextNode,pathID,\n",
    "                                step,nodeInversionInPath,\n",
    "                                nonLinearCond,\n",
    "                                pathNodeArray,\n",
    "                                fromLinks,toLinks,\n",
    "                                debug=debug,\n",
    "                                inversionThreshold=inversionThreshold)\n",
    "                    \n",
    "                    leftFarLink = leftFarLink or _leftFarLink\n",
    "                    rightFarLink = rightFarLink or _rightFarLink\n",
    "                            \n",
    "            else:\n",
    "                #Inverted node\n",
    "                # For the node find the links goint from the left of the node\n",
    "                if checkOut:\n",
    "                    # node is not the last\n",
    "                    # Find out whether it is a non-linear (e.g. reversed link or link from inversed to non-inversed node).\n",
    "                    if nextNodeInversionInPath>inversionThreshold:\n",
    "                        # The inversion is the same for two consecutive (in path) nodes\n",
    "\n",
    "                        # Link will be non-linear if previous node is staying further in linearised graph\n",
    "                        nonLinearCond  = (nodeIdx<=nextNode)\n",
    "                        # Step for checking for jumping over link\n",
    "                        step = -1\n",
    "                    else:\n",
    "                        # The inversion is not the same for two consecutive nodes.\n",
    "                        nonLinearCond = True\n",
    "                        # Step is not used and assigned for consistency\n",
    "                        step = 0\n",
    "                        \n",
    "                    _leftFarLink,_rightFarLink,fromLinks,toLinks = \\\n",
    "                    recordLinks(nodeIdx,nextNode,pathID,\n",
    "                                step,nodeInversionInPath,\n",
    "                                nonLinearCond,\n",
    "                                pathNodeArray,\n",
    "                                fromLinks,toLinks,\n",
    "                                debug=debug,\n",
    "                                inversionThreshold=inversionThreshold)\n",
    "                    \n",
    "                    leftFarLink = leftFarLink or _leftFarLink\n",
    "                    rightFarLink = rightFarLink or _rightFarLink\n",
    "                \n",
    "                if checkIn:\n",
    "                    # The node is not the first\n",
    "                    if debug:\n",
    "                        print(f\"IncomingLinks: {nodeIdx} -> {nextNode}\")\n",
    "                        print(f'Existing known links: {toLinks.get(nextNode,{})}')\n",
    "\n",
    "                    if nodeIdx-1!=nextNode:\n",
    "                        # Link does not go between consecutive blocks (from this to previous one)\n",
    "                        if (nodeIdx>nextNode and nodeIdx in toLinks.get(nextNode,{})) or (nodeIdx<=nextNode):\n",
    "                            # If we are not picking up passing through empty blocks as far link\n",
    "                            # Tandems duplicates are included as they also cause an arrow from the left of the node.\n",
    "                            rightFarLink = True\n",
    "        \n",
    "    return (leftFarLink,rightFarLink,\n",
    "            leftPathStart or leftPathEnd,\n",
    "            rightPathStart or rightPathEnd,\n",
    "            isChangedStructure,\n",
    "            isNotFitIntoComponent,\n",
    "            pathStarts,pathEnds,\n",
    "            fromLinks,toLinks,\n",
    "            occupancy,inversion,\n",
    "            invertedStarts,invertedEnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bead44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nodeStat(nodeIdx,pathNodeArray,nodeLengths):\n",
    "    '''\n",
    "    Function calculate information about node as part of the overall graph.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `nodeIdx`: int. Number of the node in the graph (1 based).\n",
    "    `pathNodeArray`: numpy.array. A 2D array (<number of paths> x <max lengths of path>), \n",
    "                     where each line present sequence of node IDs in a particular path.\n",
    "                     Path ordered as they are present in the graph.\n",
    "    `nodeLengths`: list or numpy.array. An 1D subscribable structure \n",
    "                   (normally, list or 1D numpy.array is expected) where each element i\n",
    "                   is the length of the node ID (1 based) i+1.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    \n",
    "    `nodePathsIdx`: numpy.array. A 1D numpy.array which contain ids of the paths \n",
    "                    for each appearance of the node ID in each path.\n",
    "    `nodeSeqInPath`: numpy.array. A 1D numpy.array with positions of given node in every path. \n",
    "                     The path to which this position is related to can be found from `nodePathsIdx`, \n",
    "                     e.g. if `nodeSeqInPath[i]` is `k`, then node with node ID `nodeIdx`\n",
    "                     appear in path `nodePathsIdx[i]` at position `k`.\n",
    "    `nodeLen`: int. Length of the node (in terms of nucleotides, real or simulated).\n",
    "    `uniqueNodePathsIDs`: numpy.array. A 1D array containing all path IDs (0 based, non-repeated) \n",
    "                          that contains the node.\n",
    "    `pathNodeCount`: numpy.array.: A 1D array (the same length as `uniqueNodePathsIDs`) which \n",
    "                     contains number of times the node appear in each path from `uniqueNodePathsIDs`,\n",
    "                     e.g. if `pathNodeCount[i]` is `k` then node with ID nodeIdx appears in path\n",
    "                     `uniqueNodePathsIds[i]` `k` times.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Find positions of the node in each path\n",
    "    nodePathsIdx, nodeSeqInPath = np.where(pathNodeArray == nodeIdx)\n",
    "    # Find node length\n",
    "    nodeLen = nodeLengths[nodeIdx-1]\n",
    "\n",
    "    # Find unique path IDs (numerical) and cound of given node in each path.\n",
    "    uniqueNodePathsIDs,pathNodeCount = np.unique(nodePathsIdx,return_counts=True)\n",
    "    \n",
    "    return nodePathsIdx,nodeSeqInPath,nodeLen,uniqueNodePathsIDs,pathNodeCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca34b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finaliseComponentBase(component,\n",
    "                      components,componentNucleotides,\n",
    "                      matrix,occupants,nBins,componentLengths,nucleotides,zoomLevel,accessions,inversionThreshold=0.5):\n",
    "        \n",
    "    componentLengths.append(nBins)\n",
    "    component[\"matrix\"].extend([[pathID,\n",
    "                                 int(matrixPathArray[1][0][1]>inversionThreshold),\n",
    "                                 matrixPathArray] \\\n",
    "                                for pathID,matrixPathArray in matrix.items()])\n",
    "    component['binsToCols'] = [1]*nBins\n",
    "    \n",
    "    component[\"occupants\"] = sorted(list(occupants))\n",
    "    component['last_bin'] = component['first_bin'] + nBins - 1\n",
    "    component['lastCol'] = component['firstCol'] + nBins - 1\n",
    "    component['binColStarts'] = list(range(component['firstCol'],component['lastCol']+1))\n",
    "    component['binColEnds'] = list(range(component['firstCol'],component['lastCol']+1))\n",
    "    if nucleotides!='':\n",
    "        componentNucleotides.append(nucleotides)\n",
    "    \n",
    "    firstBin = component['last_bin'] + 1\n",
    "    firstCol = component['lastCol'] + 1\n",
    "    components.append(component)\n",
    "    component = deepcopy(componentTemplate)\n",
    "    component['first_bin'] = firstBin\n",
    "    component['firstCol'] = firstCol\n",
    "    return component,components,componentNucleotides,{},set(),0,''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processAnnotationInterval(posStart,posEnd,annotation,res):\n",
    "    for pos in range(posStart,posEnd):\n",
    "        r = []\n",
    "        for name,intervals in annotation.items():\n",
    "            for interval in intervals:\n",
    "                if pos<=interval[1] and pos>=interval[0]:\n",
    "                    r.append(name)\n",
    "        res[pos] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d564e0c-de5f-419e-af78-58bc76a41ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def combineAnnotation(combAnnotation):\n",
    "    for accAnn in combAnnotation.values():\n",
    "        for annString,intList in accAnn.items():\n",
    "            accAnn[annString] = combineIntervals(intList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65287cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def updateEdges(accEdge,edgeAccessions,compNum):\n",
    "    '''\n",
    "    Function fills up either accStarts or accEnds (on which component each accession starts and \n",
    "    on which ends). `compNum` is assumed to be 1-based.\n",
    "    '''\n",
    "    \n",
    "    for accID in edgeAccessions:\n",
    "        accEdge[accID] = compNum\n",
    "        \n",
    "    return accEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363a007-2f21-4243-a779-bfe7749a2cd1",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982b148-ade3-47e5-b8ae-5a9f2e78d3ec",
   "metadata": {},
   "source": [
    "Now 'positions' key in metadata contains either one position (chr:posStart..posEnd) or two comma separated positions where one is genomic position, and another one is pangenomic position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def baseLayerZoom(graph,\n",
    "                  outputPath,outputName,\n",
    "                  pathNodeArray,pathDirArray,\n",
    "                  pathLengths,nodeLengths,\n",
    "                  pathNodeLengthsCum,\n",
    "                  maxLengthComponent,\n",
    "                  blockEdges,\n",
    "                  CPUS = cpu_count(),\n",
    "                  inversionThreshold=0.5,\n",
    "                  isSeq=True,\n",
    "                  debug=False,\n",
    "                  debugTime=False):\n",
    "\n",
    "    zoomLevel = 1\n",
    "    \n",
    "    print('\\n===========================')\n",
    "    print(f'Zoom level {zoomLevel}')\n",
    "    print('===========================')\n",
    "    zoom_level_struct = {}\n",
    "    zoom_level_struct[\"files\"] = []\n",
    "    \n",
    "    accStarts = {}\n",
    "    accEnds = {}\n",
    "    \n",
    "    invertedStarts = set()\n",
    "    invertedEnds = set()\n",
    "    \n",
    "    nodeToComponent = []\n",
    "    componentToNode = []\n",
    "    componentLengths = []\n",
    "    components = []\n",
    "    componentNucleotides = []\n",
    "\n",
    "    component = deepcopy(componentTemplate)\n",
    "    \n",
    "    numNodes = len(graph.nodes)\n",
    "    numNodesDigits = int(np.ceil(np.log10(numNodes)))\n",
    "    \n",
    "    fromLinks = {}\n",
    "    toLinks = {}\n",
    "    \n",
    "    fromComponentLinks = {}\n",
    "    toComponentLinks = {}\n",
    "    \n",
    "    occupants = set()\n",
    "    nucleotides = ''\n",
    "    matrix = {}\n",
    "\n",
    "    nodeLinks = []\n",
    "\n",
    "    nBins = 0\n",
    "    binLength = 0\n",
    "    occupancy = {}\n",
    "    inversion = {}\n",
    "    pos = {} #???\n",
    "    nodesInComp = set()\n",
    "    annotationNames = {}\n",
    "    \n",
    "    combAnnotation = {}\n",
    "    combGenPos = {}\n",
    "    combGenPosSearch = {}\n",
    "    combPangenPosSearch = {}\n",
    "    combAltChrGenPos = {}\n",
    "    annFirstBin = 0\n",
    "    \n",
    "    for nodeIdx in range(1,numNodes+1):\n",
    "        if debug or debugTime:\n",
    "            print(f'Processing node {nodeIdx:0{numNodesDigits}}/{numNodes:0{numNodesDigits}}')\n",
    "        else:\n",
    "            print(f'\\rProcessing node {nodeIdx:0{numNodesDigits}}/{numNodes:0{numNodesDigits}}',end='')\n",
    "        \n",
    "        if debugTime:\n",
    "            startNodeTime = time.time()\n",
    "        ######################\n",
    "        # Node preprocessing #\n",
    "        ######################\n",
    "        # Get some onfo about the node\n",
    "        nodePathsIdx,nodeSeqInPath,nodeLen,uniqueNodePathsIDs,pathNodeCount = \\\n",
    "        nodeStat(nodeIdx,pathNodeArray,nodeLengths)\n",
    "        \n",
    "        # Combining annotations\n",
    "        for accID,accNodeAnn in graph.nodesMetadata[nodeIdx-1].items():    \n",
    "            for annString,intervals in accNodeAnn['annotation'].items():\n",
    "                for interval in intervals:\n",
    "                    combAnnotation.setdefault(accID,{}).\\\n",
    "                    setdefault(annString,[]).\\\n",
    "                    append((interval[0]+annFirstBin,interval[1]+annFirstBin))\n",
    "            for genPos in accNodeAnn.get('positions',[]): #switch to 'positions' key and include pangenome coordinates\n",
    "                genposstring = ''\n",
    "                if 'gen' in genPos:\n",
    "                    genPosString += f'{genPos[\"gen\"][\"chr\"]}:{\"..\".join(map(str,genPos[\"gen\"][\"genomePosition\"]))}'\n",
    "                    combGenPosSearch.setdefault(accID,{}).setdefault(f'{annFirstBin}_{genPos[\"gen\"][\"chr\"]}',[]).append(genPos[\"gen\"][\"genomePosition\"])\n",
    "                if 'pangen' in genPos:\n",
    "                    genPosString += f'|{genPos[\"pangen\"][\"chr\"]}:{\"..\".join(map(str,genPos[\"pangen\"][\"genomePosition\"]))}'\n",
    "                    combPangenPosSearch.setdefault(accID,{}).setdefault(f'{annFirstBin}_{genPos[\"pangen\"][\"chr\"]}',[]).append(genPos[\"pangen\"][\"genomePosition\"])\n",
    "                combGenPos.setdefault(accID,{})[genPosString]=[(annFirstBin,annFirstBin+nodeLengths[nodeIdx-1]-1)]\n",
    "            for altGenPos in accNodeAnn.get('altChrGenPos',[]):\n",
    "                combAltChrGenPos.setdefault(accID,{})[f'{altGenPos[\"chr\"]}:{\"..\".join(map(str,altGenPos[\"genomePosition\"]))}']=[(annFirstBin,annFirstBin+nodeLengths[nodeIdx-1]-1)]\n",
    "        annFirstBin +=nodeLengths[nodeIdx-1]\n",
    "        \n",
    "        # Check whether the component should be broken before and/or after current node\n",
    "        (leftFarLink,rightFarLink,\n",
    "        leftPathStartEnd,\n",
    "        rightPathStartEnd,\n",
    "        isChangedStructure,\n",
    "        isNotFitIntoComponent,\n",
    "        pathStarts,pathEnds,\n",
    "        fromLinks,toLinks,\n",
    "        occupancy,inversion,\n",
    "        invertedCompStarts,invertedCompEnds) = checkForBreak(nodeIdx,nodeLen,\n",
    "                                             nodePathsIdx,nodeSeqInPath,\n",
    "                                             uniqueNodePathsIDs,pathNodeCount,\n",
    "                                             pathLengths,\n",
    "                                             pathNodeArray,pathDirArray,\n",
    "                                             occupancy,inversion,\n",
    "                                             fromLinks,toLinks,\n",
    "                                             nBins,maxLengthComponent,\n",
    "                                             blockEdges,\n",
    "                                             inversionThreshold=inversionThreshold,\n",
    "                                             debug=debug)\n",
    "        \n",
    "        invertedStarts.update(invertedCompStarts)\n",
    "        invertedEnds.update(invertedCompEnds)\n",
    "        \n",
    "        if debugTime:\n",
    "            print(f'Checking for breaks node {nodeIdx}. Took {time.time() - startNodeTime}')\n",
    "        ###################################\n",
    "        # Breaking component before node. #\n",
    "        ###################################\n",
    "        ## Here component should be broken before node (if determined necessary)\n",
    "        if nodeIdx>1 and \\\n",
    "           (leftPathStartEnd or leftFarLink or isChangedStructure or isNotFitIntoComponent) and \\\n",
    "           nBins>0:\n",
    "            # It is not the first component, and there is at least one flag to break before node and \n",
    "            # there is something to break (the component was not broken after previous node)\n",
    "            if debug:\n",
    "                if leftFarLink:\n",
    "                    print(f'Node {nodeIdx}: Component broken before node {nodeIdx} due to left far link.')\n",
    "                elif isChangeOccupancy or isChangeInversion:\n",
    "                    print(f'Node {nodeIdx}: Component broken before node {nodeIdx} due to change in inversion or occupancy in a path.')\n",
    "                elif isNotFitIntoComponent:\n",
    "                    print(f'Node {nodeIdx}: Component broken before node {nodeIdx} because it does not fit into current component.')\n",
    "                else:\n",
    "                    print(f'Node {nodeIdx}: Component broken before node {nodeIdx} due to start or end of a path.')\n",
    "\n",
    "            [component,components,\n",
    "            componentNucleotides,\n",
    "            matrix,occupants,\n",
    "            nBins,\n",
    "            nucleotides] = finaliseComponentBase(component,components,\n",
    "                                            componentNucleotides,\n",
    "                                            matrix,occupants,\n",
    "                                            nBins,\n",
    "                                            componentLengths,\n",
    "                                            nucleotides,\n",
    "                                            zoomLevel=zoomLevel,\n",
    "                                            accessions=graph.accessions)\n",
    "            if len(components) not in nodeToComponent[-1]:\n",
    "                nodeToComponent[-1].append(len(components))\n",
    "            componentToNode.append(list(nodesInComp))\n",
    "            nodesInComp = set()\n",
    "        \n",
    "        occupants |= set(uniqueNodePathsIDs)\n",
    "        nodesInComp.add(nodeIdx)\n",
    "        nodeToComponent.append([])\n",
    "        \n",
    "        \n",
    "        #######################################################\n",
    "        # Adding bins and forming components for zoom level 1 #\n",
    "        #######################################################\n",
    "        # TODO: This process for each bin should be parallelised\n",
    "        \n",
    "        # Building nucleotide (real or dummy) for the component.\n",
    "        if isSeq:\n",
    "            nucleotides += graph.nodesData[nodeIdx-1]\n",
    "\n",
    "        # binLength is removed as this function process only single nucleotide level.\n",
    "        \n",
    "        # Preparing annotation for each node and each accession\n",
    "        curNodeAnnotation = {}\n",
    "        \n",
    "        forwardPaths = []\n",
    "        invertedPaths = []\n",
    "        \n",
    "        for j,pathID in enumerate(uniqueNodePathsIDs):\n",
    "            if debugTime:\n",
    "                startTime = time.time()\n",
    "            \n",
    "            pathName = graph.accessions[pathID]\n",
    "            \n",
    "            # get occupancy and inversion for current node in current path.\n",
    "            occ = occupancy[pathID]\n",
    "            inv = inversion[pathID]\n",
    "            \n",
    "            if inv>inversionThreshold:\n",
    "                invertedPaths.append(pathID)\n",
    "            else:\n",
    "                forwardPaths.append(pathID)\n",
    "            \n",
    "            # Creating addition to matrix from this node for this path.\n",
    "            matrixPath = matrix.get(pathID,[[],[]])\n",
    "            \n",
    "            \n",
    "            # Absolute positinal genomic coordinate for given path for each pass of the node.\n",
    "            # NodeStarts are 1-based (position of the first node in the path will be 1).\n",
    "            nodeStarts = np.array([pathNodeLengthsCum[pathID,nodeNumInPath]-nodeLen+1 \\\n",
    "                          for nodeNumInPath in nodeSeqInPath[np.where(nodePathsIdx==pathID)[0]]])\n",
    "            \n",
    "            \n",
    "            # Generating positional pairs for each nucleotide of the node.\n",
    "            posPath = [list(zip(nodeStarts+pos,nodeStarts+pos)) for pos in range(nodeLen)]\n",
    "            \n",
    "            if debugTime:\n",
    "                print(f'Preparation for node {nodeIdx} path {pathName} took {time.time() - startTime}')\n",
    "                annotationTime = time.time()\n",
    "            \n",
    "            if debugTime:\n",
    "                print(f'Annotations for node {nodeIdx} path {pathName} finished in {time.time()-annotationTime}, overall time {time.time()-startTime}')\n",
    "                matrixTime = time.time()\n",
    "            \n",
    "            matrixPath[0].extend(range(nBins,nBins+nodeLen))\n",
    "            matrixPath[1].extend(zip([occ]*nodeLen,[inv]*nodeLen,posPath))\n",
    "            \n",
    "            matrix[pathID] = matrixPath\n",
    "            \n",
    "            if debugTime:\n",
    "                print(f'MatrixPath for node {nodeIdx} path {pathName} finished in {time.time()-matrixTime}, overall time {time.time()-startTime}')\n",
    "        \n",
    "        forwardPaths.extend(set(range(len(graph.accessions))).difference(forwardPaths+invertedPaths))\n",
    "        \n",
    "        # After all bins are formed, they should be split among components.\n",
    "        # If there are more than one component from the node, then the last component \n",
    "        # should remain open and the rest of checks should go as normal.\n",
    "        # This will allow to attach a small fully followed node after \n",
    "        # (which should be the single node but due to graph construction error was separated).\n",
    "        # Otherwise, if only one component is formed, then the normal checks should follow.\n",
    "        nBinsReduction = 0\n",
    "        if nodeLen>maxLengthComponent:\n",
    "            if debugTime:\n",
    "                postBinTime = time.time()\n",
    "            for blockStart in range(0,nodeLen+1-maxLengthComponent,maxLengthComponent):\n",
    "                blockEnd = blockStart + maxLengthComponent\n",
    "                if len(forwardPaths)>0:\n",
    "                    addLink(len(components)+1,'+',len(components)+2,'+',forwardPaths,fromComponentLinks,toComponentLinks)\n",
    "\n",
    "                if len(invertedPaths)>0:\n",
    "                    addLink(len(components)+2,'-',len(components)+1,'-',invertedPaths,fromComponentLinks,toComponentLinks)\n",
    "\n",
    "                if debug:\n",
    "                    print(f'Node {nodeIdx}: Component broken inside node {nodeIdx} due to max component length.')\n",
    "                matrixSlice = {}\n",
    "                for pathID,matrixPath in matrix.items():\n",
    "                    matrix[pathID][0] = list(range(nodeLen-blockEnd))\n",
    "                    if inversion[pathID]>inversionThreshold:\n",
    "                        matrixDataSlice = matrixPath[1][-maxLengthComponent:]\n",
    "                        matrix[pathID][1] = matrixPath[1][:-maxLengthComponent]\n",
    "                    else:\n",
    "                        matrixDataSlice = matrixPath[1][:maxLengthComponent]\n",
    "                        matrix[pathID][1] = matrixPath[1][maxLengthComponent:]\n",
    "                    matrixSlice[pathID] = [list(range(len(matrixDataSlice))),matrixDataSlice]\n",
    "                \n",
    "                if inversion[pathID]>inversionThreshold:\n",
    "                    nucleotideSlice = nucleotides[-maxLengthComponent:]\n",
    "                    nucleotides = nucleotides[:-maxLengthComponent]\n",
    "                else:\n",
    "                    nucleotideSlice = nucleotides[:maxLengthComponent]\n",
    "                    nucleotides = nucleotides[maxLengthComponent:]\n",
    "                \n",
    "                component,components,componentNucleotides,_,_,_,_ = \\\n",
    "                    finaliseComponentBase(component,components,componentNucleotides,\n",
    "                                      matrixSlice,occupants,\n",
    "                                      maxLengthComponent,componentLengths,nucleotideSlice,\n",
    "                                      zoomLevel=zoomLevel,accessions=graph.accessions)\n",
    "\n",
    "                nodeToComponent[-1].append(len(components))\n",
    "                componentToNode.append(list(nodesInComp))\n",
    "                nodesInComp = set([nodeIdx])\n",
    "                \n",
    "                nBinsReduction += maxLengthComponent\n",
    "            if debugTime:\n",
    "                print(f'Node splitting for node {nodeIdx} took {time.time()-postBinTime}')\n",
    "        nBins += nodeLen-nBinsReduction\n",
    "        \n",
    "        \n",
    "        if debugTime:\n",
    "            postBinTime = time.time()\n",
    "        #########################################\n",
    "        # Breaking component after current node #\n",
    "        #########################################\n",
    "        # If any path ends on current node, this should be recorded in the component.\n",
    "        if len(pathEnds)>0:\n",
    "            if nBins>0:\n",
    "                component.setdefault(\"ends\",set()).update(pathEnds.tolist())\n",
    "                accEnds = updateEdges(accEnds,pathEnds,len(components)+1)\n",
    "            else:\n",
    "                components[-1].setdefault(\"ends\",set()).update(pathEnds.tolist())\n",
    "                accEnds = updateEdges(accEnds,pathEnds,len(components))\n",
    "                \n",
    "        if len(pathStarts)>0:\n",
    "            if nBins>0:\n",
    "                component.setdefault(\"starts\",set()).update(pathStarts.tolist())\n",
    "                accStarts = updateEdges(accStarts,pathStarts,len(components)+1)\n",
    "            else:\n",
    "                components[-1].setdefault(\"starts\",set()).update(pathStarts.tolist())\n",
    "                accStarts = updateEdges(accStarts,pathStarts,len(components))\n",
    "        \n",
    "        if nodeIdx==len(graph.nodes) and nBins>0:\n",
    "            # Last node in graph.\n",
    "            if debug:\n",
    "                print(f'Node {nodeIdx}: Last node in the last component.')\n",
    "            component,components,componentNucleotides,matrix,occupants,nBins,nucleotides = \\\n",
    "                finaliseComponentBase(component,components,componentNucleotides,matrix,occupants,nBins,componentLengths,nucleotides,\n",
    "                                  zoomLevel=zoomLevel,accessions=graph.accessions)\n",
    "            nodeToComponent[-1].append(len(components))\n",
    "            componentToNode.append(list(nodesInComp))\n",
    "            nodesInComp = set()\n",
    "\n",
    "        elif (rightFarLink or rightPathStartEnd) and nBins>0:\n",
    "            if debug:\n",
    "                if rightFarLink:\n",
    "                    print(f'Node {nodeIdx}: Component broken after node {nodeIdx} due to right far link.')\n",
    "                else:\n",
    "                    print(f'Node {nodeIdx}: Component broken before node {nodeIdx} due to start or end of a path.')\n",
    "            component,components,componentNucleotides,matrix,occupants,nBins,nucleotides = \\\n",
    "                finaliseComponentBase(component,components,componentNucleotides,matrix,occupants,nBins,componentLengths,nucleotides,\n",
    "                                  zoomLevel=zoomLevel,accessions=graph.accessions)\n",
    "            nodeToComponent[-1].append(len(components))\n",
    "            componentToNode.append(list(nodesInComp))\n",
    "            nodesInComp = set()\n",
    "        # Not sure why this is needed. It will add com\n",
    "        elif nBins>0:\n",
    "            nodeToComponent[-1].append(len(components)+1)\n",
    "        \n",
    "        if debugTime:\n",
    "            print(f'After node {nodeIdx} component breaking took {time.time() - postBinTime}')\n",
    "            print(f'Processing of node {nodeIdx} took {time.time() - startNodeTime}')\n",
    "    \n",
    "    combineAnnotation(combAnnotation)\n",
    "    \n",
    "    return (numNodes, # number of nodes\n",
    "            numNodesDigits, # \n",
    "            nodeToComponent,\n",
    "            componentToNode,\n",
    "            componentLengths,\n",
    "            components,\n",
    "            componentNucleotides,\n",
    "            fromLinks, toLinks,\n",
    "            fromComponentLinks, toComponentLinks,\n",
    "            accStarts, accEnds,\n",
    "            invertedStarts, invertedEnds,\n",
    "            combAnnotation, combGenPos, combAltChrGenPos, combGenPosSearch, combPangenPosSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224a422",
   "metadata": {},
   "source": [
    "## Transfer from nodes to components (links and other structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61497916-8303-4863-9c12-9efa6d393e9b",
   "metadata": {},
   "source": [
    "This is one of the first processes happening while exporting graph. While graph operates with nodes (which can be linearly connected with each other in all paths),\n",
    "then exporting works with components. In almost all cases, components have at least some non-linear links with other components on both sides. The only exclusion is when a component is too large and split into several ones. In this case two components will be connected by 100% linear links. Also, graph operates with paths along with nodes, whereas exporting works with components and accession-specific links between them.\n",
    "\n",
    "These functions (with main orchestrating one is `nodeToComponentLinks`) are converting nodes and paths to components and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def splitforwardInversedNodeComp(pathList,component,isInverse):\n",
    "    forward = []\n",
    "    inversed = []\n",
    "    \n",
    "    for pathID in pathList:\n",
    "            try:\n",
    "                if component[\"matrix\"][component[\"occupants\"].index(pathID)][1]>0:\n",
    "                    inversed.append(pathID)\n",
    "                else:\n",
    "                    forward.append(pathID)\n",
    "            except (IndexError,ValueError):\n",
    "                # If it is artificial pass link.\n",
    "                if isInverse:\n",
    "                    inversed.append(pathID)\n",
    "                else:\n",
    "                    forward.append(pathID)\n",
    "                \n",
    "    return forward,inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f256eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fillLinksBase(nodeInComp,nodeToComponent,fromLinks,toLinks,fromComponentLinks,toComponentLinks,compNum,components,doLeft=True,doRight=True):\n",
    "    component = components[compNum]\n",
    "    for node,isFirstNode in zip([nodeInComp[0],nodeInComp[-1]],[True,False]):\n",
    "        # Processing all external arrival links\n",
    "        nodeToLink = toLinks.get(node,{})\n",
    "        for fromNode in nodeToLink.keys():\n",
    "            intermediateCondition = (node<fromNode)\n",
    "            \n",
    "            la,ra = splitforwardInversedNodeComp(nodeToLink[fromNode],component,intermediateCondition)\n",
    "            \n",
    "            fromFirstCompNum = nodeToComponent[fromNode-1][0]\n",
    "            fromFirstComp = components[fromFirstCompNum-1]\n",
    "            fromLastCompNum = nodeToComponent[fromNode-1][-1]\n",
    "            fromLastComp = components[fromLastCompNum-1]\n",
    "            \n",
    "            if isFirstNode:\n",
    "                #left arrivals\n",
    "                if len(la)>0 and doLeft:\n",
    "                    frd,fld = splitforwardInversedNodeComp(la,fromFirstComp,intermediateCondition)\n",
    "\n",
    "                    # from right departure\n",
    "                    if len(frd)>0 and (fromNode not in nodeInComp) and (fromLastComp['last_bin']+1!=component['first_bin']):\n",
    "                        addLink(fromLastCompNum,'+',compNum+1,'+',frd,fromComponentLinks,toComponentLinks)\n",
    "                    #from left departure\n",
    "                    if len(fld)>0 and (fromNode not in nodeInComp):\n",
    "                        addLink(fromFirstCompNum,'-',compNum+1,'+',fld,fromComponentLinks,toComponentLinks)\n",
    "            \n",
    "            if not isFirstNode:\n",
    "                #right arrivals\n",
    "                if len(ra)>0 and doRight:\n",
    "                    frd,fld = splitforwardInversedNodeComp(ra,fromFirstComp,intermediateCondition)\n",
    "\n",
    "                    #from right departures\n",
    "                    if len(frd)>0 and (fromNode not in nodeInComp):\n",
    "                        addLink(fromLastCompNum,'+',compNum+1,'-',frd,fromComponentLinks,toComponentLinks)\n",
    "                    #from left departures\n",
    "                    if len(fld)>0 and (fromNode not in nodeInComp):\n",
    "                        addLink(fromFirstCompNum,'-',compNum+1,'-',fld,fromComponentLinks,toComponentLinks)\n",
    "            \n",
    "        # Processing all external departure links\n",
    "        nodeFromLink = fromLinks.get(node,{})\n",
    "        for toNode in nodeFromLink.keys():\n",
    "            intermediateCondition = (node>toNode)\n",
    "            \n",
    "            rd,ld = splitforwardInversedNodeComp(nodeFromLink[toNode],component,intermediateCondition)\n",
    "            \n",
    "            toFirstCompNum = nodeToComponent[toNode-1][0]\n",
    "            toFirstComp = components[toFirstCompNum-1]\n",
    "            toLastCompNum = nodeToComponent[toNode-1][-1]\n",
    "            toLastComp = components[toLastCompNum-1]\n",
    "            \n",
    "            if not isFirstNode:\n",
    "                #right departures\n",
    "                if len(rd)>0 and doRight: # Check if doRight is set incorrectly for our case (121->122 at level 4)\n",
    "                    tla,tra = splitforwardInversedNodeComp(rd,toFirstComp,intermediateCondition)\n",
    "\n",
    "                    #to left arrivals\n",
    "                    if len(tla)>0:\n",
    "                        addLink(compNum+1,'+',toFirstCompNum,'+',tla,fromComponentLinks,toComponentLinks)\n",
    "\n",
    "                    #to right arrivals\n",
    "                    if len(tra)>0: # Most probably the problem is here! Check it!\n",
    "                        addLink(compNum+1,'+',toLastCompNum,'-',tra,fromComponentLinks,toComponentLinks)\n",
    "            \n",
    "            if isFirstNode:\n",
    "                #left departures\n",
    "                if len(ld)>0 and doLeft:\n",
    "                    tla,tra = splitforwardInversedNodeComp(ld,toFirstComp,intermediateCondition)\n",
    "\n",
    "                    #to left arrivals\n",
    "                    if len(tla)>0:\n",
    "                        addLink(compNum+1,'-',toFirstCompNum,'+',tla,fromComponentLinks,toComponentLinks)\n",
    "\n",
    "                    if len(tra)>0:\n",
    "                        addLink(compNum+1,'-',toLastCompNum,'-',tra,fromComponentLinks,toComponentLinks)\n",
    "                        \n",
    "    return fromComponentLinks,toComponentLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convertLink(linkFrom,linkTo,translateDict,forwardLinks,isZoom):\n",
    "    \n",
    "    linkSetConv = set()\n",
    "    if isZoom:\n",
    "        linkFound = False\n",
    "        for fromStrand,fromStrandDict in forwardLinks[linkFrom].items():\n",
    "            for toStrand,accessionList in fromStrandDict.get(linkTo,{}).items():\n",
    "                linkFromConv = translateDict[linkFrom-1][0 if fromStrand=='-' else -1]+1\n",
    "                linkToConv = translateDict[linkTo-1][0 if toStrand=='+' else -1]+1\n",
    "                linkSetConv.add((linkFromConv,linkToConv))\n",
    "                linkFound = True\n",
    "        if not linkFound:\n",
    "            warnings.warn(f'Link ({linkFrom},{linkTo}) is requested to convert, but was not found in forwardLinks!',\n",
    "                          category=RuntimeWarning)\n",
    "    else:\n",
    "        for fromStrand,fromStrandPairs in forwardLinks[linkFrom].items():\n",
    "            for toNode,toStrand in fromStrandPairs:\n",
    "                if toNode==linkTo:\n",
    "                    linkFromConv = translateDict[linkFrom-1][0 if fromStrand=='-' else -1]\n",
    "                    linkToConv = translateDict[linkTo-1][0 if toStrand=='+' else -1]\n",
    "                    linkSetConv.add((linkFromConv,linkToConv))\n",
    "                    break\n",
    "        \n",
    "    return linkSetConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recordUpdatedPairedLink(firstLinkSet,secondLinkSet,firstLink,secondLink,substituteLink,pairedLinksConv):\n",
    "    if substituteLink==(firstLink[0],secondLink[1]):\n",
    "        # Straight paired link of inverted double paired link\n",
    "        for fLink in firstLinkSet:\n",
    "            for sLink in secondLinkSet:\n",
    "                pairedLinksConv.setdefault(fLink,{})[sLink] = (fLink[0],sLink[1])\n",
    "        \n",
    "    elif substituteLink==(secondLink[0],firstLink[1]):\n",
    "        # inverted paired link or straight doublePaired link\n",
    "        \n",
    "        for fLink in firstLinkSet:\n",
    "            for sLink in secondLinkSet:\n",
    "                pairedLinksConv.setdefault(fLink,{})[sLink] = (sLink[0],fLink[1])\n",
    "    else:\n",
    "        raise RuntimeError(f'Unusual paired link encountered {firstLink}+{secondLink} -> {substituteLink}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convertRemovableComponents(translateDict,linkLengths,pairedLinks,interconnectedLinks,blockEdges,forwardLinks,isZoom=True):\n",
    "    '''\n",
    "    translateDict should be a dict in format {<old node/component id 0-based>:<new component id 1-based>}\n",
    "    pathNodeInv should be a dict of dicts of the following structure:\n",
    "    {<pathID>:{<nodeId 1-based>:<Bool inv or not>}}\n",
    "    \n",
    "    This is done through fromLinks and toLinks and throught associated directions of available accessions.\n",
    "    For this we need to loop through strands and do it separately for each strand.\n",
    "    \n",
    "    For paired links there is a possibility that a single node link will give several component links. \n",
    "    In this case, the cross product of all first and second links will be added to converted paired links.\n",
    "       \n",
    "    The substitute links should be added only to the paths that contained both first and second links in the first place.\n",
    "        This should be controlled in link removal routine.\n",
    "    \n",
    "    '''\n",
    "    if linkLengths is None or pairedLinks is None or interconnectedLinks is None or blockEdges is None:\n",
    "        return None,None,None,None\n",
    "    processedLinks = {}\n",
    "    print('Converting link to block lengths associations')\n",
    "    linkLengthsConv = {}\n",
    "    for blockLength,linkList in linkLengths.items():\n",
    "        for linkFrom,linkTo in linkList:\n",
    "            linksConv = convertLink(linkFrom,linkTo,translateDict,forwardLinks,isZoom)\n",
    "            linkLengthsConv.setdefault(blockLength,set()).update(linksConv)\n",
    "            for link in linksConv:\n",
    "                processedLinks.setdefault(link,set()).add(blockLength)\n",
    "\n",
    "    print('Converting paired links')\n",
    "    pairedLinksConv = {}\n",
    "    for firstLink,flDict in pairedLinks.items():\n",
    "        firstLinkSet = convertLink(*firstLink,translateDict,forwardLinks,isZoom)\n",
    "        for secondLink,substituteLink in flDict.items():\n",
    "            secondLinkSet = convertLink(*secondLink,translateDict,forwardLinks,isZoom)\n",
    "            recordUpdatedPairedLink(firstLinkSet,secondLinkSet,firstLink,secondLink,substituteLink,pairedLinksConv)\n",
    "    \n",
    "    print('Converting interconnected links')\n",
    "    interconnectedLinksConv = {}\n",
    "    for link,linkList in interconnectedLinks.items():\n",
    "        convertLinks = convertLink(*link,translateDict,forwardLinks,isZoom)\n",
    "        for linkC in convertLinks:\n",
    "            \n",
    "            linkListConv = interconnectedLinksConv.get(linkC,set())\n",
    "            originalLinkListLen = len(linkListConv)\n",
    "            \n",
    "            for linkI in linkList:\n",
    "                linkListConv.update(convertLink(*linkI,translateDict,forwardLinks,isZoom))\n",
    "            \n",
    "            if originalLinkListLen>0:\n",
    "                # join two blocks\n",
    "                processedLinks.pop(linkC,None)\n",
    "                linkListConv.add(linkC)\n",
    "                blockLen = []\n",
    "                for blLen,linkList in list(linkLengthsConv.items()):\n",
    "                    if linkC in linkList:\n",
    "                        linkLengthsConv[blLen] = linkList - linkListConv\n",
    "                        blockLen.append(blLen)\n",
    "                        \n",
    "                maxBlockLen = max(blockLen)\n",
    "                linkLengthsConv[maxBlockLen].update(linkListConv)\n",
    "                \n",
    "                for link in linkListConv:\n",
    "                    interconnectedLinksConv[link] = linkListConv\n",
    "            else:\n",
    "                interconnectedLinksConv[linkC] = linkListConv\n",
    "    \n",
    "    for link,blList in processedLinks.items():\n",
    "        if len(blList)>1:\n",
    "            for blockLength in blList:\n",
    "                linkLengthsConv[blockLength].remove(link)\n",
    "            linkLengthsConv[max(blList)].add(link)\n",
    "    \n",
    "    print('Converting rearrangement blocks')\n",
    "    blockEdgesConv = {translateDict[node-1][-1]:blockLength for node, blockLength in blockEdges.items()}\n",
    "    \n",
    "    return linkLengthsConv,pairedLinksConv,interconnectedLinksConv,blockEdgesConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nodeToComponentLinks(components,componentToNode,nodeToComponent,\n",
    "                         fromLinks,toLinks,graph,\n",
    "                         fromComponentLinks,toComponentLinks,\n",
    "                         linkLengths=None,pairedLinks=None,interconnectedLinks=None,blockEdges=None,\n",
    "                         debug=False):\n",
    "    numComps = len(components)\n",
    "    numCompsDigits = int(np.ceil(np.log10(numComps)))\n",
    "    \n",
    "    for compNum in range(numComps):\n",
    "        if debug:    \n",
    "            print(f'Processing component links {compNum+1:0{numCompsDigits}}/{numComps:0{numCompsDigits}}')\n",
    "        else:\n",
    "            print(f'\\rProcessing component links {compNum+1:0{numCompsDigits}}/{numComps:0{numCompsDigits}}',end='')\n",
    "\n",
    "        component = components[compNum]\n",
    "        \n",
    "        nodeInComp = componentToNode[compNum]\n",
    "        nodeInComp.sort()\n",
    "\n",
    "        if len(nodeInComp)>1:\n",
    "            fromComponentLinks,toComponentLinks = fillLinksBase(nodeInComp,nodeToComponent,fromLinks,toLinks,fromComponentLinks,toComponentLinks,compNum,components)\n",
    "        elif len(nodeInComp)==1:\n",
    "\n",
    "            mainNode = nodeInComp[0]\n",
    "            doLeft = False\n",
    "            if compNum==0:\n",
    "                doLeft = True\n",
    "            elif mainNode not in componentToNode[compNum-1]:\n",
    "                doLeft = True\n",
    "\n",
    "            doRight = False # Check this conditions as they may be causing issue!\n",
    "            if compNum==(len(components)-1):\n",
    "                doRight = True\n",
    "            elif mainNode not in componentToNode[compNum+1]:\n",
    "                doRight = True\n",
    "\n",
    "            fromComponentLinks,toComponentLinks = fillLinksBase(nodeInComp,nodeToComponent,fromLinks,toLinks,fromComponentLinks,toComponentLinks,compNum,components,doLeft,doRight)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Component {compNum} does not have any associated nodes!\")\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    linkLengthsConv,pairedLinksConv,interconnectedLinksConv,blockEdgesConv = \\\n",
    "    convertRemovableComponents(nodeToComponent,linkLengths,pairedLinks,interconnectedLinks,blockEdges,graph.forwardLinks,isZoom=False)\n",
    "    \n",
    "    return fromComponentLinks,toComponentLinks,linkLengthsConv,pairedLinksConv,interconnectedLinksConv,blockEdgesConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8c4bb",
   "metadata": {},
   "source": [
    "## Identifying collapsible links and rearrangement blocks (works incorrectly, left now for compatibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15dcab3-abd1-4baf-aa38-33fdab99f076",
   "metadata": {},
   "source": [
    "In order to be able to generate multiple zoom levels of the graph view, non-linear links describing small (too small to show at the given \n",
    "zoom level) rearrangements should disappear whereas links describing larger blocks should persist. This will allow to see larger rearrangements clearly\n",
    "on higher zoom levels.\n",
    "\n",
    "In order to do it, each link should be associated with some size (or rearrangement), so, that when each zoom level is generated, they can be removed when the rearrangement cannot be shown at the given zoom level.\n",
    "\n",
    "Some links are also associated with each other, and when they are removed new links (usually linear ones) should be reinstated to make larger rearrangements clearer.\n",
    "\n",
    "At the moment, the process of identifying these sizes is not working great as it leaves too much non-linear links to the very top level where suddenly all non-linear links disappear and the whole graph from over-complicated jumps to pretty much trivial without any rearrangements. If to use digital map analogy, most of country roads persist while you zoom out on the map until almost the whole Earth is in view and then at some point the view becomes just a blue/green ball with very rough boundary of continents and oceans.\n",
    "\n",
    "At the moment, all associated links get into a pool of so called interconnected links and if one link gets associated with specific size, then all links get the same association, and then maximum size is selected. But that means that if one link describes one small and also on the edge of large rearrangement, and another link is only associated with large rearrangement, then the latter link will also be associated with the size of large rearrangement and will stay until the zoom level where the large rearrangement is too small to show. That is incorrect. \n",
    "\n",
    "I think, each link should get its own associations with sizes (and maximum should be taken) and clearing of the link should happen individually. Yet, if one link with smaller size and one with larger size are paired, the reinstated link should appear after smaller link removed.\n",
    "\n",
    "Another alternative is just to get contiguous blocks in each path and associate each link pair (describing start and end of each block) as a pair of links that needs to be cleared in association with the size of this block. Need control of repeats in these blocks. If it happens, then a single link can describe a whole rearrangement. In addition, an extra control for inversion is also needed. In particular, if outside the block the numbers do not create a range to fin the inverted node (e.g. 1+,4-,3+, or 3+,2-,5+), then it should be ignorred for this step. It means there is a smaller rearrangement within larger one.\n",
    "\n",
    "Another alternative (described in TODO) is to convert paths of nodes to paths of edges and operate with them. I guess, it is not far away from the previous paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf41e5",
   "metadata": {},
   "source": [
    "### Identifying path breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def findBreaksInPath(combinedArray,nextNodeDict):\n",
    "    return [pos for pos,node in enumerate(combinedArray[:-1]) \\\n",
    "                      if (node<0 and combinedArray[pos+1]!=-1*nextNodeDict.inverse.get(-1*node,[-1*node-1])[0]) or\\\n",
    "                         (node>0 and combinedArray[pos+1]!=nextNodeDict.get(node,node+1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identifyPathBreaks(combinedNodeDirArray,pathLengths,pathNextNode):\n",
    "    pathBreakCoordPairs = []\n",
    "    for pathID in range(combinedNodeDirArray.shape[0]):\n",
    "        # separate this block into separate function as it will be used in link processing!\n",
    "        tempBreaks = [(pathID,pos) for pos in findBreaksInPath(combinedNodeDirArray[pathID,:pathLengths[pathID]], pathNextNode[pathID])]\n",
    "                      \n",
    "        pathBreakCoordPairs.extend(tempBreaks)\n",
    "    \n",
    "    return pathBreakCoordPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a96a5",
   "metadata": {},
   "source": [
    "### Block processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def interweaveArrays(a,b):\n",
    "    if len(a.shape)>1 or len(b.shape)>1:\n",
    "        raise ValueError('Both input arrays should be 1D arrays')\n",
    "        \n",
    "    if a.shape[0]!=b.shape[0]:\n",
    "        raise ValueError('Both input arrays should have the same lengths')\n",
    "        \n",
    "    c = np.empty((a.size + b.size,), dtype=a.dtype)\n",
    "    c[0::2] = a\n",
    "    c[1::2] = b\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extractGapsBlocks(block,path,nodeLengths,getComplex=False):\n",
    "    '''\n",
    "    This function either split block by gaps (e.g. block [1,2,4,5,6,8] will yield [1,2],[4,5,6],[8])\n",
    "    \n",
    "    If `getComplex` is set to True, then first gaps are filtered for nodes that are not passed by the path. \n",
    "    After that, edges are identified and then for them nodes not passed by the path are filtered out.\n",
    "    Then we find the longest block out of edges, and then the longest edge combine with all gaps and find \n",
    "    the shortest one. That shortest one is going to be the one returned.\n",
    "    \n",
    "    E.g. block [1,2,4,5,8] will give edges [1,2],[4,5],[8] and gaps [3],[6,7].\n",
    "    \n",
    "    If path does not contain 6, then edges will be the same, but gaps will be [3],[6]\n",
    "    \n",
    "    If path does not contain 3, then edges will be [1,2,4,5],[8] and gaps [6,7]\n",
    "    \n",
    "    The exact block which will be returned depends on sizes of each node.\n",
    "        \n",
    "    Return\n",
    "    ======\n",
    "    \n",
    "    List of blocks and whether any gaps are identified or not\n",
    "    '''\n",
    "    \n",
    "    gapList = np.where(np.diff(np.sort(block))!=1)[0]\n",
    "    \n",
    "    gapPosInterweaved = interweaveArrays(gapList,gapList+1)\n",
    "\n",
    "    gapEdgeNodes = np.array(block)[gapPosInterweaved]\n",
    "\n",
    "    gapEdgeNodes[1::2] = gapEdgeNodes[1::2]-1\n",
    "    \n",
    "    gaps = np.split(np.arange(1,max(path)+1),gapEdgeNodes)[1::2]\n",
    "    \n",
    "    if len(gaps)>0:\n",
    "        if getComplex:\n",
    "            # It looks like it is possible to identify edges first and then filter gaps for nodes not used by the path\n",
    "            # But that will create edges that contain nodes that are not used by the path, which still bring us to two filtering.\n",
    "            \n",
    "            # Removing parts (or whole) of gaps, which are not actually gaps, but nodes never used by the path.\n",
    "            gaps = list(filter(lambda bl: len(bl)>0,[tuple((gap[np.isin(gap,path)]).tolist()) for gap in gaps]))\n",
    "            \n",
    "            # Removing parts of the edges that turned out to contain nodes not passed by the path\n",
    "            edges = list(filter(lambda bl: len(bl)>0,[tuple((bl[np.isin(bl,path)]).tolist()) for bl in np.split(block,gapList+1)]))\n",
    "\n",
    "            edgesLengths = [sum([nodeLengths[node-1] for node in bl]) for bl in edges]\n",
    "            maxEdgeInd = np.argmax(edgesLengths)\n",
    "            maxEdge = edges[maxEdgeInd]\n",
    "            maxEdgeLength = edgesLengths[maxEdgeInd]\n",
    "            \n",
    "            gapLengths = [sum([nodeLengths[node-1] for node in bl]) for bl in gaps]\n",
    "            gaps += [maxEdge]\n",
    "            gapLengths += [maxEdgeLength]\n",
    "            \n",
    "            minBlockInd = np.argmin(gapLengths)\n",
    "            minBlock = gaps[minBlockInd]\n",
    "            return [tuple(minBlock)],True\n",
    "        else:\n",
    "            return [tuple(bl) for bl in np.split(block,gapList+1)],True\n",
    "    else:\n",
    "        return [tuple(block)],False#checkSplitBlock(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkSplitBlock(block,gapList=None):\n",
    "    '''\n",
    "    Not used at the moment\n",
    "    \n",
    "    Function checks if the block has any gaps and split into a list of blocks between gaps \n",
    "    (alternatively fill gaps or leave things as they are).\n",
    "    At the moment the gapped block will be converted to list of blocks between gaps\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `block`: np.array 1D. An array of node ids conprising the block.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    `blockList`: list[np.array]. A list of numpy arrays of eligible blocks.\n",
    "    '''\n",
    "    if gapList is None:\n",
    "        gapList = np.where(np.diff(np.sort(block))!=1)[0]\n",
    "    \n",
    "    if len(gapList>0):\n",
    "        return [tuple(bl) for bl in np.split(block,gapList+1).tolist()]\n",
    "    else:\n",
    "        return [tuple(block)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d10455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def blockListToLengths(blockList,nodeLengths):\n",
    "    return [np.sum([nodeLengths[node-1] for node in block]) for block in blockList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _blockToLengthLink(link,blockList,linkL,nodeLengths):\n",
    "    linkL.append((link,max(blockListToLengths(blockList,nodeLengths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e00979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convertBlocksToLengths(linksBlocks,nodeLengths):\n",
    "    ''' \n",
    "    Converting blocks associated with each link to lengths and then selecting the longest one (?)\n",
    "    '''\n",
    "    \n",
    "    p = Pool(cpu_count()-2)\n",
    "    m = Manager()\n",
    "    linkL = m.list()\n",
    "    numLinks = len(linksBlocks)\n",
    "    for i,_ in enumerate(p.starmap(partial(_blockToLengthLink,linkL=linkL,nodeLengths=nodeLengths),linksBlocks.items()),start=1):\n",
    "        print(f'\\rConverting blocks to block lengths {i}/{numLinks}',end='')\n",
    "    \n",
    "    print('\\nConversion finished.')\n",
    "    \n",
    "    linkLengths = {}\n",
    "    \n",
    "    for linkNum,[link,blockSize] in enumerate(linkL,start=1):\n",
    "        # This option creates a dict <link>:<block length>    \n",
    "        # linkLengths[link] = max(blockListToLengths(blockList,nodeLengths))\n",
    "        print(f'\\rReformating links to block lengths associations {linkNum}/{numLinks}', end = '')\n",
    "        # This option creates a dict <block length>:[<link>,<link>,...]\n",
    "        linkLengths.setdefault(blockSize,set()).add(link)\n",
    "    \n",
    "        # The second option will be easier to work with during layers generation.\n",
    "    print('\\nReformating finished.')\n",
    "    return linkLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa07de",
   "metadata": {},
   "source": [
    "### Link processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def addToLinkPool(link1,link2,interconnectedLinks):\n",
    "    # Change signature of the function in the downstream function\n",
    "    \n",
    "    interconnectedLinks.setdefault(link1,set()).add(link2)\n",
    "    interconnectedLinks.setdefault(link2,set()).add(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc922e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def blockFromSingleLink(pathID,link,pathNodeInversionRate,pathNextNode):\n",
    "    '''\n",
    "    Identify block from a single link\n",
    "    It is the block that the link bounds, i.e.:\n",
    "    If link if forward then it is inside the link + any side that is inverted\n",
    "    If link is backward, then it is inside + any side that is normal direction.\n",
    "    '''\n",
    "    nodeDir = pathNodeInversionRate[pathID]\n",
    "    leftNode,rightNode = link\n",
    "    leftDir = nodeDir.get(leftNode,False)\n",
    "    rightDir = nodeDir.get(rightNode,False)\n",
    "    \n",
    "    if leftNode<rightNode:\n",
    "        # Next used node after `leftNode`\n",
    "        blockStart = pathNextNode[pathID].get(leftNode,leftNode+1)\n",
    "        # Last used node before `rightNode`\n",
    "        blockEnd = pathNextNode[pathID].inverse.get(rightNode,[rightNode-1])[0]\n",
    "        block = set(range(blockStart,blockEnd+1))\n",
    "        if leftDir:\n",
    "            block.add(leftNode)\n",
    "        if rightDir:\n",
    "            block.add(rightNode)\n",
    "    else:\n",
    "        # Next used node after `leftNode`\n",
    "        blockStart = pathNextNode[pathID].get(rightNode,rightNode+1)\n",
    "        # Last used node before `rightNode`\n",
    "        blockEnd = pathNextNode[pathID].inverse.get(leftNode,[leftNode-1])[0]\n",
    "        \n",
    "        block = set(range(blockStart,blockEnd+1))\n",
    "        if not leftDir:\n",
    "            block.add(leftNode)\n",
    "        if not rightDir:\n",
    "            block.add(rightNode)\n",
    "    \n",
    "    return tuple(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkIndividualLink(link,pathID,usedSecondInPairLink):\n",
    "    '''\n",
    "    Function checks if this link is already second in pair. If it is, then it is not considered separately (return True?). \n",
    "    Otherwise, it should be considered and block generated (using `blockFromSingleLink`) and associated with this link.\n",
    "    '''\n",
    "    return link in usedSecondInPairLink.get(pathID,set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15247d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processDoublePairedLinks(leftLink,rightLink,pathID,doublePairedLinks,pairedLinks,interconnectedLinks,linksBlocks,pathNextNode):\n",
    "    # Check if it is second link already.\n",
    "    # If it is, pair second link of the current pair with first link of the previous pair\n",
    "    # If not, add second link of the current pair with pathID as a key and first link as a value (or list).\n",
    "    # TBD: Should used links be removed from doublePairedLinks?\n",
    "    \n",
    "    pathDouble = doublePairedLinks.get(pathID,{})\n",
    "    \n",
    "    if leftLink in pathDouble:\n",
    "        firstLeftLink = pathDouble[leftLink]\n",
    "        commonLink = leftLink\n",
    "        secondRightLink = rightLink\n",
    "        if pathNextNode[pathID].get(secondRightLink[0],secondRightLink[0]+1)==firstLeftLink[1]:\n",
    "            pairedLinks.setdefault(secondRightLink,{})[firstLeftLink] = (secondRightLink[0],firstLeftLink[1])\n",
    "            pairedLinks.setdefault(firstLeftLink,{})[secondRightLink] = (secondRightLink[0],firstLeftLink[1])\n",
    "        \n",
    "            addToLinkPool(secondRightLink,firstLeftLink,interconnectedLinks)\n",
    "    \n",
    "    \n",
    "    doublePairedLinks.setdefault(pathID,{})[rightLink] = leftLink\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processIndividualLink(link,pathID,pathNodeInversionRate,pathNextNode,usedSecondInPairLink):\n",
    "    if usedSecondInPairLink is not None and checkIndividualLink(link,pathID,usedSecondInPairLink):\n",
    "        # If usedSecondInPairPath is provided, check if the link was already second in pair. In that case do not do anything.\n",
    "        return []\n",
    "    \n",
    "    block = blockFromSingleLink(pathID,link,pathNodeInversionRate,pathNextNode)\n",
    "    \n",
    "    return [block]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recordLinkBlockAssociation(link,blockList,linksBlocks):\n",
    "    linksBlocks.setdefault(link,set()).update(blockList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def findNextNode(node,combinedArray):\n",
    "    return np.where(combinedArray==node)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processPseudoPair(breakPos,returnPos,pathID,\n",
    "                      pathNodeArray,combinedNodeDirArray,pathNextNode,nodeLengths,\n",
    "                      usedSecondInPairPath,pairedLinks,linksBlocks):\n",
    "    \n",
    "    #######Processing pseudo paired links\n",
    "    mainLink = tuple(pathNodeArray[pathID,breakPos:breakPos+2].tolist())\n",
    "    dirStart = combinedNodeDirArray[pathID,breakPos]\n",
    "    # Collect all unique nodes between breakPos+1:returnPos\n",
    "    # Process block through min(max(edges),gaps) = minBlock\n",
    "    blockList,isGaps = extractGapsBlocks(np.unique(pathNodeArray[pathID,breakPos+1:returnPos]).tolist(),pathNodeArray[pathID],nodeLengths,getComplex=True)\n",
    "\n",
    "    # Find the last non-linear link between breakPos+1:returnPos (let's call it (e,f))\n",
    "    breaksInBlock = findBreaksInPath(combinedNodeDirArray[pathID,breakPos+1:returnPos],pathNextNode[pathID])\n",
    "    \n",
    "    ###########\n",
    "    # When searching for the last non-linear link, do not consider pure change of direction as non-linearity!!!\n",
    "    # I.e. if the links is (4-,3+) or (2+,3-) they are not non-linear for this purpose\n",
    "    # Links like (2-,4+) (given that 3 exist in the path). At the same time, I believe (possibly wrongly), \n",
    "    # that if the direction for the link start is reversed (e.g. 2-,3+), it should already be considered as non-linear.\n",
    "    #########################\n",
    "    \n",
    "    returnLinkStart = None\n",
    "    for auxBreak in breaksInBlock[::-1]:\n",
    "        \n",
    "        node = pathNodeArray[pathID,auxBreak+breakPos+1]\n",
    "        nextNode = pathNodeArray[pathID,auxBreak+1+breakPos+1]\n",
    "        if (dirStart>0 and nextNode!=pathNextNode[pathID].get(node,node+1)) or\\\n",
    "           (dirStart<0 and nextNode!=pathNextNode[pathID].inverse.get(node,node-1)):\n",
    "            returnLinkStart = auxBreak+breakPos+1\n",
    "            break\n",
    "    \n",
    "    if returnLinkStart is not None:\n",
    "        lastNonLinearLink = tuple(pathNodeArray[pathID,returnLinkStart:returnLinkStart+2].tolist())\n",
    "\n",
    "        if not checkIndividualLink(lastNonLinearLink,pathID,usedSecondInPairPath):\n",
    "            # If (e,f) was not used in pairs before, (?) it should be associated with minBlock\n",
    "            recordLinkBlockAssociation(lastNonLinearLink,blockList,linksBlocks)\n",
    "            # (e,f) should be consider as used in pair in the path, but not actually paired!\n",
    "            usedSecondInPairPath.setdefault(pathID,set()).add(lastNonLinearLink)\n",
    "\n",
    "\n",
    "    if (isGaps and checkIndividualLink(mainLink,pathID,usedSecondInPairPath)) or mainLink in pairedLinks:\n",
    "        # If there were gaps and leftLink was second in pair: do not associate leftLink with minBlock\n",
    "        # Reset blockList, so, nothing will be recorded for the main link\n",
    "        blockList = []\n",
    "\n",
    "    ######## End of pseudo pairs processing\n",
    "    \n",
    "    return blockList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processStartsEnds(mainLink,linkStarts,linkEnds,interconnectedLinks,forwardLinks):\n",
    "    '''\n",
    "    Currently not in use.\n",
    "    \n",
    "    TODO!!! Need to add checks for whether one link is intersecting the other or one is fully inside.\n",
    "    '''\n",
    "    potentialStartLinks = linkStarts.get(mainLink[0],set())\n",
    "    potentialEndLinks = linkEnds.get(mainLink[1],set())\n",
    "\n",
    "    if mainLink not in potentialStartLinks or mainLink not in potentialEndLinks:\n",
    "        strandSwitch = {'+':'-','-':'+'}\n",
    "        startStrands = set()\n",
    "        endStrands = set()\n",
    "        for startStrand,strandDict in forwardLinks[mainLink[0]].items():\n",
    "                if (mainLink[1],'+') in strandDict:\n",
    "                    startStrands.add(startStrand)\n",
    "                    endStrands.add('+')\n",
    "                if (mainLink[1],'-') in strandDict:\n",
    "                    startStrands.add(startStrand)\n",
    "                    endStrands.add('-')\n",
    "        \n",
    "        if mainLink not in potentialStartLinks:\n",
    "            for link in potentialStartLinks:\n",
    "                for startStrand in startStrands:\n",
    "                    toNodeStrandList = forwardLinks[link[0]].get(strandSwitch[startStrand],[])\n",
    "                    if ((link[1],'+') in toNodeStrandList or (link[1],'-') in toNodeStrandList) and \\\n",
    "                    ((startStrand=='+') and (mainLink[1]>link[1]) or\\\n",
    "                    (startStrand=='-') and (mainLink[1]<link[1])):\n",
    "                        addToLinkPool(mainLink,link,interconnectedLinks)\n",
    "            linkStarts.setdefault(mainLink[0],set()).add(mainLink)\n",
    "\n",
    "        if mainLink not in potentialEndLinks:\n",
    "            for link in potentialEndLinks:\n",
    "                for endStrand in endStrands:\n",
    "                    if ((link[1],strandSwitch[endStrand]) in forwardLinks[link[0]].get('+',[]) or\\\n",
    "                       (link[1],strandSwitch[endStrand]) in forwardLinks[link[0]].get('-',[])) and \\\n",
    "                        ((endStrand=='+') and (mainLink[0]<link[0]) or\\\n",
    "                        (endStrand=='-') and (mainLink[0]>link[0])):\n",
    "                        addToLinkPool(mainLink,link,interconnectedLinks)\n",
    "            linkEnds.setdefault(mainLink[1],set()).add(mainLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def postprocessLinksBlocks(linksBlocks,interconnectedLinks):\n",
    "    G = nx.Graph(interconnectedLinks)\n",
    "    \n",
    "    connLinks = list(nx.connected_components(G))\n",
    "    numConnLinks = len(connLinks)\n",
    "    \n",
    "    for connLinkListNum,connLinkList in enumerate(connLinks):\n",
    "        print(f'\\rPostprocessing interconnected links {connLinkListNum+1}/{numConnLinks}',end='')\n",
    "        blocks = set().union(*[linksBlocks[link] for link in connLinkList])\n",
    "        for link in connLinkList:\n",
    "            interconnectedLinks.setdefault(link,set()).update(connLinkList-set([link]))\n",
    "            linksBlocks[link] = blocks\n",
    "    \n",
    "    print('\\nPreprocessing interconnected links finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processPathBreaks(pathBreakCoordPairs,\n",
    "                       pathNodeArray,pathNextNode,combinedNodeDirArray,pathNodeInversionRate,pathLengths,nodeLengths,forwardLinks):\n",
    "    linksBlocks = {}\n",
    "    pairedLinks = {}\n",
    "    interconnectedLinks = {}\n",
    "    linkStarts = {}\n",
    "    linkEnds = {}\n",
    "    doublePairedLinks = {}\n",
    "    usedSecondInPairPath = {}\n",
    "    \n",
    "    print('Processing path breaks...')\n",
    "    numPathBreaks = len(pathBreakCoordPairs)\n",
    "    for breakID,(pathID,breakPos) in enumerate(pathBreakCoordPairs):\n",
    "        print(f'\\rProcessing path break number {breakID+1}/{numPathBreaks}',end='')\n",
    "        leftLink = (pathNodeArray[pathID,breakPos],pathNodeArray[pathID,breakPos+1])\n",
    "        \n",
    "        if combinedNodeDirArray[pathID,breakPos]<0:\n",
    "            # Inverted node\n",
    "            nextExpectedNode = -1*pathNextNode[pathID].inverse.get(leftLink[0],[leftLink[0]-1])[0]\n",
    "        else:\n",
    "            # normal node\n",
    "            nextExpectedNode = pathNextNode[pathID].get(leftLink[0],leftLink[0]+1)\n",
    "\n",
    "        nextPos = findNextNode(nextExpectedNode,combinedNodeDirArray[pathID,breakPos+1:pathLengths[pathID]])+breakPos+1\n",
    "\n",
    "        blockList = []\n",
    "\n",
    "        if len(nextPos)>0:\n",
    "            # There is the node next-in-order after the break\n",
    "            returnPos = np.sort(nextPos)[0]\n",
    "\n",
    "            # Is it even possible???\n",
    "            if returnPos==breakPos+1:\n",
    "                continue\n",
    "\n",
    "            rightLink = (pathNodeArray[pathID,returnPos-1],pathNodeArray[pathID,returnPos])\n",
    "            substituteLink = (pathNodeArray[pathID,breakPos],pathNodeArray[pathID,returnPos])\n",
    "\n",
    "            # I suspect the first condition cannot be violated by constructtion of the leftLink (always non-linear) \n",
    "            # and substitute link (always linear)\n",
    "            if leftLink!=substituteLink and rightLink!=substituteLink:\n",
    "                blockList,isGaps = extractGapsBlocks(np.unique(pathNodeArray[pathID,breakPos+1:returnPos]).tolist(),pathNodeArray[pathID],nodeLengths)\n",
    "                #################################################################################################\n",
    "                # !!!IMPORTANT!!!: When the link is substituted during generation of next layer, if the new link \n",
    "                # is not (k,k+1) (or (k+1,k) for inverted nodes), then for each path it has to be checked whether\n",
    "                # follow-through links through empty components can be established. Implemented, but need tested.\n",
    "                #################################################################################################\n",
    "\n",
    "                # Adding paired links with their substitution\n",
    "                pairedLinks.setdefault(leftLink,{})[rightLink] = substituteLink\n",
    "                pairedLinks.setdefault(rightLink,{})[leftLink] = substituteLink\n",
    "                usedSecondInPairPath.setdefault(pathID,set()).add(rightLink)\n",
    "\n",
    "                processDoublePairedLinks(leftLink,rightLink,pathID,doublePairedLinks,pairedLinks,interconnectedLinks,linksBlocks,pathNextNode)\n",
    "\n",
    "                addToLinkPool(leftLink,rightLink,interconnectedLinks)\n",
    "                recordLinkBlockAssociation(rightLink,blockList,linksBlocks)\n",
    "            else:\n",
    "                # Paired link is actually not a paired link. Substituting with one of the pair.\n",
    "                \n",
    "                # It is impossible condition. Do I need it here?\n",
    "                if leftLink==substituteLink:\n",
    "                    leftLink = rightLink\n",
    "                \n",
    "                blockList = processPseudoPair(breakPos,returnPos,pathID,\n",
    "                                              pathNodeArray,combinedNodeDirArray,pathNextNode,nodeLengths,\n",
    "                                              usedSecondInPairPath,pairedLinks,linksBlocks)\n",
    "                    \n",
    "        else:\n",
    "            #########################\n",
    "            # Search for loop position ONLY for main link of the same direction (if both from and to are either forward or inverted)\n",
    "            # Additional condition is that it should be backward link (constituting possible repeat start)\n",
    "            # If they are of different direction or not reversed, then treat as single link!\n",
    "            #########################\n",
    "            if ((combinedNodeDirArray[pathID,breakPos]>0 and leftLink[0]>leftLink[1]) or \\\n",
    "               (combinedNodeDirArray[pathID,breakPos]<0 and leftLink[0]<leftLink[1])) and \\\n",
    "               np.prod(combinedNodeDirArray[pathID,breakPos:breakPos+2])>0:\n",
    "                loopPos = findNextNode(combinedNodeDirArray[pathID,breakPos],combinedNodeDirArray[pathID,breakPos+1:pathLengths[pathID]])+breakPos+1\n",
    "            else:\n",
    "                loopPos = []\n",
    "\n",
    "            if len(loopPos)>0:\n",
    "                # Should happen the same as with pseudo paired links above except all searches should happen in breakPos+1:returnPos+1\n",
    "                returnPos = loopPos[0]\n",
    "                blockList = processPseudoPair(breakPos,returnPos+1,pathID,\n",
    "                                              pathNodeArray,combinedNodeDirArray,pathNextNode,nodeLengths,\n",
    "                                              usedSecondInPairPath,pairedLinks,linksBlocks)\n",
    "            else:\n",
    "            \n",
    "                # There is no node next-in-order after the break\n",
    "                # Process a single link as individual\n",
    "                blockList = processIndividualLink(leftLink,pathID,pathNodeInversionRate,pathNextNode,doublePairedLinks)\n",
    "\n",
    "                # Finding associated link in path, which are not paired, but interconnected.\n",
    "                # That is aimed at identifying paired inversed links (start and end of inversed block)\n",
    "                curLinkFrom = combinedNodeDirArray[pathID,breakPos]\n",
    "                curLinkFromDir = np.sign(curLinkFrom) # 1 for normal and -1 for inverted\n",
    "                curLinkFrom *= curLinkFromDir\n",
    "                curLinkTo = combinedNodeDirArray[pathID,breakPos+1]\n",
    "                curLinkToDir = np.sign(curLinkTo) # 1 for normal and -1 for inverted\n",
    "                curLinkTo *= curLinkToDir\n",
    "                \n",
    "                if curLinkFromDir!=curLinkToDir:\n",
    "                    if curLinkFromDir>0:\n",
    "                        potentialNextFrom = -1*curLinkFromDir*pathNextNode[pathID].get(curLinkFrom,curLinkFrom+1)\n",
    "                        potentialNextTo = -1*curLinkToDir*pathNextNode[pathID].get(curLinkTo,curLinkTo+1)\n",
    "                        nextPosFrom = np.where(combinedNodeDirArray[pathID,breakPos:pathLengths[pathID]]==potentialNextFrom)[0]+breakPos\n",
    "                        nextPosTo = np.where(combinedNodeDirArray[pathID,breakPos+1:pathLengths[pathID]]==potentialNextTo)[0]+breakPos+1\n",
    "                    else:\n",
    "                        potentialNextFrom = -1*curLinkFromDir*pathNextNode[pathID].inverse.get(curLinkFrom,[curLinkFrom-1])[0]\n",
    "                        potentialNextTo = -1*curLinkToDir*pathNextNode[pathID].inverse.get(curLinkTo,[curLinkTo-1])[0]\n",
    "                        nextPosFrom = np.where(combinedNodeDirArray[pathID,breakPos+1:pathLengths[pathID]]==potentialNextFrom)[0][::-1]+breakPos+1\n",
    "                        nextPosTo = np.where(combinedNodeDirArray[pathID,breakPos+2:pathLengths[pathID]]==potentialNextTo)[0][::-1]+breakPos+2\n",
    "                    \n",
    "                    \n",
    "                    for posFrom in nextPosFrom:\n",
    "                        fromLink,toLink = pathNodeArray[pathID,posFrom:posFrom+2]\n",
    "                        if curLinkFrom+curLinkFromDir==fromLink:\n",
    "                            addToLinkPool(leftLink,(fromLink,toLink),interconnectedLinks)\n",
    "                            break\n",
    "                        \n",
    "                    for posTo in nextPosTo:\n",
    "                        fromLink,toLink = pathNodeArray[pathID,posTo-1:posTo+1]\n",
    "                        if curLinkTo+curLinkFromDir==toLink:\n",
    "                            addToLinkPool(leftLink,(fromLink,toLink),interconnectedLinks)\n",
    "                            break\n",
    "                            \n",
    "        # processStartsEnds(leftLink,linkStarts,linkEnds,interconnectedLinks,forwardLinks)    \n",
    "        if len(blockList)>0:\n",
    "            \n",
    "            recordLinkBlockAssociation(leftLink,blockList,linksBlocks)\n",
    "    postprocessLinksBlocks(linksBlocks,interconnectedLinks)\n",
    "    \n",
    "    print('\\nProcessing path breaks finished.')\n",
    "    return linksBlocks,pairedLinks,interconnectedLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72baaf10",
   "metadata": {},
   "source": [
    "### Rearrangement blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def addBlockEdge(edge,size,blockEdges):\n",
    "    curBlockSize = blockEdges.get(edge,0)\n",
    "    blockEdges[edge] = max(curBlockSize,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54842d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identifyRearrangementBlocks(nodesStructure,nodeLengths):\n",
    "    '''\n",
    "    block Edges is a dict with a structure:\n",
    "    <edge of block>:<size of block>\n",
    "    <edge of block> pointing to the node before (!) the break.\n",
    "    In other words, if it is the start of the block, it will point to the node just before the block,\n",
    "    and if it is the end of the block, it will point to the last node of the block.\n",
    "    '''\n",
    "    \n",
    "    blockEdges = {}\n",
    "    \n",
    "    structQueue = []\n",
    "    indQueue = []\n",
    "    nodeNum = len(nodesStructure)\n",
    "    print('Identifying rearrangement blocks')\n",
    "    for nodeId,nodeStruct in enumerate(nodesStructure):\n",
    "        print(f'\\rProcessing node {nodeId+1}/{nodeNum}',end='')\n",
    "        \n",
    "        try:\n",
    "            blockStartInd = structQueue.index(nodeStruct)\n",
    "\n",
    "            blockStart = indQueue[blockStartInd]\n",
    "            blockEnd = nodeId+1\n",
    "            if blockStart+1==blockEnd:\n",
    "                indQueue[-1] = nodeId+1\n",
    "                continue\n",
    "            del structQueue[blockStartInd:]\n",
    "            del indQueue[blockStartInd:]\n",
    "            blockSize = np.sum([nodeLengths[node-1] for node in range(blockStart+1,blockEnd)])\n",
    "            addBlockEdge(blockStart,blockSize,blockEdges)\n",
    "            addBlockEdge(blockEnd-1,blockSize,blockEdges)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        structQueue.append(nodeStruct)\n",
    "        indQueue.append(nodeId+1)\n",
    "    \n",
    "    print('\\nIdentifying rearrangement blocks finished.')\n",
    "    return blockEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a81e7",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def getRemovableStructures(graph = None,\n",
    "                           nodeLengths=None, pathLengths = None, \n",
    "                           pathNodeArray = None, pathDirArray = None,\n",
    "                           pathNextNode = None, forwardLinks = None,\n",
    "                           inversionThreshold=0.5):\n",
    "    # Preparing paths for link-block identifications\n",
    "    \n",
    "    if nodeLengths is None or pathLengths is None or pathNodeArray is None or pathDirArray is None and pathNextNode is not None or forwardLinks is None:\n",
    "        if graph is not None:\n",
    "            forwardLinks = graph.forwardLinks\n",
    "            \n",
    "            nodeLengths = calcNodeLengths(graph)\n",
    "\n",
    "            pathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n",
    "        else:\n",
    "            raise AttributeError(f'Either `graph` or `nodeLengths`, `pathLengths`, `pathNodeArray` and `pathDirArray` should be provided.')\n",
    "\n",
    "    pathNodeInversionRate,nodesStructure,combinedNodeDirArray = getNodesStructurePathNodeInversionRate(pathNodeArray,pathDirArray,pathLengths,inversionThreshold=inversionThreshold)\n",
    "\n",
    "    pathNextNode = getNextNodePath(pathNodeArray,pathLengths)\n",
    "    \n",
    "    pathBreakCoordPairs = identifyPathBreaks(combinedNodeDirArray,pathLengths,pathNextNode)\n",
    "    \n",
    "    linksBlocks,pairedLinks,interconnectedLinks = processPathBreaks(pathBreakCoordPairs,\n",
    "                      pathNodeArray,pathNextNode,combinedNodeDirArray,pathNodeInversionRate,pathLengths,nodeLengths,forwardLinks)\n",
    "    \n",
    "    linkLengths = convertBlocksToLengths(linksBlocks,nodeLengths)\n",
    "    \n",
    "    blockEdges = identifyRearrangementBlocks(nodesStructure,nodeLengths)\n",
    "    \n",
    "    return linkLengths,pairedLinks,interconnectedLinks,blockEdges,pathNodeInversionRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7476e-a0fd-4c65-83f0-aa84a421ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def getBlockEdges(graph = None,\n",
    "                   nodeLengths=None, pathLengths = None, \n",
    "                   pathNodeArray = None, pathDirArray = None,\n",
    "                   pathNextNode = None, forwardLinks = None,\n",
    "                   inversionThreshold=0.5):\n",
    "    # Preparing paths for link-block identifications\n",
    "    \n",
    "    if nodeLengths is None or pathLengths is None or pathNodeArray is None or pathDirArray is None and pathNextNode is not None or forwardLinks is None:\n",
    "        if graph is not None:\n",
    "            forwardLinks = graph.forwardLinks\n",
    "            \n",
    "            nodeLengths = calcNodeLengths(graph)\n",
    "\n",
    "            pathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n",
    "        else:\n",
    "            raise AttributeError(f'Either `graph` or `nodeLengths`, `pathLengths`, `pathNodeArray` and `pathDirArray` should be provided.')\n",
    "\n",
    "    pathNodeInversionRate,nodesStructure,combinedNodeDirArray = getNodesStructurePathNodeInversionRate(pathNodeArray,pathDirArray,pathLengths,inversionThreshold=inversionThreshold)\n",
    "    \n",
    "    blockEdges = identifyRearrangementBlocks(nodesStructure,nodeLengths)\n",
    "    \n",
    "    return blockEdges,pathNodeInversionRate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92bcb0",
   "metadata": {},
   "source": [
    "## Generating zoom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335effd-d3f2-4796-86d3-06547ba973d4",
   "metadata": {},
   "source": [
    "This set of functions (with `nextLayerZoom` being main orchestration function) doing the job of generating next zoom level by collapsing\n",
    "columns and then components together after smaller non-linear links are removed (by different set of functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a7213",
   "metadata": {},
   "source": [
    "### Finalising bin and component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def addLink(fromComp,fromStrand,toComp,toStrand,pathList,fromComponentLinks,toComponentLinks):\n",
    "    fromComponentLinks.setdefault(fromComp,{}).setdefault(fromStrand,{}).\\\n",
    "                       setdefault(toComp,{}).setdefault(toStrand,set()).\\\n",
    "                       update(pathList)\n",
    "    toComponentLinks.setdefault(toComp,{}).setdefault(toStrand,{}).\\\n",
    "                     setdefault(fromComp,{}).setdefault(fromStrand,set()).\\\n",
    "                     update(pathList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getOccInvChange(binColLengths,binBlockLength,binOcc,binInv,prevOcc,prevInv,inversionThreshold=0.5):\n",
    "    occChanged = False\n",
    "    invChanged = False\n",
    "    occ = {}\n",
    "    inv = {}\n",
    "    \n",
    "    for pathID in binOcc:\n",
    "        \n",
    "        # Averaging occupancy\n",
    "        occ[pathID] = sum([bl*bo for bl,bo in zip(binColLengths,binOcc[pathID])])/binBlockLength\n",
    "        # Do comparison through floor and then abs difference > 0\n",
    "        if np.abs(np.floor(occ[pathID]+0.5)-np.floor(prevOcc.get(pathID,occ[pathID])+0.5))>0 \\\n",
    "            and occ[pathID]>0.5 and prevOcc.get(pathID,occ[pathID])>0.5:\n",
    "            occChanged = True\n",
    "        prevOcc[pathID] = occ[pathID]\n",
    "        \n",
    "        # Averaging invertion\n",
    "        inv[pathID] = sum([bl*bo*bi for bl,bo,bi in zip(binColLengths,binOcc[pathID],binInv[pathID])])/(binBlockLength*occ[pathID])\n",
    "        if (inv[pathID]-inversionThreshold)*(prevInv.get(pathID,inv[pathID])-inversionThreshold)<0 or \\\n",
    "        (inv[pathID]-inversionThreshold)*(prevInv.get(pathID,inv[pathID])-inversionThreshold)==0 and \\ \n",
    "        inv[pathID]*prevInv.get(pathID,inv[pathID])>inversionThreshold*inversionThreshold:\n",
    "            # The second comdition after `or` is taking the case where one is equal to inversionThreshold\n",
    "            # and another is more than inversionThreshold.\n",
    "            invChanged = True\n",
    "        prevInv[pathID] = inv[pathID]\n",
    "        \n",
    "    return occChanged,invChanged,occ,inv,prevOcc,prevInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def getOccInv(binColLengths,binBlockLength,binOcc,binInv,inversionThreshold=0.5):\n",
    "    occ = {}\n",
    "    inv = {}\n",
    "    \n",
    "    for pathID in binOcc:\n",
    "        # Averaging occupancy\n",
    "        occ[pathID] = sum([bl*bo for bl,bo in zip(binColLengths,binOcc[pathID])])/binBlockLength\n",
    "        \n",
    "        # Averaging invertion\n",
    "        inv[pathID] = sum([bl*bo*bi for bl,bo,bi in zip(binColLengths,binOcc[pathID],binInv[pathID])])/(binBlockLength*occ[pathID])\n",
    "\n",
    "    return occ,inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6949a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def combineIntervals(posPath):\n",
    "    posArray = np.array(posPath)\n",
    "    posArray = posArray[np.argsort(posArray[:,0]),:]\n",
    "    posIntersect = (posArray[1:,1]-(posArray[:-1,0]-1))*\\\n",
    "                    (posArray[:-1,1]-(posArray[1:,0]-1))\n",
    "    newPos = [[posArray[0,0]]]\n",
    "    candidates = [posArray[0,1]]\n",
    "    for jointNum in range(len(posIntersect)):\n",
    "        if posIntersect[jointNum]>=0:\n",
    "            candidates.extend(posArray[jointNum+1,:].tolist())\n",
    "        else:\n",
    "            newPos[-1].append(np.max(candidates))\n",
    "            newPos.append([posArray[jointNum+1,0]])\n",
    "            candidates = [posArray[jointNum+1,1]]\n",
    "\n",
    "    newPos[-1].append(np.max(candidates))    \n",
    "    return newPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recordBinZoom(occ,inv,binPosArray,nBins,nCols,\n",
    "                  binBlockLength,binBlockLengths,\n",
    "                  binColLengths,\n",
    "                  binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                  matrix,inversionThreshold=0.5):\n",
    "    # need to check for occupancy and inversion change in comparison with previous bin\n",
    "    # and if that happens, break the component before this bin (should be special boolean returned.\n",
    "    \n",
    "    for pathID in binPosArray:\n",
    "        pathMatrix = matrix.setdefault(pathID,[[],[]])\n",
    "        pathMatrix[0].append(nBins)\n",
    "        \n",
    "        # Adding everything to matrix element with Combined positions and Annotations (already combined through set)\n",
    "        pathMatrix[1].append([occ[pathID],inv[pathID],combineIntervals(binPosArray[pathID])])\n",
    "    \n",
    "    binBlockLengths.append(binBlockLength)\n",
    "    \n",
    "    binColStarts.append(binColStart)\n",
    "    binColEnds.append(binColEnd)\n",
    "    \n",
    "    return 0,[],binColStarts,binColEnds,{},{},{},nBins+1,nCols+binBlockLength,binBlockLengths,matrix\n",
    "    # binBlockLength,binColLengths,binColStarts,binColEnds,\n",
    "    # binOcc,binInv,binPosArray,binAnn,nBins,nCols,\n",
    "    # binBlockLengths,matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def getAverageInv(binBlockLengths,matrixPathArray):\n",
    "    num = 0\n",
    "    dem = 0\n",
    "    for blockCols,matrixEl in zip(binBlockLengths,matrixPathArray[1]):\n",
    "        num += blockCols*matrixEl[1]\n",
    "        dem += blockCols\n",
    "    return num/dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba160f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finaliseComponentZoom(component,components,componentLengths,#componentNucleotides,\n",
    "                          nBins,nCols,occupants,\n",
    "                          binBlockLengths,binColStarts,binColEnds,\n",
    "                          matrix,starts,ends,\n",
    "                          forwardPaths,invertedPaths,\n",
    "                          compInvNum,compInvDen,inversionThreshold=0.5):\n",
    "\n",
    "    componentLengths.append(nBins)\n",
    "    component[\"matrix\"].extend(sorted([[pathID,\n",
    "                                 (compInv := int(compInvNum[pathID]/compInvDen[pathID]>inversionThreshold)),\n",
    "                                 [matrixPathArray[0],matrixPathArray[1][::(-1 if compInv else 1)]]] \\\n",
    "                                for pathID,matrixPathArray in matrix.items()],key=lambda el: el[0]))\n",
    "    \n",
    "    component['binsToCols'] = binBlockLengths\n",
    "    component[\"occupants\"] = sorted(list(matrix.keys()))\n",
    "    \n",
    "    component['last_bin'] = component['first_bin'] + nBins - 1\n",
    "    \n",
    "    component['firstCol'] = min(binColStarts)\n",
    "    component['lastCol'] = max(binColEnds)\n",
    "    \n",
    "    # Is it needed?\n",
    "    component['binColStarts'] = binColStarts\n",
    "    component['binColEnds'] = binColEnds\n",
    "    \n",
    "    curStarts = starts.intersection(forwardPaths)\n",
    "    if len(curStarts)>0:\n",
    "        component['starts'] = list(curStarts)\n",
    "    starts -= set(forwardPaths)\n",
    "        \n",
    "    curEnds = ends.intersection(invertedPaths)\n",
    "    if len(curEnds)>0:\n",
    "        component['ends'] = list(curEnds)\n",
    "    ends -= set(invertedPaths)    \n",
    "        \n",
    "    firstBin = component['last_bin'] + 1\n",
    "    components.append(component)\n",
    "    component = deepcopy(componentTemplate)\n",
    "    component['first_bin'] = firstBin\n",
    "    return component,components,componentLengths,0,0,set(),[],[],[],{},starts,ends,{},{}\n",
    "    # component,components,componentLengths,(componentNucleotides),nBins,nCols,occupants,\n",
    "    # binBlockLengths,binColStarts,binColEnds,matrix,starts,ends,compInvNum,compInvDen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finaliseBinZoom(compNum,\n",
    "                    binOcc,binInv,binPosArray,\n",
    "                    nBins,nCols,\n",
    "                    binBlockLength,binBlockLengths,binColLengths,\n",
    "                    binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                    matrix,\n",
    "                    newComponent,newComponents,newComponentLengths,#compAccDir,#newComponentNucleotides,\n",
    "                    newFromComponentLinks,newToComponentLinks,\n",
    "                    occupants,linkLengths,\n",
    "                    starts,ends,\n",
    "                    forwardPaths,invertedPaths,pathsToInversion,\n",
    "                    newToOldInd,oldToNewInd,\n",
    "                    inversionThreshold=0.5):\n",
    "    \n",
    "    # Consider removing this condition and joining all bins together. But not now :)\n",
    "    occ,inv = getOccInv(binColLengths,binBlockLength,binOcc,binInv,inversionThreshold=inversionThreshold)\n",
    "    \n",
    "    for pathID in pathsToInversion:\n",
    "        invertedPaths.add(pathID)\n",
    "        forwardPaths -= set([pathID])\n",
    "\n",
    "    \n",
    "    binBlockLength,binColLengths,binColStarts,binColEnds,binOcc,binInv,binPosArray,nBins,nCols,binBlockLengths,matrix = \\\n",
    "    recordBinZoom(occ,inv,binPosArray,\n",
    "                    nBins,nCols,binBlockLength,binBlockLengths,binColLengths,\n",
    "                    binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                    matrix,\n",
    "                    inversionThreshold=inversionThreshold)\n",
    "    \n",
    "    return binColLengths,binColStarts,binColEnds,\\\n",
    "            binOcc,binInv,binPosArray,nBins,nCols,\\\n",
    "            binBlockLength,binBlockLengths,matrix,\\\n",
    "            newComponent,newComponents,newComponentLengths,\\\n",
    "            newFromComponentLinks,newToComponentLinks,\\\n",
    "            occupants,\\\n",
    "            starts,ends,newToOldInd,oldToNewInd,\\\n",
    "            forwardPaths,invertedPaths,set() # The last one is pathsToInversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbd658",
   "metadata": {},
   "source": [
    "### Break component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1691fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def getMatrixPathElement(matrix,pathID):\n",
    "    res = [el for el in matrix if el[0]==pathID]\n",
    "    if len(res)==1:\n",
    "        return res[0]\n",
    "    elif len(res)>0:\n",
    "        warnings.warn(f\"More than one element for path {pathID} is found!\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkChange(compNum,components,zoomLevel,blockEdges):\n",
    "    doBreak = False\n",
    "    curComp = components[compNum]\n",
    "    \n",
    "    if compNum+1 in blockEdges:\n",
    "        return True\n",
    "    \n",
    "    return doBreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def joinComponents(leftComp,rightComp, maxLengthComponent, inversionThreshold=0.5):\n",
    "    '''\n",
    "    !!!  Currently not used\n",
    "    \n",
    "    If the joining was successful, the function will return a joined component.\n",
    "    \n",
    "    If the joining was not successful and was aborted for one of the following reasons, it will return a list of original components. \n",
    "    The reasons for aborting the joining can be the following:\n",
    "    - In one of the paths the invertion is lower than threshold in one component and higher in the other.\n",
    "    - Left component contains at least one end\n",
    "    - Right component contains at least one start\n",
    "    \n",
    "    The function will not check links for coming or going on the right of the left component and left of the right component. \n",
    "    It will just get left links from left component and right links from right component and assign them to the new component.\n",
    "    '''\n",
    "    \n",
    "    if leftComp['last_bin']-leftComp['first_bin']+1 + rightComp['last_bin']-rightComp['first_bin']+1 > maxLengthComponent:\n",
    "        return [leftComp,rightComp]\n",
    "    \n",
    "    if leftComp.get('ends',False):\n",
    "        # End of a path\n",
    "        return [leftComp,rightComp]\n",
    "    \n",
    "    newComp = {}\n",
    "    newComp['first_bin'] = min(leftComp['first_bin'],rightComp['first_bin'])\n",
    "    newComp['last_bin'] = max(leftComp['last_bin'],rightComp['last_bin'])\n",
    "    newComp['firstCol'] = min(leftComp['firstCol'],rightComp['firstCol'])\n",
    "    newComp['lastCol'] = max(leftComp['lastCol'],rightComp['lastCol'])\n",
    "    \n",
    "    leftCompNumBins = leftComp['last_bin']-leftComp['first_bin']+1\n",
    "    \n",
    "    newComp['occupants'] = list(set(leftComp['occupants']).union(rightComp['occupants']))\n",
    "    \n",
    "    for pathID in newComp['occupants']:\n",
    "        leftPathElement = getMatrixPathElement(leftComp['matrix'],pathID)\n",
    "        rightPathElement = getMatrixPathElement(rightComp['matrix'],pathID)\n",
    "        if leftPathElement is None and rightPathElement is None:\n",
    "            continue\n",
    "        \n",
    "        if leftPathElement is None:\n",
    "            if len([el for el in rightPathElement[2][1] if el[2][0][0]==1 or el[2][-1][0]==1])>0:\n",
    "                # Start of a path\n",
    "                return [leftComp,rightComp]\n",
    "            rightPathElement[2][0] = [el+leftCompNumBins for el in rightPathElement[2][0]]\n",
    "            newComp.setdefault(\"matrix\",[]).append(rightPathElement)\n",
    "            continue\n",
    "        \n",
    "        if rightPathElement is None:\n",
    "            newComp.setdefault(\"matrix\",[]).append(leftPathElement)\n",
    "            continue\n",
    "        \n",
    "        if (leftPathElement[1]>inversionThreshold and rightPathElement[1]<=inversionThreshold) or \\\n",
    "           (leftPathElement[1]<=inversionThreshold and rightPathElement[1]>inversionThreshold):\n",
    "            return [leftComp,rightComp]\n",
    "        \n",
    "        newPathElement = []\n",
    "        newPathElement.append(pathID)\n",
    "        newPathElement.append(leftPathElement[1])\n",
    "        pathMatrix = []\n",
    "        pathMatrix.append(leftPathElement[2][0] + [el+leftCompNumBins for el in rightPathElement[2][0]])\n",
    "        pathMatrix.append(leftPathElement[2][1] + rightPathElement[2][1])\n",
    "        newPathElement.append(pathMatrix)\n",
    "        newComp.setdefault(\"matrix\",[]).append(newPathElement)\n",
    "        \n",
    "    newComp['larrivals'] = leftComp['larrivals']\n",
    "    newComp['ldepartures'] = leftComp['ldepartures']\n",
    "    newComp['rarrivals'] = rightComp['rarrivals']\n",
    "    newComp['rdepartures'] = rightComp['rdepartures']\n",
    "    ends = list(set(leftComp.get('ends',[])).union(rightComp.get('ends',[])))\n",
    "    if len(ends)>0:\n",
    "        newComp['ends'] = ends\n",
    "    \n",
    "    return newComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkLinksZoom(compNum,fromComponentLinks,toComponentLinks):\n",
    "    # Check only outgoing from the right and incoming to the right\n",
    "    # and check outgoing from the left on and incoming to the left on the next node.\n",
    "    if any(['-' in toNodeDicts for toNodeDicts in fromComponentLinks.get(compNum+1,{}).get('+',{}).values()]):\n",
    "        # If there is an outgoing link from positive block (right) to negative block (anywhere)\n",
    "        return True\n",
    "\n",
    "    if any(['+' in fromNodeDicts for fromNodeDicts in toComponentLinks.get(compNum+1,{}).get('-',{}).values()]):\n",
    "        # If there is an incoming link to the negative block (right) from positive block (anywhere)\n",
    "        return True\n",
    "    \n",
    "    if any([toNodes!=compNum+2 for toNodes in fromComponentLinks.get(compNum+1,{}).get('+',{}).keys()]):\n",
    "        # If there is an outgoing link from positive block (right) to anywhere \n",
    "        # (positive block because negative downstream was excluded above) except the next component\n",
    "        return True\n",
    "    \n",
    "    if any([fromNodes!=compNum+2 for fromNodes in toComponentLinks.get(compNum+1,{}).get('-',{}).keys()]):\n",
    "        # If there is an incoming link to negative block (right) from anywhere\n",
    "        # (negavive block because positive upstream was excluded above) except the next component\n",
    "        return True\n",
    "    \n",
    "    if any([fromNodes!=compNum+1 or '-' in fromNodesDict for fromNodes,fromNodesDict in toComponentLinks.get(compNum+2,{}).get('+',{}).items()]):\n",
    "        # If there is an incoming link to positive block of next component (left) from anywhere except current component\n",
    "        return True\n",
    "    \n",
    "    if any([toNodes!=compNum+1 or '+' in toNodesDict for toNodes,toNodesDict in fromComponentLinks.get(compNum+2,{}).get('-',{}).items()]):\n",
    "        # If there is an outgoing link from negative block (left) to anywhere except current component\n",
    "        return True\n",
    "    \n",
    "    if not (('+' in fromComponentLinks.get(compNum+1,{}).get('+',{}).get(compNum+2,{})) or \\\n",
    "            ('-' in toComponentLinks.get(compNum+1,{}).get('-',{}).get(compNum+2,{}))):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkForBreaksZoom(zoomLevel,compNum,components,fromComponentLinks,toComponentLinks, blockEdges):\n",
    "    if compNum<len(components)-1:\n",
    "        breakByLinks = checkLinksZoom(compNum,fromComponentLinks,toComponentLinks)\n",
    "        breakByChange = checkChange(compNum,components,zoomLevel, blockEdges)\n",
    "    else:\n",
    "        return [false]\n",
    "    return [breakByLinks,breakByChange]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307439c",
   "metadata": {},
   "source": [
    "### Update links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def splitPositiveNegative(compID,accs,components):\n",
    "    '''\n",
    "    This function simply pulls all accession presented in the component and split them into forward and inversed.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `compID`: int. Number of the component in the current zoom layer (0-based).\n",
    "    `accs`: int or Iterable. Should provide either overall number of accession/paths in the graph or \n",
    "            a list of all (intended) accessions for the given component. It is used only for carrying over links \n",
    "            through empty components for some accessions.\n",
    "    `components`: list[dict]. List of component dictionaries, one of the main data structure representing zoom layer.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    `posAcc`: list[int]. IDs of accession which has forward direction in given component.\n",
    "    `negAcc`: list[int]. IDs of accession which has inverse direction in given component.\n",
    "    \n",
    "    '''\n",
    "    if isinstance(accs,int):\n",
    "        emptyAcc = set(range(accs))\n",
    "    elif isinstance(accs,Iterable):\n",
    "        emptyAcc = set(accs)\n",
    "    else:\n",
    "        raise TypeError(f'`accs` should be either int or Iterable, but {type(accs)} was given.')\n",
    "    posAcc = []\n",
    "    negAcc = []\n",
    "    for pathID,pathInversion,_ in components[compID]['matrix']:\n",
    "        emptyAcc -= set([pathID])\n",
    "        if pathInversion==1:\n",
    "            negAcc.append(pathID)\n",
    "        else:\n",
    "            posAcc.append(pathID)\n",
    "    \n",
    "    return posAcc,negAcc,emptyAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def intersectAccLists(accList,dirDict):\n",
    "    overallLinkAccList = set()\n",
    "    for linkAccList in dirDict.values():\n",
    "        overallLinkAccList.update(linkAccList)\n",
    "    return set(accList).intersection(overallLinkAccList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def updateLinks(newToOldInd,oldToNewInd,\n",
    "                fromComponentLinks,toComponentLinks,\n",
    "                linkLengths,pairedLinks,interconnectedLinks,blockEdges,accStarts,accEnds,components,compAccDir,\n",
    "                newFromComponentLinks={},newToComponentLinks={}):\n",
    "    '''\n",
    "    newToOldInd and oldToNewInd: both index and values are 0-based numbers of components \n",
    "    in previous and current zoomlayer.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for newComp,oldCompList in enumerate(newToOldInd):\n",
    "        leftOldCompId = oldCompList[0] + 1\n",
    "        rightOldCompId = oldCompList[-1] + 1\n",
    "        newCompId = newComp + 1\n",
    "        \n",
    "        newCompPosAcc,newCompNegAcc,newCompEmptyAcc = splitPositiveNegative(newComp,list(accStarts.keys()),components)\n",
    "        \n",
    "        # The next two if blocks set conditions for processing or not processing left and right of the current new component\n",
    "        # The side should not be processed if the end of the previous node is coming from the same component \n",
    "        # (on lower zoom level) as the begininng of the current component.\n",
    "        if newComp>0:\n",
    "            if oldCompList[0]==newToOldInd[newComp-1][-1]:\n",
    "                doLeft = False\n",
    "            else:\n",
    "                doLeft = True\n",
    "        else:\n",
    "            doLeft = True\n",
    "            \n",
    "        if newComp<len(newToOldInd)-1:\n",
    "            if oldCompList[-1]==newToOldInd[newComp+1][0]:\n",
    "                doRight = False\n",
    "            else:\n",
    "                doRight = True\n",
    "        else:\n",
    "            doRight = True\n",
    "        \n",
    "        # Departure on the right (from positive block)\n",
    "        if doRight:\n",
    "            fromComp = rightOldCompId\n",
    "            for toCompDict in fromComponentLinks.get(fromComp,{}).values():\n",
    "                for toOldComp,toOldCompDirDict in toCompDict.items():\n",
    "                    # To positive strand\n",
    "                    # Getting to comp (new) id\n",
    "                    toNewCompId = oldToNewInd[toOldComp-1][0]\n",
    "                    toCompPosAcc,toCompNegAcc,toCompEmptyAcc = splitPositiveNegative(toNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the to comp (old) is on the left of the new comp\n",
    "                    if newToOldInd[toNewCompId][0]==toOldComp-1:\n",
    "                        # Getting accession list which come out from positive and goes to positive.\n",
    "                        accList = intersectAccLists(list(set(newCompPosAcc).intersection(toCompPosAcc)),\n",
    "                                                    toOldCompDirDict)\n",
    "\n",
    "                        if len(accList)>0:\n",
    "                            if not(toNewCompId+1==newCompId and \\\n",
    "                                (fromComp-1!=newToOldInd[toNewCompId][-1] or toOldComp-1!=newToOldInd[toNewCompId][0])):\n",
    "                                # If the link is self loop (which happens sometimes), then it has to come out from right component\n",
    "                                # and go to the left component, otherwise it should not be carried forward.\n",
    "                                addLink(newCompId,'+',toNewCompId+1,'+',\n",
    "                                        accList,\n",
    "                                        newFromComponentLinks,newToComponentLinks)\n",
    "                        \n",
    "                        ### Wrap creating empty links into a separate function!!!\n",
    "                        if fromComp+1==toOldComp and newCompId+1==toNewCompId+1:\n",
    "                            continuityLinks = set(fromComponentLinks.get(fromComp,{}).get('+',{}).get(fromComp+1,{}).get('+',[]))\n",
    "                            emptyLinks = (set(newCompPosAcc)|set(newCompEmptyAcc)).intersection(set(toCompPosAcc)|set(toCompEmptyAcc)).intersection(continuityLinks)\n",
    "                            if len(emptyLinks)>0:\n",
    "                                addLink(newCompId,'+',toNewCompId+1,'+',emptyLinks,newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                    # To negative strand\n",
    "                    # Getting to comp (new) id\n",
    "                    toNewCompId = oldToNewInd[toOldComp-1][-1]\n",
    "                    toCompPosAcc,toCompNegAcc,toCompEmptyAcc = splitPositiveNegative(toNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the to comp (old) is on the right of the new comp\n",
    "                    if newToOldInd[toNewCompId][-1]==toOldComp-1:\n",
    "                        # Getting accession list which come out from positive and goes to negative.\n",
    "                        accList = intersectAccLists(list(set(newCompPosAcc).intersection(toCompNegAcc)),\n",
    "                                                    toOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            addLink(newCompId,'+',toNewCompId+1,'-',\n",
    "                                    accList,\n",
    "                                    newFromComponentLinks,newToComponentLinks)\n",
    "        \n",
    "        # Departure on the left (from negative block)\n",
    "        if doLeft:\n",
    "            fromComp = leftOldCompId\n",
    "            for toCompDict in fromComponentLinks.get(fromComp,{}).values():\n",
    "                for toOldComp,toOldCompDirDict in toCompDict.items():\n",
    "                    # To positive strand\n",
    "                    # Getting to comp (new) id\n",
    "                    toNewCompId = oldToNewInd[toOldComp-1][0]\n",
    "                    toCompPosAcc,toCompNegAcc,toCompEmptyAcc = splitPositiveNegative(toNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the to comp (old) is on the left of the new comp\n",
    "                    if newToOldInd[toNewCompId][0]==toOldComp-1:\n",
    "                        # Getting accession list which come out from negative and goes to positive.\n",
    "                        accList = intersectAccLists(list(set(newCompNegAcc).intersection(toCompPosAcc)),\n",
    "                                                    toOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            addLink(newCompId,'-',toNewCompId+1,'+',\n",
    "                                    accList,\n",
    "                                    newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                    # to Negative strand\n",
    "                    # Getting to comp (new) id\n",
    "                    toNewCompId = oldToNewInd[toOldComp-1][-1]\n",
    "                    toCompPosAcc,toCompNegAcc,toCompEmptyAcc = splitPositiveNegative(toNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the to comp (old) is on the right of the new comp\n",
    "                    if newToOldInd[toNewCompId][-1]==toOldComp-1:\n",
    "                        accList = intersectAccLists(list(set(newCompNegAcc).intersection(toCompNegAcc)),\n",
    "                                                    toOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            if not (toNewCompId+1==newCompId and \\\n",
    "                                (fromComp-1!=newToOldInd[toNewCompId][0] or toOldComp-1!=newToOldInd[toNewCompId][-1])):\n",
    "                                # If the link is self loop (which happens sometimes), then it has to come out from left component\n",
    "                                # and go to the right component, otherwise it should not be carried forward.\n",
    "                                addLink(newCompId,'-',toNewCompId+1,'-',\n",
    "                                        accList,\n",
    "                                        newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                        if fromComp-1==toOldComp and newCompId-1==toNewCompId+1:\n",
    "                            continuityLinks = set(fromComponentLinks.get(fromComp,{}).get('-',{}).get(fromComp+1,{}).get('-',[]))\n",
    "                            emptyLinks = (set(newCompNegAcc)|set(newCompEmptyAcc)).intersection(set(toCompNegAcc)|set(toCompEmptyAcc)).intersection(continuityLinks)\n",
    "                            if len(emptyLinks)>0:\n",
    "                                addLink(newCompId,'-',toNewCompId+1,'-',emptyLinks,newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "\n",
    "        # Arrival on the left (to positive block)\n",
    "        if doLeft:\n",
    "            toComp = leftOldCompId\n",
    "            for fromCompDict in toComponentLinks.get(toComp,{}).values():\n",
    "                for fromOldComp,fromOldCompDirDict in fromCompDict.items():\n",
    "                    # From positive strand\n",
    "                    # Getting from comp (new) id\n",
    "                    fromNewCompId = oldToNewInd[fromOldComp-1][-1]\n",
    "                    fromCompPosAcc,fromCompNegAcc,fromCompEmptyAcc = splitPositiveNegative(fromNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the from comp (old) is on the right of the new comp\n",
    "                    if newToOldInd[fromNewCompId][-1]==fromOldComp-1:\n",
    "                        accList = intersectAccLists(list(set(newCompPosAcc).intersection(fromCompPosAcc)),\n",
    "                                                    fromOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            if not (fromNewCompId+1==newCompId and \\\n",
    "                                (fromOldComp-1!=newToOldInd[fromNewCompId][-1] or toComp-1!=newToOldInd[fromNewCompId][0])):\n",
    "                                # If the link is self loop (which happens sometimes), then it has to come out from right component\n",
    "                                # and go to the left component, otherwise it should not be carried forward.\n",
    "                                addLink(fromNewCompId+1,'+',newCompId,'+',\n",
    "                                        accList,\n",
    "                                        newFromComponentLinks,newToComponentLinks)\n",
    "                        if toComp-1==fromOldComp and newCompId-1==fromNewCompId+1:\n",
    "                            continuityLinks = set(toComponentLinks.get(toComp,{}).get('+',{}).get(toComp-1,{}).get('+',[]))\n",
    "                            emptyLinks = (set(newCompPosAcc)|set(newCompEmptyAcc)).intersection(set(fromCompPosAcc)|set(fromCompEmptyAcc)).intersection(continuityLinks)\n",
    "                            if len(emptyLinks)>0:\n",
    "                                addLink(fromNewCompId+1,'+',newCompId,'+',emptyLinks,newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                    # From negative strand\n",
    "                    # Getting to comp (new) id\n",
    "                    fromNewCompId = oldToNewInd[fromOldComp-1][0]\n",
    "                    fromCompPosAcc,fromCompNegAcc,fromCompEmptyAcc = splitPositiveNegative(fromNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the to comp (old) is on the left of the new comp\n",
    "                    if newToOldInd[fromNewCompId][0]==fromOldComp-1:\n",
    "                        accList = intersectAccLists(list(set(newCompPosAcc).intersection(fromCompNegAcc)),\n",
    "                                                    fromOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            addLink(fromNewCompId+1,'-',newCompId,'+',\n",
    "                                    accList,\n",
    "                                    newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "        if doRight:\n",
    "            # Arrival on the right (to negative block)\n",
    "#             for toComp in [leftOldCompId,rightOldCompId]:\n",
    "            toComp = rightOldCompId\n",
    "            for fromCompDict in toComponentLinks.get(toComp,{}).values():\n",
    "                for fromOldComp,fromOldCompDirDict in fromCompDict.items():\n",
    "                    # From positive strand\n",
    "                    # Getting from comp (new) id\n",
    "                    fromNewCompId = oldToNewInd[fromOldComp-1][-1]\n",
    "                    fromCompPosAcc,fromCompNegAcc,fromCompEmptyAcc = splitPositiveNegative(fromNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the from comp (old) is on the right of the new comp\n",
    "                    if newToOldInd[fromNewCompId][0]==fromOldComp-1:\n",
    "                        accList = intersectAccLists(list(set(newCompNegAcc).intersection(fromCompPosAcc)),\n",
    "                                                    fromOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            addLink(fromNewCompId+1,'+',newCompId,'-',\n",
    "                                    accList,\n",
    "                                    newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                    # From negative strand\n",
    "                    # Getting from comp (new) id\n",
    "                    fromNewCompId = oldToNewInd[fromOldComp-1][0]\n",
    "                    fromCompPosAcc,fromCompNegAcc,fromCompEmptyAcc = splitPositiveNegative(fromNewCompId,list(accStarts.keys()),components)\n",
    "                    # Check whether the from comp (old) is on the left of the new comp\n",
    "                    if newToOldInd[fromNewCompId][-1]==fromOldComp-1:\n",
    "                        accList = intersectAccLists(list(set(newCompNegAcc).intersection(fromCompNegAcc)),\n",
    "                                                    fromOldCompDirDict)\n",
    "                        if len(accList)>0:\n",
    "                            if not (fromNewCompId+1==newCompId and \\\n",
    "                                (fromOldComp-1!=newToOldInd[fromNewCompId][0] or toComp-1!=newToOldInd[fromNewCompId][-1])):\n",
    "                                # If the link is self loop (which happens sometimes), then it has to come out from left component\n",
    "                                # and go to the right component, otherwise it should not be carried forward.\n",
    "                                addLink(fromNewCompId+1,'-',newCompId,'-',\n",
    "                                        accList,\n",
    "                                        newFromComponentLinks,newToComponentLinks)\n",
    "\n",
    "                        if toComp+1==fromOldComp and newCompId+1==fromNewCompId+1:\n",
    "                            continuityLinks = set(toComponentLinks.get(toComp,{}).get('-',{}).get(toComp-1,{}).get('-',[]))\n",
    "                            emptyLinks = (set(newCompNegAcc)|set(newCompEmptyAcc)).intersection(set(fromCompNegAcc)|set(fromCompEmptyAcc)).intersection(continuityLinks)\n",
    "                            if len(emptyLinks)>0:\n",
    "                                addLink(fromNewCompId+1,'-',newCompId,'-',emptyLinks,newFromComponentLinks,newToComponentLinks)\n",
    "    \n",
    "    for acc in accStarts.keys():\n",
    "        accStarts[acc] = oldToNewInd[accStarts[acc]-1][0]+1\n",
    "        accEnds[acc] = oldToNewInd[accEnds[acc]-1][0]+1\n",
    "    \n",
    "    linkLengthsConv,pairedLinksConv,interconnectedLinksConv,blockEdgesConv = \\\n",
    "        convertRemovableComponents(oldToNewInd,linkLengths,pairedLinks,interconnectedLinks,blockEdges,fromComponentLinks)\n",
    "    return newFromComponentLinks,newToComponentLinks,accStarts,accEnds,linkLengthsConv,pairedLinksConv,interconnectedLinksConv,blockEdgesConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cf680",
   "metadata": {},
   "source": [
    "### Main layer generation function + assistant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def isStartEnd(compNum,components):\n",
    "    isBreak = False\n",
    "    leftComp = components[compNum]\n",
    "    if compNum<len(components)-1:\n",
    "        rightComp = components[compNum+1]\n",
    "    else:\n",
    "        rightComp = None\n",
    "    \n",
    "    \n",
    "    for path,compInvBin,pathMatrix in leftComp['matrix']:\n",
    "        if compInvBin==1:\n",
    "            if path in leftComp.get('starts',[]):\n",
    "                isBreak = True\n",
    "        else:\n",
    "            if path in leftComp.get('ends',[]):\n",
    "                isBreak = True\n",
    "    \n",
    "    if rightComp is not None:\n",
    "        for path,compInvBin,pathMatrix in rightComp['matrix']:\n",
    "            if compInvBin==1:\n",
    "                if path in rightComp.get('ends',[]):\n",
    "                    isBreak = True\n",
    "            else:\n",
    "                if path in rightComp.get('starts',[]):\n",
    "                    isBreak = True\n",
    "    \n",
    "    return isBreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nextLayerZoom(zoomLevel,components,componentLengths,#componentNucleotides,\n",
    "                  fromComponentLinks,toComponentLinks,graph,\n",
    "                  accStarts,accEnds,\n",
    "                  maxLengthComponent,\n",
    "                  linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                  inversionThreshold=0.5,\n",
    "                  debug=False,debugTime=False):\n",
    "\n",
    "    print('\\n===========================')\n",
    "    print(f'Zoom level {zoomLevel}')\n",
    "    print('===========================')\n",
    "    \n",
    "    numComponents = len(components)\n",
    "    numComponentsDigits = int(np.ceil(np.log10(numComponents)))\n",
    "    \n",
    "    newComponent = deepcopy(componentTemplate)\n",
    "    newComponents = []\n",
    "    newComponentLengths = []\n",
    "    newComponentNucleotides = []\n",
    "\n",
    "    newFromComponentLinks = {}\n",
    "    newToComponentLinks = {}\n",
    "    \n",
    "\n",
    "    occupants = set()\n",
    "    newToOldInd = [[]]\n",
    "    oldToNewInd = []\n",
    "    \n",
    "    binMeanOcc = {}\n",
    "    binMeanInv = {}\n",
    "    binOcc = {}\n",
    "    binInv = {}\n",
    "    binPosArray = {}\n",
    "    binAnn = {}\n",
    "    \n",
    "    binColStarts = []\n",
    "    binColEnds = []\n",
    "    binColLengths = []\n",
    "    binBlockLength = 0\n",
    "    binBlockLengths = []\n",
    "\n",
    "    matrix = {}\n",
    "    nBins = 0\n",
    "    nCols = 0\n",
    "\n",
    "    starts = set()\n",
    "    ends = set()\n",
    "\n",
    "    compInvNum = {}\n",
    "    compInvDen = {}\n",
    "    \n",
    "    compAccDir = {}\n",
    "    \n",
    "    for compNum,component in enumerate(components):\n",
    "        if debug or debugTime:\n",
    "            print(f'Processing component {compNum+1:0{numComponentsDigits}}/{numComponents:0{numComponentsDigits}}')\n",
    "        else:\n",
    "            print(f'\\rProcessing component {compNum+1:0{numComponentsDigits}}/{numComponents:0{numComponentsDigits}}',end='')\n",
    "        pathsToInversion = set()\n",
    "        forwardPaths = set(range(len(graph.accessions)))\n",
    "        invertedPaths = set()\n",
    "        \n",
    "        oldToNewInd.append([])\n",
    "        newToOldInd[-1].append(compNum)\n",
    "        occupants.update(component['occupants'])\n",
    "        starts.update(component.get('starts',[]))\n",
    "        ends.update(component.get('ends',[]))\n",
    "        compNumBins = component['last_bin']-component['first_bin']+1\n",
    "        for binNum in range(0,compNumBins):\n",
    "            if binBlockLength>0:\n",
    "                binColStart = min(binColStart,component['binColStarts'][binNum])\n",
    "                binColEnd = max(binColEnd,component['binColEnds'][binNum])\n",
    "            else:\n",
    "                binColStart = component['binColStarts'][binNum]\n",
    "                binColEnd = component['binColEnds'][binNum]\n",
    "\n",
    "            binColLengths.append(component['binsToCols'][binNum])\n",
    "            binBlockLength += binColLengths[-1]\n",
    "            # check this process for order of added bins\n",
    "            # compInvBin should be calculated on the fly as average of added bins weighted by colLength\n",
    "            # It should be kept until the component closed.\n",
    "            # If average inversion changes from one side to the other, then inverted paths should be changed,\n",
    "            # and possibly some lists should be reversed.\n",
    "            \n",
    "            for pathID,oldCompInv,pathMatrix in component['matrix']:\n",
    "                compInvBin = compInvNum.get(pathID,oldCompInv)/compInvDen.get(pathID,1)\n",
    "                if compInvBin==1:\n",
    "                    pathsToInversion.add(pathID)\n",
    "                    compAccDir.setdefault(compNum,{})[pathID]='-'\n",
    "                else:\n",
    "                    pathsToInversion.discard(pathID)\n",
    "                    compAccDir.setdefault(compNum,{})[pathID]='+'\n",
    "                occupiedBins = pathMatrix[0]\n",
    "                binsMatrix = pathMatrix[1]\n",
    "\n",
    "                try:\n",
    "                    # For inverted old components reading should happen from the other side to the front.\n",
    "                    if oldCompInv==1:\n",
    "                        binPos = occupiedBins.index(componentLengths[compNum] - 1 - binNum)\n",
    "                    else:\n",
    "                        binPos = occupiedBins.index(binNum)\n",
    "                        \n",
    "                    binOcc.setdefault(pathID,[]).append(binsMatrix[binPos][0])\n",
    "                    binInv.setdefault(pathID,[]).append(binsMatrix[binPos][1])\n",
    "                    binPosArray.setdefault(pathID,[]).extend(binsMatrix[binPos][2])\n",
    "                    \n",
    "                    compInvNum[pathID] = compInvNum.get(pathID,0) + binColLengths[-1]*binOcc[pathID][-1]*binInv[pathID][-1]\n",
    "                    compInvDen[pathID] = compInvDen.get(pathID,0) + binColLengths[-1]*binOcc[pathID][-1]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # Finding out the size of the next bin to add\n",
    "            if binNum+1<compNumBins:\n",
    "                #If this component (old) has not ended yet, then get the size of the next bin\n",
    "                nextBinColLength = component['binsToCols'][binNum+1]\n",
    "            elif compNum+1<len(components):\n",
    "                # If we already read the last bin of the current component\n",
    "                # and it is not the last component, then we can do one of the two options:\n",
    "                \n",
    "                # Option 1: Get the first bin of the next component\n",
    "                # This will allow us to avoid adding a large bin to the small last bin of the previous component.\n",
    "                # But if the first bin of the next component was really small (can happen only if it was a a one bin component \n",
    "                # or there was a particular reason to break bin, it will still allow to add part of the component to the newly \n",
    "                # forming bin and the rest to the new bin\n",
    "                nextBinColLength = components[compNum+1]['binsToCols'][0]\n",
    "                \n",
    "                # Option 2: Get the whole size of the next component\n",
    "                # This option will not allow spliting a component in any way in such a way that a bin will get pieces of two \n",
    "                # neighbouring components. It will allow adding a very small single bin component to the bin which partially \n",
    "                # used by the end of previous component.\n",
    "                # nextBinColLength = sum(components[compNum+1]['binsToCols'])\n",
    "                \n",
    "            else:\n",
    "                nextBinColLength = 0\n",
    "            if binBlockLength+nextBinColLength > zoomLevel and binBlockLength>0:\n",
    "                \n",
    "                # If bin got equal or larger than target zoom level bin size \n",
    "                # (it can grow over by less than previous zoom level)\n",
    "                # then bin is closing and new bin will be formed.\n",
    "                [binColLengths,binColStarts,binColEnds,binOcc,binInv,binPosArray,nBins,nCols,\n",
    "                    binBlockLength,binBlockLengths,matrix,\n",
    "                    newComponent,newComponents,newComponentLengths,\n",
    "                    newFromComponentLinks,newToComponentLinks,\n",
    "                    occupants,\n",
    "                    starts,ends,newToOldInd,oldToNewInd,\n",
    "                    forwardPaths,invertedPaths,pathsToInversion] = \\\n",
    "                finaliseBinZoom(compNum,binOcc,binInv,binPosArray,\n",
    "                    nBins,nCols,\n",
    "                    binBlockLength,binBlockLengths,binColLengths,\n",
    "                    binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                    matrix,\n",
    "                    newComponent,newComponents,newComponentLengths,\n",
    "                    newFromComponentLinks,newToComponentLinks,\n",
    "                    occupants,linkLengths,\n",
    "                    starts,ends,\n",
    "                    forwardPaths,invertedPaths,pathsToInversion,\n",
    "                    newToOldInd,oldToNewInd,\n",
    "                    inversionThreshold=inversionThreshold)\n",
    "\n",
    "            if nBins==maxLengthComponent:\n",
    "                \n",
    "                if nBins>0:\n",
    "                \n",
    "                    # Add links from current component to the next one\n",
    "                    if len(forwardPaths)>0:\n",
    "                        addLink(len(newComponents)+1,'+',len(newComponents)+2,'+',forwardPaths,newFromComponentLinks,newToComponentLinks)\n",
    "                    if len(invertedPaths)>0:\n",
    "                        addLink(len(newComponents)+2,'-',len(newComponents)+1,'-',invertedPaths,newFromComponentLinks,newToComponentLinks)\n",
    "                    # Add new element to newToOldInd\n",
    "                    \n",
    "                    if binNum==componentLengths[compNum]-1:\n",
    "                        newToOldInd.append([])\n",
    "                    else:\n",
    "                        newToOldInd.append([compNum])\n",
    "                    # Add number of next element to oldToNewInd\n",
    "                    oldToNewInd[-1].append(len(newComponents))\n",
    "                    \n",
    "                    # close component\n",
    "\n",
    "                    newComponent,newComponents,newComponentLengths,\\\n",
    "                    nBins,nCols,occupants,binBlockLengths,binColStarts,binColEnds,\\\n",
    "                    matrix,starts,ends,compInvNum,compInvDen = \\\n",
    "                        finaliseComponentZoom(newComponent,newComponents,newComponentLengths,\n",
    "                                              nBins,nCols,occupants,binBlockLengths,\n",
    "                                              binColStarts,binColEnds,\n",
    "                                              matrix,starts,ends,\n",
    "                                              forwardPaths,invertedPaths,\n",
    "                                              compInvNum,compInvDen,inversionThreshold=inversionThreshold)\n",
    "                    occupants = set(component['occupants'])\n",
    "        \n",
    "        isEndBreak = isStartEnd(compNum,components)\n",
    "        \n",
    "        if isEndBreak and binBlockLength>0:\n",
    "            [binColLengths,binColStarts,binColEnds,binOcc,binInv,binPosArray,nBins,nCols,\n",
    "            binBlockLength,binBlockLengths,matrix,\n",
    "            newComponent,newComponents,newComponentLengths,\n",
    "            newFromComponentLinks,newToComponentLinks,\n",
    "            occupants,\n",
    "            starts,ends,newToOldInd,oldToNewInd,\n",
    "            forwardPaths,invertedPaths,pathsToInversion] = \\\n",
    "                finaliseBinZoom(compNum,binOcc,binInv,binPosArray,\n",
    "                    nBins,nCols,\n",
    "                    binBlockLength,binBlockLengths,binColLengths,\n",
    "                    binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                    matrix,\n",
    "                    newComponent,newComponents,newComponentLengths,\n",
    "                    newFromComponentLinks,newToComponentLinks,\n",
    "                    occupants,linkLengths,\n",
    "                    starts,ends,\n",
    "                    forwardPaths,invertedPaths,pathsToInversion,\n",
    "                    newToOldInd,oldToNewInd,\n",
    "                    inversionThreshold=inversionThreshold)\n",
    "            \n",
    "        if compNum==len(components)-1:\n",
    "            # Close the bin\n",
    "            if binBlockLength>0:\n",
    "                [binColLengths,binColStarts,binColEnds,binOcc,binInv,binPosArray,nBins,nCols,\n",
    "                binBlockLength,binBlockLengths,matrix,\n",
    "                newComponent,newComponents,newComponentLengths,\n",
    "                newFromComponentLinks,newToComponentLinks,\n",
    "                occupants,\n",
    "                starts,ends,newToOldInd,oldToNewInd,\n",
    "                forwardPaths,invertedPaths,pathsToInversion] = \\\n",
    "                    finaliseBinZoom(compNum,binOcc,binInv,binPosArray,\n",
    "                        nBins,nCols,\n",
    "                        binBlockLength,binBlockLengths,binColLengths,\n",
    "                        binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                        matrix,\n",
    "                        newComponent,newComponents,newComponentLengths,\n",
    "                        newFromComponentLinks,newToComponentLinks,\n",
    "                        occupants,linkLengths,\n",
    "                        starts,ends,\n",
    "                        forwardPaths,invertedPaths,pathsToInversion,\n",
    "                        newToOldInd,oldToNewInd,\n",
    "                        inversionThreshold=inversionThreshold)\n",
    "            \n",
    "            # break the last component\n",
    "            if nBins>0:\n",
    "                \n",
    "                oldToNewInd[-1].append(len(newComponents))\n",
    "                \n",
    "                newComponent,newComponents,newComponentLengths,\\\n",
    "                nBins,nCols,occupants,binBlockLengths,binColStarts,binColEnds,\\\n",
    "                matrix,starts,ends,compInvNum,compInvDen = \\\n",
    "                    finaliseComponentZoom(newComponent,newComponents,newComponentLengths,\n",
    "                                          nBins,nCols,occupants,binBlockLengths,\n",
    "                                          binColStarts,binColEnds,\n",
    "                                          matrix,starts,ends,\n",
    "                                          range(len(graph.accessions)),range(len(graph.accessions)),\n",
    "                                          compInvNum,compInvDen,inversionThreshold=inversionThreshold)\n",
    "\n",
    "        elif isEndBreak or any(checkForBreaksZoom(zoomLevel,compNum,components,fromComponentLinks,toComponentLinks,blockEdges)):\n",
    "            # Break at the end of the component because of the links\n",
    "            if binBlockLength>0:\n",
    "                [binColLengths,binColStarts,binColEnds,binOcc,binInv,binPosArray,nBins,nCols,\n",
    "                binBlockLength,binBlockLengths,matrix,\n",
    "                newComponent,newComponents,newComponentLengths,\n",
    "                newFromComponentLinks,newToComponentLinks,\n",
    "                occupants,\n",
    "                starts,ends,newToOldInd,oldToNewInd,\n",
    "                forwardPaths,invertedPaths,pathsToInversion] = \\\n",
    "                finaliseBinZoom(compNum,binOcc,binInv,binPosArray,\n",
    "                    nBins,nCols,\n",
    "                    binBlockLength,binBlockLengths,binColLengths,\n",
    "                    binColStart,binColStarts,binColEnd,binColEnds,\n",
    "                    matrix,\n",
    "                    newComponent,newComponents,newComponentLengths,\n",
    "                    newFromComponentLinks,newToComponentLinks,\n",
    "                    occupants,linkLengths,\n",
    "                    starts,ends,\n",
    "                    forwardPaths,invertedPaths,pathsToInversion,\n",
    "                    newToOldInd,oldToNewInd,\n",
    "                    inversionThreshold=inversionThreshold)\n",
    "            \n",
    "            # Break component\n",
    "            if nBins>0:\n",
    "                \n",
    "                newToOldInd.append([])\n",
    "                oldToNewInd[-1].append(len(newComponents))\n",
    "                \n",
    "                newComponent,newComponents,newComponentLengths,\\\n",
    "                nBins,nCols,occupants,binBlockLengths,binColStarts,binColEnds,\\\n",
    "                matrix,starts,ends,compInvNum,compInvDen = \\\n",
    "                    finaliseComponentZoom(newComponent,newComponents,newComponentLengths,\n",
    "                                          nBins,nCols,occupants,binBlockLengths,\n",
    "                                          binColStarts,binColEnds,\n",
    "                                          matrix,starts,ends,\n",
    "                                          range(len(graph.accessions)),range(len(graph.accessions)),\n",
    "                                          compInvNum,compInvDen,inversionThreshold=inversionThreshold)\n",
    "                \n",
    "        elif nBins>0 or binBlockLength>0:\n",
    "            oldToNewInd[-1].append(len(newComponents))\n",
    "    \n",
    "    print('\\nProcessing component finished.')\n",
    "    \n",
    "    newFromComponentLinks,newToComponentLinks,accStarts,accEnds,newLinkLengths,newPairedLinks,newInterconnectedLinks,newBlockEdges = \\\n",
    "        updateLinks(newToOldInd,oldToNewInd,fromComponentLinks,toComponentLinks,\n",
    "                    linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                    accStarts,accEnds,\n",
    "                    newComponents,compAccDir,newFromComponentLinks,newToComponentLinks)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return newComponents,newComponentLengths,\\\n",
    "            newFromComponentLinks,newToComponentLinks,\\\n",
    "            accStarts,accEnds,newLinkLengths,newPairedLinks,newInterconnectedLinks,newBlockEdges\\\n",
    "            ,oldToNewInd,newToOldInd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0f26a",
   "metadata": {},
   "source": [
    "## Clear elements too small to show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3452d8-ec1d-4725-b7bb-32c102042674",
   "metadata": {},
   "source": [
    "This set of functions (with the orchestrating function being `clearInvisible`) look at earlier identified non-linear link to size (or number of nucleotides) \n",
    "associations and if the next zoom level is larger than some sizes, then these links are removed (with reinstating of some of linear links instead).\n",
    "\n",
    "After that Isolation blocks are identified and removed. Isolation block is a contiguous block of components (columns) that are connected only to each other but not to any of components outside the block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13b68c",
   "metadata": {},
   "source": [
    "### Removing links and rearrangement blocks associated to too small blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72578f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def removeLink(fromComponentLinks,toComponentLinks,linkList,remLinks,link,pairedLink=None,subLink=None,subLinks=None,remLinkAccessions=None):\n",
    "    '''\n",
    "    This function remove the main link.\n",
    "    \n",
    "    If paired and substitute links are provided, the paired link will be checked (if it is not removed or \n",
    "    in the queue to be removed), it will be added to the queue\n",
    "    \n",
    "    After that common accessions for the same strand (for each separately) for start of main link and and end of paired link\n",
    "    are found and substitute link is established for all such accessions.\n",
    "    \n",
    "    If the substitute link is not (k,k+1), but (k,k+p), then in componentLinks all links (k,k+1),(k+1,k+1),...,(k+p-1,k+p)\n",
    "    are established.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    accessionMainLink = {}\n",
    "\n",
    "    if pairedLink is not None and subLink is not None and subLinks is not None and remLinkAccessions is not None:\n",
    "        processPair = True\n",
    "    else:\n",
    "        processPair = False\n",
    "        \n",
    "    if link not in remLinks: # Check validity.\n",
    "        # removing links from fromComponentLinks\n",
    "        for fromStrand,fromStrandDict in list(fromComponentLinks.get(link[0],{}).items()):\n",
    "\n",
    "            try:\n",
    "                if processPair:\n",
    "                    toCompDict = fromComponentLinks[link[0]][fromStrand].pop(link[1])\n",
    "                    for toStrand,accessionList in list(toCompDict.items()):\n",
    "                        if toStrand==fromStrand and link[0]+(1 if toStrand=='+' else -1)==link[1]:\n",
    "                            fromComponentLinks[link[0]][fromStrand].setdefault(link[1],{}).setdefault(toStrand,set()).update(accessionList)\n",
    "                        accessionMainLink.setdefault(fromStrand,set()).update(accessionList)\n",
    "                else:\n",
    "                    allCleared = True\n",
    "                    for toStrand,accessionList in list(fromStrandDict.get(link[1],{}).items()):\n",
    "                        if toStrand!=fromStrand or link[0]+(1 if toStrand=='+' else -1)!=link[1]:\n",
    "                            del fromComponentLinks[link[0]][fromStrand][link[1]][toStrand]\n",
    "                        else:\n",
    "                            allCleared = False\n",
    "                    \n",
    "                    if allCleared:\n",
    "                        del fromComponentLinks[link[0]][fromStrand][link[1]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "\n",
    "            if len(fromComponentLinks[link[0]][fromStrand])==0:\n",
    "                del fromComponentLinks[link[0]][fromStrand]\n",
    "\n",
    "        if link[0] in fromComponentLinks:\n",
    "            if len(fromComponentLinks[link[0]])==0:\n",
    "                del fromComponentLinks[link[0]]\n",
    "\n",
    "        # removing links from toComponentLinks\n",
    "        for toStrand,toStrandDict in list(toComponentLinks.get(link[1],{}).items()):\n",
    "\n",
    "            try:\n",
    "                allCleared = True\n",
    "                for fromStrand,accessionList in list(toStrandDict.get(link[0],{}).items()):\n",
    "                    if toStrand!=fromStrand or link[0]+(1 if toStrand=='+' else -1)!=link[1]:\n",
    "                        del toComponentLinks[link[1]][toStrand][link[0]][fromStrand]\n",
    "                    else:\n",
    "                        allCleared = False\n",
    "                \n",
    "                if allCleared:\n",
    "                    del toComponentLinks[link[1]][toStrand][link[0]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            if len(toComponentLinks[link[1]][toStrand])==0:\n",
    "                del toComponentLinks[link[1]][toStrand]\n",
    "\n",
    "        if link[1] in toComponentLinks:\n",
    "            if len(toComponentLinks[link[1]])==0:\n",
    "                del toComponentLinks[link[1]]\n",
    "\n",
    "        remLinks.add(link)\n",
    "    \n",
    "    if processPair:\n",
    "        if len(accessionMainLink)>0:\n",
    "            remLinkAccessions[link]==accessionMainLink\n",
    "        else:\n",
    "            accessionMainLink = remLinkAccessions.get(link,{})\n",
    "            \n",
    "    if processPair and len(accessionMainLink)>0 and subLink not in subLinks:\n",
    "        if pairedLink not in linkList and pairedLink not in remLinks:\n",
    "            # If for some reason paired link is not in the list to be removed or removed already, \n",
    "            # it should also be removed. \n",
    "            # BE CAREFUL \n",
    "            # Although, if there is an issue with moving from layer to layer this can mask the issue.\n",
    "            \n",
    "            # The paired link is not removed here, it is added to the linkList and will be processed separately.\n",
    "            linkList.add(pairedLink)\n",
    "        \n",
    "        # Here we need to get accession list for each toStrand for paired link (for the end of link)\n",
    "        accessionPaired = {}\n",
    "        for toStrand,toStrandDict in list(toComponentLinks.get(link[1],{}).items()):\n",
    "\n",
    "            try:\n",
    "                fromCompDict = toComponentLinks[link[1]][toStrand].pop(link[0])\n",
    "                for fromStrand,accessionList in fromCompDict:\n",
    "                    accessionPaired.setdefault(toStrand,set()).update(accessionList)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        subLinks.add(subLink)\n",
    "        \n",
    "        for strand,strandAccMain in accessionMainLink.items():\n",
    "            # Then intersect it with accessionsMainLink\n",
    "            strandAccPaired = accessionPaired.get(strand,set())\n",
    "            accIntersect = strandAccMain & strandAccPaired\n",
    "\n",
    "            # Establish substitute links \n",
    "            step = 1 if strand=='+' else -1\n",
    "            \n",
    "            for fromComp in range(subLink[0],subLink[1],step):\n",
    "                toComp = fromComp + step\n",
    "                if (strand=='+' and fromComp>toComp) or (strand=='-' and fromComp<toComp):\n",
    "                    _fromComp = toComp\n",
    "                    _toComp = fromComp\n",
    "                else:\n",
    "                    _fromComp = fromComp\n",
    "                    _toComp = toComp\n",
    "                    \n",
    "                addLink(_fromComp,strand,_toComp,strand,accIntersect,fromComponentLinks,toComponentLinks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8376aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def processCollapsibleBlocks(zoomLevel,linkLengths,pairedLinks,interconnectedLinks,fromComponentLinks,toComponentLinks):\n",
    "    \n",
    "    # Need to remove the links below zoom level everywhere, but the substitute links,\n",
    "    # associated with the pair should be established ONLY for paths where both links from the pair present!\n",
    "    remLinkAccessions = {}\n",
    "    for blockLength in sorted(list(linkLengths.keys())):\n",
    "        if blockLength<zoomLevel:\n",
    "            linkList = linkLengths.pop(blockLength)\n",
    "            remLinks = set()\n",
    "            subLinks = set()\n",
    "            \n",
    "            while len(linkList)>0:\n",
    "                link = linkList.pop()\n",
    "                try:\n",
    "                    pairedDict = pairedLinks.pop(link)\n",
    "                \n",
    "                    for pairedLink,subLink in pairedDict.items():\n",
    "                        removeLink(fromComponentLinks,toComponentLinks,linkList,remLinks,link,pairedLink,subLink,subLinks,remLinkAccessions)\n",
    "                except KeyError:\n",
    "                    removeLink(fromComponentLinks,toComponentLinks,linkList,remLinks,link)\n",
    "                \n",
    "                try:\n",
    "                    interconnectedLinks.pop(link)\n",
    "                except KeyError:\n",
    "                    warnings.warn(f'Link {link} was not found in interconnectedLinks. Possibly it was removed earlier, but it is better to double check.')\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df365be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clearRearrangementBlocks(zoomLevel,blockEdges):\n",
    "    return {edge:size for edge,size in blockEdges.items() if size>=zoomLevel}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eee058",
   "metadata": {},
   "source": [
    "### Find isolated blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3278f",
   "metadata": {},
   "source": [
    "#### Identify empty edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ff9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def testStartEnd(compNum,isLeft,components,accStarts,accEnds):\n",
    "    if isLeft:\n",
    "        startInv = 0\n",
    "        endInv = 1\n",
    "    else:\n",
    "        startInv = 1\n",
    "        endInv = 0\n",
    "        \n",
    "    for accStart,startComp in accStarts.items():\n",
    "        if startComp == compNum:\n",
    "            comp = components[compNum-1]\n",
    "            try:\n",
    "                accessionInd = comp['occupants'].index(accStart)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if comp['matrix'][accessionInd][1]==startInv:\n",
    "                return True\n",
    "    \n",
    "    for accEnd,endComp in accEnds.items():\n",
    "        if endComp == compNum:\n",
    "            comp = components[compNum-1]\n",
    "            try:\n",
    "                accessionInd = comp['occupants'].index(accStart)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if comp['matrix'][accessionInd][1]==endInv:\n",
    "                return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def findEmptyEdges(fromComponentLinks,toComponentLinks,accStarts,accEnds,components):\n",
    "    '''\n",
    "    Identify all empty edges by simply finding components that do not appear either in toComponentLinks (left empty)\n",
    "    or fromComponentLinks (right empty)\n",
    "    '''\n",
    "    \n",
    "    allCompSet = set((np.array(range(len(components)))+1).tolist())\n",
    "    toLinkedSet = set(toComponentLinks.keys())\n",
    "    fromLinkedSet = set(fromComponentLinks.keys())\n",
    "\n",
    "    leftEmptyCandidates = list(allCompSet - toLinkedSet)\n",
    "\n",
    "    leftEmptyList = []\n",
    "\n",
    "    for leftEmptyCand in leftEmptyCandidates:\n",
    "        if testStartEnd(leftEmptyCand,True,components,accStarts,accEnds):\n",
    "            continue\n",
    "\n",
    "        if '-' in fromComponentLinks.get(leftEmptyCand,{}):\n",
    "            continue\n",
    "\n",
    "        leftEmptyList.append(leftEmptyCand)\n",
    "\n",
    "    rightEmptyCandidates = list(allCompSet - fromLinkedSet)\n",
    "\n",
    "    rightEmptyList = []\n",
    "\n",
    "    for rightEmptyCand in rightEmptyCandidates:\n",
    "        if testStartEnd(rightEmptyCand,False,components,accStarts,accEnds):\n",
    "            continue\n",
    "\n",
    "        if '-' in toComponentLinks.get(rightEmptyCand,{}):\n",
    "            continue\n",
    "\n",
    "        rightEmptyList.append(rightEmptyCand)\n",
    "        \n",
    "    return leftEmptyList,rightEmptyList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f46c8",
   "metadata": {},
   "source": [
    "#### Identify isolated blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41916d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkExternalLinks(blockStart,blockEnd,fromComponentLinks,toComponentLinks,components):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    \n",
    "    componentID: int. If an external links found, then a list of components inside the block involved \n",
    "                    in external links returned. If no external links found, then empty list is returned.\n",
    "    '''\n",
    "    \n",
    "    externalLinksComponents = []\n",
    "    \n",
    "    for compID in range(blockStart,blockEnd+1):\n",
    "        passToNext = False\n",
    "        \n",
    "        if len(components[compID-1].get('ends',[]))>0 or len(components[compID-1].get('starts',[]))>0:\n",
    "            externalLinksComponents.append(compID)\n",
    "            continue\n",
    "        \n",
    "        for fromStrandDict in fromComponentLinks.get(compID,{}).values():\n",
    "            for toComp in fromStrandDict.keys():\n",
    "                if toComp>blockEnd or toComp<blockStart:\n",
    "                    externalLinksComponents.append(compID)\n",
    "                    passToNext = True\n",
    "                    break\n",
    "            if passToNext:\n",
    "                break\n",
    "        \n",
    "        if passToNext:\n",
    "            continue\n",
    "            \n",
    "        for toStrandDict in toComponentLinks.get(compID,{}).values():\n",
    "            for fromComp in toStrandDict.keys():\n",
    "                if fromComp>blockEnd or fromComp<blockStart:\n",
    "                    externalLinksComponents.append(compID)\n",
    "                    passToNext = True\n",
    "                    break\n",
    "            if passToNext:\n",
    "                break\n",
    "    \n",
    "    return externalLinksComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def createNewBoundaries(blockStart,blockEnd,externalLinksComps,leftEmptyList,rightEmptyList):\n",
    "    leftEmptyArray = np.sort(np.array(leftEmptyList))\n",
    "    rightEmptyArray = np.sort(np.array(rightEmptyList))\n",
    "    \n",
    "    if externalLinksComps[0]==blockStart:\n",
    "        suitableStarts = np.where((leftEmptyArray>blockStart)&(leftEmptyArray<=blockEnd))[0]\n",
    "        if len(suitableStarts)>0:\n",
    "            blockSplit = [[leftEmptyArray[suitableStarts[0]]]]\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        blockSplit = [[blockStart]]\n",
    "    \n",
    "    doEnd = True\n",
    "    \n",
    "    for breakNum,breakPoint in enumerate(externalLinksComps):\n",
    "        \n",
    "        # Adding end\n",
    "        if doEnd:\n",
    "            suitableEnds = np.where((rightEmptyArray>=blockSplit[-1][0]) & (rightEmptyArray<breakPoint))[0]\n",
    "            if len(suitableEnds)>0:\n",
    "                blockSplit[-1].append(rightEmptyArray[suitableEnds[-1]])\n",
    "            else:\n",
    "                del blockSplit[-1]\n",
    "        \n",
    "        #adding start\n",
    "        if breakNum+1<len(externalLinksComps):\n",
    "            nextBoundary = externalLinksComps[breakNum+1]\n",
    "        else:\n",
    "            nextBoundary = blockEnd+1\n",
    "            \n",
    "        if breakPoint<blockEnd:\n",
    "            suitableStarts = np.where((leftEmptyArray>breakPoint) & (leftEmptyArray<=nextBoundary))[0]\n",
    "            if len(suitableStarts)>0:\n",
    "                blockSplit.append([leftEmptyArray[suitableStarts[0]]])\n",
    "                doEnd = True\n",
    "            else:\n",
    "                doEnd = False\n",
    "        else:\n",
    "            doEnd = False\n",
    "            break\n",
    "    \n",
    "    if doEnd:\n",
    "        suitableEnds = np.where((rightEmptyArray>=blockSplit[-1][0]) & (rightEmptyArray<=blockEnd))[0]\n",
    "        if len(suitableEnds)>0:\n",
    "            blockSplit[-1].append(rightEmptyArray[suitableEnds[-1]])\n",
    "        else:\n",
    "            del blockSplit[-1]\n",
    "    \n",
    "    if len(blockSplit)>0:\n",
    "        if len(blockSplit[0])>0:\n",
    "            return blockSplit\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for `createNewBoundaries`\n",
    "import numpy as np\n",
    "\n",
    "st = [2,5,6,8]\n",
    "end = [2,3,4,6,8,9,10,11]\n",
    "\n",
    "blocks = [[2,11],[2,3],[5,11],[8,11],[8,9],[8,11]]\n",
    "blockSplits = [[[2,3],[5,11]],[[2,2]],[[6,6],[8,11]],[[8,9]],[[8,8]],[]]\n",
    "externals = [[4],[3],[5,7],[10],[9],[8,9,10,11]]\n",
    "\n",
    "for bl,blSpl,ext in zip(blocks,blockSplits,externals):\n",
    "    blSplTT = createNewBoundaries(*bl,ext,st,end)\n",
    "    assert blSpl == blSplTT,f'Expected {blSpl}, but got {blSplTT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84764d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another test for `createNewBoundaries`\n",
    "leftEmptyList = [2056, 3080, 3081, 2092, 2099, 1593, 3643, 2627, 1116, 2653, 2655, 3168, 2658, 613, 1637, 1638, 106, 1654, 2695, 2192, 1169, 1686, 2714, 3757, 2233, 3781, 723, 1240, 224, 1761, 1762, 1766, 3323, 1804, 786, 2331, 802, 2850, 807, 811, 1839, 1841, 3396, 3397, 1863, 3400, 843, 3423, 1898, 1899, 882, 884, 3463, 402, 2451, 3478, 408, 3482, 934, 426, 1962, 3504, 3516, 3519, 3520, 451, 1994, 1995, 972, 2506, 463, 3024, 1493, 1494, 3542, 1525]\n",
    "rightEmptyList = [402, 2451, 3478, 407, 280, 3482, 2848, 802, 934, 807, 426, 811, 2091, 2092, 3757, 1839, 3504, 1841, 3516, 3519, 3405, 463, 722, 1240, 1761, 1762, 1766, 1899]\n",
    "blockStart = 3396\n",
    "blockEnd = 3405\n",
    "externalLinksComps = [3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405]\n",
    "\n",
    "createNewBoundaries(blockStart,blockEnd,externalLinksComps,leftEmptyList,rightEmptyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identifyIsolatedBlocks(leftEmptyList,rightEmptyList,fromComponentLinks,toComponentLinks,components):\n",
    "    if len(leftEmptyList)==0 or len(rightEmptyList)==0:\n",
    "        return []\n",
    "    \n",
    "    leftmostLeftEmpty = min(leftEmptyList)\n",
    "    rightmostRightEmpty = max(rightEmptyList)\n",
    "\n",
    "    isolatedBlockCandidates = []\n",
    "    isolatedBlockList = []\n",
    "\n",
    "    if leftmostLeftEmpty<=rightmostRightEmpty:\n",
    "        isolatedBlockCandidates.append([leftmostLeftEmpty,rightmostRightEmpty])\n",
    "\n",
    "    while len(isolatedBlockCandidates)>0:\n",
    "\n",
    "        isolatedBlock = isolatedBlockCandidates.pop()\n",
    "\n",
    "        externalLinksComps = checkExternalLinks(*isolatedBlock,fromComponentLinks,toComponentLinks,components)\n",
    "\n",
    "        if len(externalLinksComps)==0:\n",
    "            isolatedBlockList.append(isolatedBlock)\n",
    "        else:\n",
    "            isolatedBlockCandidates.extend(createNewBoundaries(*isolatedBlock,externalLinksComps,leftEmptyList,rightEmptyList))\n",
    "    \n",
    "    return isolatedBlockList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac3fdb",
   "metadata": {},
   "source": [
    "### Removing Isolated Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def updateLinksRemoveComp(oldToNewInd,fromComponentLinks,toComponentLinks,\n",
    "                          linkLengths,pairedLinks,interconnectedLinks,blockEdges,accStarts,accEnds):\n",
    "    \n",
    "    newFromComponentLinks={}\n",
    "    newToComponentLinks={}\n",
    "    \n",
    "    for oldFromComp,fromCompDict in fromComponentLinks.items():\n",
    "        if oldFromComp-1 in oldToNewInd:\n",
    "            newFromCompDict = newFromComponentLinks.setdefault(oldToNewInd[oldFromComp-1][0]+1,{})\n",
    "            for fromStrand,fromStrandDict in fromCompDict.items():\n",
    "                newFromStrandDict = newFromCompDict.setdefault(fromStrand,{})\n",
    "                for toComp,toCompDict in fromStrandDict.items():\n",
    "                    if toComp-1 in oldToNewInd:\n",
    "                        newFromStrandDict[oldToNewInd[toComp-1][0]+1] = toCompDict\n",
    "                \n",
    "    for oldToComp,toCompDict in toComponentLinks.items():\n",
    "        if oldToComp-1 in oldToNewInd:\n",
    "            newToCompDict = newToComponentLinks.setdefault(oldToNewInd[oldToComp-1][0]+1,{})\n",
    "            for toStrand,toStrandDict in toCompDict.items():\n",
    "                newToStrandDict = newToCompDict.setdefault(toStrand,{})\n",
    "                for fromComp,fromCompDict in toStrandDict.items():\n",
    "                    if fromComp-1 in oldToNewInd:\n",
    "                        newToStrandDict[oldToNewInd[fromComp-1][0]+1] = fromCompDict\n",
    "    \n",
    "    for acc in accStarts.keys():\n",
    "        accStarts[acc] = oldToNewInd[accStarts[acc]-1][0]+1\n",
    "        accEnds[acc] = oldToNewInd[accEnds[acc]-1][0]+1\n",
    "    \n",
    "    newLinkLengths = {}\n",
    "    for blockLength,linkList in linkLengths.items():\n",
    "        for link in linkList:\n",
    "            if (link[0]-1 not in oldToNewInd and link[1]-1 not in oldToNewInd):\n",
    "                continue\n",
    "            elif (link[0]-1 not in oldToNewInd or link[1]-1 not in oldToNewInd):\n",
    "                raise RuntimeError(f'One side of link {link} is found in the isolated block, and another one not. \\\n",
    "                It is against definition of isolated block.')\n",
    "            \n",
    "            newLinkFrom = oldToNewInd[link[0]-1][0]+1\n",
    "            newLinkTo = oldToNewInd[link[1]-1][0]+1\n",
    "            newLinkLengths.setdefault(blockLength,set()).add((newLinkFrom,newLinkTo))\n",
    "    \n",
    "    newPairedLinks = {}\n",
    "    for mainLink,pairedDict in pairedLinks.items():\n",
    "        for pairedLink,subLink in pairedDict.items():\n",
    "            isInIsolatedBlock = [side-1 not in oldToNewInd for side in itertools.chain(mainLink,pairedLink,subLink)]\n",
    "            if all(isInIsolatedBlock):\n",
    "                continue\n",
    "            elif any(isInIsolatedBlock):\n",
    "                raise RuntimeError(f'All sides of paired links {mainLink} - {pairedLink} : {subLink} should be \\\n",
    "                either in or out of isolated block but at least one side is inside and at least one outside.')\n",
    "            \n",
    "            newMainLink = (oldToNewInd[mainLink[0]-1][0]+1,oldToNewInd[mainLink[1]-1][0]+1)\n",
    "            newPairedLink = (oldToNewInd[pairedLink[0]-1][0]+1,oldToNewInd[pairedLink[1]-1][0]+1)\n",
    "            newSubLink = (oldToNewInd[subLink[0]-1][0]+1,oldToNewInd[subLink[1]-1][0]+1)\n",
    "            newPairedLinks.setdefault(newMainLink,{})[newPairedLink] = newSubLink\n",
    "    \n",
    "    newInterconnectedLinks = {}\n",
    "    for linkC,linkList in interconnectedLinks.items():\n",
    "        if (linkC[0]-1 not in oldToNewInd and linkC[1]-1 not in oldToNewInd):\n",
    "            continue\n",
    "        elif (linkC[0]-1 not in oldToNewInd or linkC[1]-1 not in oldToNewInd):\n",
    "            raise RuntimeError(f'One side of link {link} (key of interconnectedLinks) is found in the isolated block, and another one not. \\\n",
    "                It is against definition of isolated block.')\n",
    "        newLinkC = (oldToNewInd[linkC[0]-1][0]+1,oldToNewInd[linkC[1]-1][0]+1)\n",
    "        for link in linkList:\n",
    "            if (link[0]-1 not in oldToNewInd and link[1]-1 not in oldToNewInd):\n",
    "                continue\n",
    "            elif (link[0]-1 not in oldToNewInd or link[1]-1 not in oldToNewInd):\n",
    "                raise RuntimeError(f'One side of link {link} (link from values of interconnectedLinks for key {linkC}) is found in the isolated block, and another one not. \\\n",
    "                It is against definition of isolated block.')\n",
    "            newInterconnectedLinks.setdefault(newLinkC,set()).add((oldToNewInd[link[0]-1][0]+1,oldToNewInd[link[1]-1][0]+1))\n",
    "        if len(newInterconnectedLinks[newLinkC])==0:\n",
    "            # Is it correct procedure?\n",
    "            newInterconnectedLinks.pop(newLinkC)\n",
    "        \n",
    "    # Update blockEdges\n",
    "    newBlockEdges = {}\n",
    "    for comp,blockLength in blockEdges.items():\n",
    "        if comp-1 in oldToNewInd:\n",
    "            newBlockEdges[oldToNewInd[comp-1][0]+1] = blockLength\n",
    "    \n",
    "    return newFromComponentLinks,newToComponentLinks,accStarts,accEnds,newLinkLengths,newPairedLinks,newInterconnectedLinks,newBlockEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def removeIsolatedBlocks(isolatedBlockList,components,componentLengths,\n",
    "                         fromComponentLinks,toComponentLinks,\n",
    "                         accStarts,accEnds,linkLengths,pairedLinks,interconnectedLinks,blockEdges):\n",
    "    if len(isolatedBlockList)>0:\n",
    "    \n",
    "        newToOldInd = list(range(len(components)))\n",
    "\n",
    "        for isolatedBlockStart,isolatedBlockEnd in sorted(isolatedBlockList,key=lambda el: el[1],reverse=True):\n",
    "            # deleting components and creating translation from new index to old index\n",
    "            for compToRemove in range(isolatedBlockEnd,isolatedBlockStart-1,-1):\n",
    "                del components[compToRemove-1]\n",
    "                del componentLengths[compToRemove-1]\n",
    "                del newToOldInd[compToRemove-1]\n",
    "\n",
    "        # Converting new to old index to old to new index\n",
    "        oldToNewInd = {el:[ind] for ind,el in enumerate(newToOldInd)}\n",
    "        # Converting new to old index from list to dict (not sure it is needed in the first place)\n",
    "        # newToOldInd = {ind:[el] for ind,el in enumerate(newToOldInd)}\n",
    "\n",
    "        # `oldToNewInd` as well as `newToOldInd` are 0-based both index and values. To connect them to links (which are 1-based), \n",
    "        # for indexing you need to do linkComp-1, to get back to links, +1, `oldToNewInd[linkComp-1][0]+1`\n",
    "\n",
    "        # updating links\n",
    "        fromComponentLinks,toComponentLinks,accStarts,accEnds,linkLengths,pairedLinks,interconnectedLinks,blockEdges = \\\n",
    "            updateLinksRemoveComp(oldToNewInd,fromComponentLinks,toComponentLinks,\n",
    "                                  linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                                  accStarts,accEnds)\n",
    "    else:\n",
    "        oldToNewInd = {}\n",
    "        newToOldInd = []\n",
    "    return components,componentLengths,fromComponentLinks,toComponentLinks,accStarts,accEnds,\\\n",
    "            linkLengths,pairedLinks,interconnectedLinks,blockEdges,\\\n",
    "            oldToNewInd,newToOldInd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a81a7",
   "metadata": {},
   "source": [
    "### Clearing small element wrapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a258eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clearInvisible(zoomLevel,linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                   fromComponentLinks,toComponentLinks,\n",
    "                   accStarts,accEnds,\n",
    "                   components,componentLengths):\n",
    "    print('Removing links according to collapsible blocks')\n",
    "    \n",
    "    # TODO This process should be changed to linkLengths dict using paired links\n",
    "    numOfComponents = len(components)\n",
    "    # Link removal\n",
    "    processCollapsibleBlocks(zoomLevel,linkLengths,pairedLinks,interconnectedLinks,fromComponentLinks,toComponentLinks)\n",
    "\n",
    "    blockEdges = clearRearrangementBlocks(zoomLevel,blockEdges)\n",
    "    \n",
    "    # Identify empty (without links) edges\n",
    "    leftEmptyList,rightEmptyList = findEmptyEdges(fromComponentLinks,toComponentLinks,accStarts,accEnds,components)\n",
    "\n",
    "    # Identify isolated blocks ,i.e. without any links on both sides and no outside links from inside\n",
    "    isolatedBlockList = identifyIsolatedBlocks(leftEmptyList,rightEmptyList,fromComponentLinks,toComponentLinks,components)\n",
    "    \n",
    "    # Remove identified isolated blocks\n",
    "    components,componentLengths,fromComponentLinks,toComponentLinks,accStarts,accEnds,\\\n",
    "    linkLengths,pairedLinks,interconnectedLinks,blockEdges,\\\n",
    "    oldToNewInd,newToOldInd = \\\n",
    "    removeIsolatedBlocks(isolatedBlockList,components,componentLengths,\n",
    "                         fromComponentLinks,toComponentLinks,\n",
    "                         accStarts,accEnds,\n",
    "                         linkLengths,pairedLinks,interconnectedLinks,blockEdges)\n",
    "    \n",
    "    print(f'All links associated with collapsibleComponents <{zoomLevel} were removed. \\\n",
    "    {numOfComponents-len(components)} components were deleted as isolated.')\n",
    "    \n",
    "    return components,componentLengths,fromComponentLinks,toComponentLinks,accStarts,accEnds,\\\n",
    "            linkLengths,pairedLinks,interconnectedLinks,blockEdges,\\\n",
    "            oldToNewInd,newToOldInd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b164ee",
   "metadata": {},
   "source": [
    "## Exporting layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2655ce-7d76-4e9c-bb29-dc55696f729d",
   "metadata": {},
   "source": [
    "These functions, with the main one being `exportLayer`, are exporting prepared zoom level (cleaned and collapsed by other functions) into\n",
    "Pantograph Visualisation tool data structures (JSON chunk files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def createZoomLevelDir(outputPath,outputName,zoomLevel):\n",
    "    '''\n",
    "    Creates a directory for zoom level chunks. The function will take care of correct directory level separator.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    `outputPath`: str. Absolute path to location where the visualisation data should be created\n",
    "    `outputName`: str. The name of the visualisation case, will be a directory at the path given by `outputPath`\n",
    "    `zoomLevel`: int or str. zoom level (maximum number of nucleotides in a single column/bin).\n",
    "                 A directory will be ceated\n",
    "                 \n",
    "    Return\n",
    "    ======\n",
    "    The function does not return anything, but creates a directory tree `outputPath`, \n",
    "    within it creates a directory `outputName` and inside it creates a directory `zoomLevel`\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(f'{outputPath}{os.path.sep}{outputName}{os.path.sep}{zoomLevel}',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finaliseChunk(rootStruct,zoomLevel,chunk,nucleotides,nBins,chunkNum,curCompCols,prevTotalCols,outputPath,outputName):\n",
    "    endChunkBin = chunk['components'][-1]['last_bin']\n",
    "    endChunkCol = chunk['components'][-1]['lastCol']\n",
    "    chunk['last_bin'] = endChunkBin\n",
    "    chunk['last_col'] = endChunkCol\n",
    "    \n",
    "    localPath = f'{outputPath}{os.path.sep}{outputName}{os.path.sep}{zoomLevel}{os.path.sep}'\n",
    "    \n",
    "    fileName = f'chunk{chunkNum}_zoom{zoomLevel}.schematic.json'\n",
    "    \n",
    "    with open(f'{localPath}{fileName}','w') as f:\n",
    "        json.dump(chunk,f,cls=NpEncoder)\n",
    "        \n",
    "    rootStruct['zoom_levels'][zoomLevel]['files'].append({\n",
    "        'file': fileName,\n",
    "        'first_bin':chunk['first_bin'],\n",
    "        'first_col':chunk['first_col'],\n",
    "        'last_bin':chunk['last_bin'],\n",
    "        'last_col':chunk['last_col'],\n",
    "        'compVisCol': chunk['compVisCol'],\n",
    "        'chunkVisCol': sum(chunk['compVisCol'].values()), # Is it necessary?\n",
    "    })\n",
    "    \n",
    "    if nucleotides!='':\n",
    "        fastaName = f'seq_chunk{chunkNum}_zoom{zoomLevel}.fa'\n",
    "        rootStruct['zoom_levels'][zoomLevel]['files'][-1]['fasta'] = fastaName\n",
    "        \n",
    "        with open(f'{localPath}{fastaName}','w') as f:\n",
    "            f.write(f'>first_bin:{chunk[\"first_bin\"]} last_bin:{chunk[\"last_bin\"]}\\n')\n",
    "            f.write(nucleotides)\n",
    "        \n",
    "    chunk = deepcopy(chunkTemplate)\n",
    "    chunk['first_bin'] = endChunkBin + 1\n",
    "    chunk['first_col'] = endChunkCol + 1\n",
    "    return rootStruct,chunk,0,chunkNum + 1,prevTotalCols+curCompCols,0,'' #rootStruct,chunk,nBins,chunkNum,prevTotalCols,curCompCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def addLinksToComp(compNum,components,fromComponentLinks,toComponentLinks):\n",
    "    component = deepcopy(components[compNum])\n",
    "    numLinks = 0\n",
    "    \n",
    "    toCompDict = toComponentLinks.get(compNum+1,{}) # !!! Check that in fromComponentLinks component numbering is 1-based\n",
    "    if '+' in toCompDict:\n",
    "        for fromCompNum,fromCompDict in toCompDict['+'].items():\n",
    "            fromComp = components[fromCompNum-1]\n",
    "            for fromStrand,participants in fromCompDict.items():\n",
    "                doAddArrival = True\n",
    "                if fromStrand=='+':\n",
    "                    upstreamBin = fromComp['last_bin']\n",
    "                    upstreamCol = fromComp['lastCol']\n",
    "                    fromRight = True\n",
    "                    if component['first_bin']-upstreamBin==1:\n",
    "                        doAddArrival = False\n",
    "                elif fromStrand=='-':\n",
    "                    upstreamBin = fromComp['first_bin']\n",
    "                    upstreamCol = fromComp['firstCol']\n",
    "                    fromRight = False\n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised direction of node link {fromStrand}')\n",
    "                if doAddArrival:\n",
    "                    if upstreamBin + 1 != component['first_bin']:\n",
    "                        numLinks += 1\n",
    "                    component['larrivals'].append({\n",
    "                            'upstream': upstreamBin,\n",
    "                            'downstream': component['first_bin'],\n",
    "                            'upstreamCol': upstreamCol,\n",
    "                            'downstreamCol': component['firstCol'],\n",
    "                            'otherSideRight': fromRight,\n",
    "                            'participants': participants\n",
    "                        })\n",
    "    \n",
    "    if '-' in toCompDict:\n",
    "        for fromCompNum,fromCompDict in toCompDict['-'].items():\n",
    "            fromComp = components[fromCompNum-1]\n",
    "            for fromStrand,participants in fromCompDict.items():\n",
    "                if fromStrand=='+':\n",
    "                    upstreamBin = fromComp['last_bin']\n",
    "                    upstreamCol = fromComp['lastCol']\n",
    "                    fromRight = True\n",
    "                elif fromStrand=='-':\n",
    "                    upstreamBin = fromComp['first_bin']\n",
    "                    upstreamCol = fromComp['firstCol']\n",
    "                    fromRight = False\n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised direction of node link {fromStrand}')\n",
    "                if upstreamBin - 1 != component['last_bin']:\n",
    "                    numLinks += 1\n",
    "                component['rarrivals'].append({\n",
    "                        'upstream': upstreamBin,\n",
    "                        'downstream': component['last_bin'],\n",
    "                        'upstreamCol': upstreamCol,\n",
    "                        'downstreamCol': component['lastCol'],\n",
    "                        'otherSideRight': fromRight,\n",
    "                        'participants': participants\n",
    "                    })\n",
    "    \n",
    "    fromCompDict = fromComponentLinks.get(compNum+1,{}) # !!! Check that in fromComponentLinks component numbering is 1-based\n",
    "    if '+' in fromCompDict:\n",
    "        for toCompNum,toCompDict in fromCompDict['+'].items():\n",
    "            toComp = components[toCompNum-1]\n",
    "            for toStrand,participants in toCompDict.items():\n",
    "                if toStrand=='+':\n",
    "                    downstreamBin = toComp['first_bin']\n",
    "                    downstreamCol = toComp['firstCol']\n",
    "                    toRight = False\n",
    "                elif toStrand=='-':\n",
    "                    downstreamBin = toComp['last_bin']\n",
    "                    downstreamCol = toComp['lastCol']\n",
    "                    toRight = True\n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised direction of node link {toStrand}')\n",
    "                if component['last_bin'] + 1 != downstreamBin: \n",
    "                    numLinks += 1\n",
    "                component['rdepartures'].append({\n",
    "                        'upstream': component['last_bin'],\n",
    "                        'downstream': downstreamBin,\n",
    "                        'upstreamCol': component['lastCol'],\n",
    "                        'downstreamCol': downstreamCol,\n",
    "                        'otherSideRight': toRight,\n",
    "                        'participants': participants\n",
    "                    })\n",
    "    if '-' in fromCompDict:\n",
    "        for toCompNum,toCompDict in fromCompDict['-'].items():\n",
    "            toComp = components[toCompNum-1]\n",
    "            for toStrand,participants in toCompDict.items():\n",
    "                doAddDeparture = True\n",
    "                if toStrand=='+':\n",
    "                    downstreamBin = toComp['first_bin']\n",
    "                    downstreamCol = toComp['firstCol']\n",
    "                    toRight = False\n",
    "                elif toStrand=='-':\n",
    "                    downstreamBin = toComp['last_bin']\n",
    "                    downstreamCol = toComp['lastCol']\n",
    "                    toRight = True\n",
    "                    if component['first_bin']-downstreamBin==1:\n",
    "                        doAddDeparture = False\n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised direction of node link {toStrand}')\n",
    "                if doAddDeparture:\n",
    "                    if component['first_bin'] -1 != downstreamBin:\n",
    "                        numLinks += 1\n",
    "                    component['ldepartures'].append({\n",
    "                            'upstream': component['first_bin'],\n",
    "                            'downstream': downstreamBin,\n",
    "                            'upstreamCol': component['firstCol'],\n",
    "                            'downstreamCol': downstreamCol,\n",
    "                            'otherSideRight': toRight,\n",
    "                            'participants': participants\n",
    "                        })\n",
    "        \n",
    "    return component, numLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def checkLinks(leftComp,rightComp):\n",
    "    leftRdepCond = np.all([link['upstream']+1==link['downstream'] for link in leftComp['rdepartures']])\n",
    "    leftRarrCond = np.all([link['upstream']-1==link['downstream'] for link in leftComp['rarrivals']])\n",
    "\n",
    "    rightRdepCond = np.all([link['upstream']-1==link['downstream'] for link in rightComp['ldepartures']])\n",
    "    rightRarrCond = np.all([link['upstream']+1==link['downstream'] for link in rightComp['larrivals']])\n",
    "    \n",
    "    return leftRdepCond and leftRarrCond and rightRarrCond and rightRdepCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def searchIndicesPosRecord(redisConn,redisCaseID,zoomLevel,accessions,posMapping):\n",
    "    for pathID,posMappingPath in posMapping.items():\n",
    "        iset_add(redisConn, f'{redisCaseID}.{zoomLevel}.{accessions[pathID]}.Pos',posMappingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exportLayer(zoomLevel,components,componentNucleotides,\n",
    "                fromComponentLinks,toComponentLinks,\n",
    "                rootStruct,\n",
    "                outputPath,outputName,\n",
    "                maxLengthComponent,maxLengthChunk,\n",
    "                inversionThreshold=0.5,\n",
    "                redisConn=None,\n",
    "                redisCaseID=None,\n",
    "                accessions=None,\n",
    "                debug=False):\n",
    "    \n",
    "    #Create the directory to hold zoomLevel chunks and fasta files (if available)\n",
    "    createZoomLevelDir(outputPath,outputName,zoomLevel)\n",
    "\n",
    "    chunkList = rootStruct[\"zoom_levels\"].setdefault(zoomLevel,{\n",
    "        \"last_bin\": components[-1][\"last_bin\"],\n",
    "        \"last_col\": components[-1][\"lastCol\"],\n",
    "        \"files\":[]\n",
    "    })\n",
    "\n",
    "    chunk = deepcopy(chunkTemplate)\n",
    "    chunkNum = 0\n",
    "    prevTotalCols = 0\n",
    "    curCompCols = 0\n",
    "    nucleotides = ''\n",
    "    nBins = 0\n",
    "    numComps = len(components)\n",
    "    numCompsDigits = int(np.ceil(np.log10(numComps)))\n",
    "    \n",
    "    if redisConn:\n",
    "        accessions = rootStruct[\"pathNames\"]\n",
    "    posMapping = {}\n",
    "    geneMapping = {}\n",
    "    for compNum in range(numComps):\n",
    "        if debug:    \n",
    "            print(f'Recording component {compNum+1:0{numCompsDigits}}/{numComps:0{numCompsDigits}}')\n",
    "        else:\n",
    "            print(f'\\rRecording component {compNum+1:0{numCompsDigits}}/{numComps:0{numCompsDigits}}',end='')\n",
    "        component,numLinks = addLinksToComp(compNum,components,fromComponentLinks,toComponentLinks)\n",
    "        if redisConn is not None:\n",
    "            \n",
    "            for pathID,inv,matrixPathArray in component['matrix']:\n",
    "                pathRedisStartTime = time.time()\n",
    "                for binNum,binMatrix in zip(*matrixPathArray):\n",
    "                    if binMatrix[1]>inversionThreshold:\n",
    "                        overallBin = component['last_bin']-binNum\n",
    "                    else:\n",
    "                        overallBin = component['first_bin']+binNum\n",
    "                    \n",
    "                    posMapping.setdefault(pathID,{}).update({int(overallBin):[(int(posStart),int(posEnd)) for posStart,posEnd in binMatrix[2]]})\n",
    "\n",
    "        compNBins = component[\"last_bin\"]-component[\"first_bin\"]+1\n",
    "        nBins += compNBins\n",
    "\n",
    "        if len(componentNucleotides)>0:\n",
    "            nucleotides += componentNucleotides[compNum]\n",
    "        chunk[\"components\"].append(component)\n",
    "        chunk['compVisCol'][component['first_bin']] = compNBins + numLinks\n",
    "        \n",
    "        # End of chunk\n",
    "        if compNum<len(components)-1:\n",
    "            if len(chunk['components'])>=maxLengthChunk:\n",
    "                rootStruct,chunk,nBins,chunkNum,prevTotalCols,curCompCols,nucleotides = finaliseChunk(\n",
    "                                                                                          rootStruct,\n",
    "                                                                                          zoomLevel,\n",
    "                                                                                          chunk,\n",
    "                                                                                          nucleotides,\n",
    "                                                                                          nBins,\n",
    "                                                                                          chunkNum,\n",
    "                                                                                          curCompCols,\n",
    "                                                                                          prevTotalCols,\n",
    "                                                                                          outputPath,\n",
    "                                                                                          outputName)\n",
    "        else:\n",
    "            rootStruct,chunk,nBins,chunkNum,prevTotalCols,curCompCols,nucleotides = finaliseChunk(\n",
    "                                                                                      rootStruct,\n",
    "                                                                                      zoomLevel,\n",
    "                                                                                      chunk,\n",
    "                                                                                      nucleotides,\n",
    "                                                                                      nBins,\n",
    "                                                                                      chunkNum,\n",
    "                                                                                      curCompCols,\n",
    "                                                                                      prevTotalCols,\n",
    "                                                                                      outputPath,\n",
    "                                                                                      outputName)\n",
    "    if redisConn is not None:\n",
    "        searchIndicesPosRecord(redisConn,redisCaseID,zoomLevel,accessions,posMapping)\n",
    "        try:\n",
    "            redisConn.bgsave()\n",
    "        except ResponseError:\n",
    "            pass\n",
    "    if not debug:\n",
    "        print()\n",
    "    \n",
    "    print(f'Recording zoom level {zoomLevel} finished.')\n",
    "\n",
    "    return rootStruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4170850",
   "metadata": {},
   "source": [
    "## Main exporter wrapper with its helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589b939-8398-4466-b0e2-f3d9425796f2",
   "metadata": {},
   "source": [
    "This is the main orchestrating function that export a single graph to Pantograph Visualisation tool with a couple of auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compLinksToAccCompLinks(compLinks,doCompDir=False):\n",
    "    accCompLinks = {}\n",
    "    if doCompDir:\n",
    "        accCompDir = {}\n",
    "    \n",
    "    for comp1,comp1Dict in compLinks.items():\n",
    "        for strand1,strand1Dict in comp1Dict.items():\n",
    "            for comp2,comp2Dict in strand1Dict.items():\n",
    "                for strand2,accList in comp2Dict.items():\n",
    "                    for acc in accList:\n",
    "                        # Do we need to include strand or component only is enough. Here are two options, just choose one:\n",
    "#                         accCompLinks.setdefault(acc,{}).setdefault((comp1,strand1),set()).add((comp2,strand2))\n",
    "                        accCompLinks.setdefault(acc,{}).setdefault(comp1,set()).add(comp2)\n",
    "                        \n",
    "                        if doCompDir:\n",
    "                            accCompDir.setdefault(acc,{})[comp1] = strand1\n",
    "                            accCompDir.setdefault(acc,{})[comp2] = strand2\n",
    "    if doCompDir:\n",
    "        return accCompLinks,accCompDir\n",
    "    else:\n",
    "        return accCompLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c86728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recordZoomLevelForDebug(zoomNodeToComponent,\n",
    "                            zoomComponentToNodes,\n",
    "                            zoomComponents,\n",
    "                            nodeToComponent,\n",
    "                            componentToNodes,\n",
    "                            components,\n",
    "                            zoomLevel):\n",
    "    '''\n",
    "    A function which records result of segmentation to dictionaries, \n",
    "    which holds results for all zoom levels.\n",
    "    It is currently used only for debugging purposes and in normal operation \n",
    "    all zoom level dictionaries are not created and used.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    Parameters are self-explanatory. Those starting from `zoom` are dictionary\n",
    "    holding all zoom levels, whereas the same parameter names without `zoom`\n",
    "    in the beginning are data for zoom level given in `zoomLevel` parameter.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    Returns modified dictionaries with `zoom` in the beginning of the names. Theoretically,\n",
    "    it should work without return at all (through pointers passed to the function), but it\n",
    "    did not work before and to be on the safe side, this is done through return.\n",
    "    \n",
    "    '''\n",
    "    zoomNodeToComponent[zoomLevel] = nodeToComponent\n",
    "    zoomComponentToNodes[zoomLevel] = componentToNodes\n",
    "    zoomComponents[zoomLevel] = components\n",
    "    \n",
    "    return  zoomNodeToComponent, \\\n",
    "            zoomComponentToNodes, \\\n",
    "            zoomComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11811a1d-59b6-4ef1-a7c7-3ef428687b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def searchIndicesGeneRecord(redisConn,redisCaseID,geneMapping,genPosMapping,altChrGenPosMapping,genPosSearchMapping,pangenPosSearchMapping):\n",
    "    '''\n",
    "    Recording prepared metadata structures into Redis DB\n",
    "    '''\n",
    "    for pathID,geneMappingPath in geneMapping.items():\n",
    "        iset_add(redisConn,f'{redisCaseID}.{pathID}.Gene',geneMappingPath)\n",
    "    for pathID,genPosMappingPath in genPosMapping.items():\n",
    "        iset_add(redisConn,f'{redisCaseID}.{pathID}.GenPos',genPosMappingPath)\n",
    "    for pathID,genPosSearchMappingPath in genPosSearchMapping.items():\n",
    "        iset_add(redisConn,f'{redisCaseID}.{pathID}.GenPosSearch',genPosSearchMappingPath)\n",
    "    for pathID,pangenPosSearchMappingPath in pangenPosSearchMapping.items():\n",
    "        iset_add(redisConn,f'{redisCaseID}.{pathID}.PangenPosSearch',pangenPosSearchMappingPath)\n",
    "    for pathID,altChrGenPosMappingPath in altChrGenPosMapping.items():\n",
    "        iset_add(redisConn,f'{redisCaseID}.{pathID}.AltChrGenPos',altChrGenPosMappingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a54742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exportToPantograph(graph=None, inputPath=None, GenomeGraphParams={}, \n",
    "                       outputPath=None, outputName=None, outputSuffix=None, \n",
    "                       isSeq=True,\n",
    "                       nodeLengths=None,\n",
    "                       redisConn=None,\n",
    "                       zoomLevels=[1], fillZoomLevels=True, maxLengthComponent=100, maxLengthChunk=20, inversionThreshold=0.5,\n",
    "                       debug=False, returnDebugData=False):\n",
    "    '''\n",
    "    This function is used by `exportProject` function and should not normally be used independently now.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if graph is None:\n",
    "        if inputPath is not None:\n",
    "            print('Loading Genome')\n",
    "            graph = GenomeGraph(gfaPath=inputPath, isGFASeq=isSeq, **GenomeGraphParams)\n",
    "        else:\n",
    "            raise ValueError(\"Either graph or inputpath to GFA file should be provided\")\n",
    "\n",
    "    if outputPath is None and outputName is None:\n",
    "        if inputPath is not None:\n",
    "            if outputSuffix is not None:\n",
    "                outputPath, outputName = pathConvert(inputPath, suffix = outputSuffix)\n",
    "            else:\n",
    "                outputPath, outputName = pathConvert(inputPath)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"If inputPath is not given, then outputPath and outputName should be provided.\")\n",
    "    else:\n",
    "        if outputSuffix is not None:\n",
    "            outputName = outputName + outputSuffix\n",
    "    print(f'Recording Pantograph data to {outputPath}{os.path.sep}{outputName}')\n",
    "\n",
    "\n",
    "    rootStruct = deepcopy(rootStructTemplate)\n",
    "    rootStruct[\"pathNames\"] = graph.accessions\n",
    "\n",
    "    if returnDebugData:\n",
    "        # temporary structure for testing, in normal mode each zoomlevel \n",
    "        # should be just saved as soon as it is processed and only previous and current zoom level should be preserved.\n",
    "        zoomComponentLengths = {}\n",
    "        zoomNodeToComponent = {}\n",
    "        zoomComponentToNodes = {}\n",
    "        zoomComponents = {}  \n",
    "        zoomCompNucleotides = {}\n",
    "        zoomAccStarts = {}\n",
    "        zoomAccEnds = {}\n",
    "        zoomFromComponentLinks = {}\n",
    "        zoomToComponentLinks = {}\n",
    "        \n",
    "        zoomLinkLengths = {}\n",
    "        zoomPairedLinks = {}\n",
    "        zoomInterconnectedLinks = {}\n",
    "        zoomBlockEdges = {}\n",
    "        \n",
    "        zoomOldToNewRemoval = {}\n",
    "        zoomNewToOldRemoval = {}\n",
    "        zoomLinkLengthsRemoval = {}\n",
    "        zoomPairedLinksRemoval = {}\n",
    "        zoomInterconnectedLinksRemoval = {}\n",
    "        zoomBlockEdgesRemoval = {}\n",
    "        \n",
    "        zoomFromComponentLinksRemoval = {}\n",
    "        zoomToComponentLinksRemoval = {}\n",
    "        \n",
    "        \n",
    "    if nodeLengths is None:\n",
    "        nodeLengths = calcNodeLengths(graph)\n",
    "\n",
    "    pathLengths,pathNodeArray,pathNodeLengths,pathDirArray,pathNodeLengthsCum = initialPathAnalysis(graph,nodeLengths)\n",
    "    \n",
    "    if zoomLevels[0]>1:\n",
    "        zoomLevels = [1] + zoomLevels\n",
    "    \n",
    "    if len(zoomLevels)>1 or fillZoomLevels:\n",
    "        linkLengths,pairedLinks,interconnectedLinks,blockEdges,pathNodeInversion = \\\n",
    "                    getRemovableStructures(nodeLengths = nodeLengths, pathLengths = pathLengths, \n",
    "                                           pathNodeArray = pathNodeArray, pathDirArray = pathDirArray,forwardLinks = graph.forwardLinks,\n",
    "                                           inversionThreshold = inversionThreshold)\n",
    "    else:\n",
    "        linkLengths = None\n",
    "        pairedLinks = None\n",
    "        interconnectedLinks = None\n",
    "        blockEdges,pathNodeInversion = getBlockEdges(nodeLengths = nodeLengths, pathLengths = pathLengths, \n",
    "                                   pathNodeArray = pathNodeArray, pathDirArray = pathDirArray,forwardLinks = graph.forwardLinks,\n",
    "                                   inversionThreshold = inversionThreshold)\n",
    "    \n",
    "    if returnDebugData:\n",
    "        initialLinkLengths = deepcopy(linkLengths)\n",
    "        initialPairedLinks = deepcopy(pairedLinks)\n",
    "        initialInterconnectedLinks = deepcopy(interconnectedLinks)\n",
    "        initialBlockEdges = deepcopy(blockEdges)\n",
    "    \n",
    "    #Process lowest level zoom to create component structure.\n",
    "    [numNodes, # number of nodes\n",
    "    numNodesDigits, # \n",
    "    nodeToComponent,\n",
    "    componentToNode,\n",
    "    componentLengths,\n",
    "    components,\n",
    "    componentNucleotides,\n",
    "    fromLinks,toLinks,\n",
    "    fromComponentLinks,toComponentLinks,\n",
    "    accStarts,accEnds,\n",
    "    invertedStarts,invertedEnds,\n",
    "    combAnnotation,combGenPos,combAltChrGenPos,combGenPosSearch,combPangenPosSearch] = baseLayerZoom(graph,\n",
    "                                      outputPath,outputName,\n",
    "                                      pathNodeArray,pathDirArray,\n",
    "                                      pathLengths,nodeLengths,\n",
    "                                      pathNodeLengthsCum,\n",
    "                                      maxLengthComponent,\n",
    "                                      blockEdges,\n",
    "                                      inversionThreshold=inversionThreshold,\n",
    "                                      isSeq=isSeq)\n",
    "    print()\n",
    "    if returnDebugData:\n",
    "        zoomNodeToComponent,zoomComponentToNodes, zoomComponents = \\\n",
    "        recordZoomLevelForDebug(zoomNodeToComponent,zoomComponentToNodes,zoomComponents,\n",
    "                                nodeToComponent,componentToNode,components,\n",
    "                                1)\n",
    "    \n",
    "    if redisConn is not None:\n",
    "        searchIndicesGeneRecord(redisConn,outputName,combAnnotation,combGenPos,combAltChrGenPos, combGenPosSearch, combPangenPosSearch)\n",
    "    \n",
    "    fromComponentLinks,toComponentLinks, \\\n",
    "    linkLengths,pairedLinks,interconnectedLinks,blockEdges = \\\n",
    "    nodeToComponentLinks(components,componentToNode,nodeToComponent,\n",
    "                         fromLinks,toLinks,graph,\n",
    "                         fromComponentLinks,toComponentLinks,\n",
    "                         linkLengths,pairedLinks,interconnectedLinks,blockEdges,debug=debug)\n",
    "    \n",
    "    if returnDebugData:\n",
    "        zoomFromComponentLinks[1] = deepcopy(fromComponentLinks)\n",
    "        zoomToComponentLinks[1] = deepcopy(toComponentLinks)\n",
    "        zoomLinkLengths[1] = deepcopy(linkLengths)\n",
    "        zoomPairedLinks[1] = deepcopy(pairedLinks)\n",
    "        zoomInterconnectedLinks[1] = deepcopy(interconnectedLinks)\n",
    "    \n",
    "    rootStruct = deepcopy(rootStructTemplate)\n",
    "    rootStruct[\"pathNames\"] = graph.accessions\n",
    "    rootStruct[\"pangenome_length\"] = np.sum(componentLengths)\n",
    "\n",
    "    exportLayer(1,components,componentNucleotides,\n",
    "                fromComponentLinks,toComponentLinks,\n",
    "                rootStruct,\n",
    "                outputPath,outputName,\n",
    "                maxLengthComponent,maxLengthChunk,\n",
    "                inversionThreshold=inversionThreshold,\n",
    "                redisConn=redisConn,redisCaseID=outputName)\n",
    "    \n",
    "    _zoomLevels = deepcopy(zoomLevels)\n",
    "    \n",
    "    if fillZoomLevels:\n",
    "        if len(linkLengths)>0:\n",
    "            maxLinkLength = max(linkLengths.keys())\n",
    "        else:\n",
    "            maxLinkLength = 0\n",
    "        if len(blockEdges)>0:\n",
    "            maxRearrangementLength = max(blockEdges.values())\n",
    "        else:\n",
    "            maxRearrangementLength = 0\n",
    "            \n",
    "        maxBlock = max(maxLinkLength,maxRearrangementLength)\n",
    "\n",
    "        while _zoomLevels[-1]<=maxBlock:\n",
    "            _zoomLevels.append(_zoomLevels[-1]*2)\n",
    "    \n",
    "    for zoomLevel in _zoomLevels[1:]:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            # Collapsing is done through removing some particular links (not satisfying specific conditions)\n",
    "            components,componentLengths,fromComponentLinks,toComponentLinks,accStarts,accEnds,\\\n",
    "            linkLengths,pairedLinks,interconnectedLinks,blockEdges,\\\n",
    "            oldToNewInd,newToOldInd = \\\n",
    "            clearInvisible(zoomLevel,linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                           fromComponentLinks,toComponentLinks,\n",
    "                           accStarts,accEnds,components,componentLengths)\n",
    "\n",
    "            if returnDebugData:\n",
    "                zoomOldToNewRemoval[zoomLevel] = deepcopy(oldToNewInd)\n",
    "                zoomNewToOldRemoval[zoomLevel] = deepcopy(newToOldInd)\n",
    "                zoomLinkLengthsRemoval[zoomLevel] = deepcopy(linkLengths)\n",
    "                zoomPairedLinksRemoval[zoomLevel] = deepcopy(pairedLinks)\n",
    "                zoomInterconnectedLinksRemoval[zoomLevel] = deepcopy(interconnectedLinks)\n",
    "                zoomBlockEdgesRemoval[zoomLevel] = deepcopy(blockEdges)\n",
    "                zoomFromComponentLinksRemoval[zoomLevel] = deepcopy(fromComponentLinks)\n",
    "                zoomToComponentLinksRemoval[zoomLevel] = deepcopy(toComponentLinks)\n",
    "\n",
    "            # Process next zoom level\n",
    "            components,componentLengths,fromComponentLinks,toComponentLinks,accStarts,accEnds,\\\n",
    "            linkLengths,pairedLinks,interconnectedLinks,blockEdges,oldToNewInd,newToOldInd= \\\n",
    "            nextLayerZoom(zoomLevel,\n",
    "                          components,componentLengths,\n",
    "                          fromComponentLinks,toComponentLinks,graph,\n",
    "                          accStarts,accEnds,\n",
    "                          maxLengthComponent,linkLengths,pairedLinks,interconnectedLinks,blockEdges,\n",
    "                          inversionThreshold=inversionThreshold)\n",
    "\n",
    "            if returnDebugData:\n",
    "                zoomNodeToComponent, zoomComponentToNodes, zoomComponents = \\\n",
    "                recordZoomLevelForDebug(zoomNodeToComponent,zoomComponentToNodes,zoomComponents,\n",
    "                                        oldToNewInd,newToOldInd,components,\n",
    "                                        zoomLevel)\n",
    "\n",
    "                zoomFromComponentLinks[zoomLevel] = deepcopy(fromComponentLinks)\n",
    "                zoomToComponentLinks[zoomLevel] = deepcopy(toComponentLinks)\n",
    "                zoomLinkLengths[zoomLevel] = deepcopy(linkLengths)\n",
    "                zoomPairedLinks[zoomLevel] = deepcopy(pairedLinks)\n",
    "                zoomInterconnectedLinks[zoomLevel] = deepcopy(interconnectedLinks)\n",
    "\n",
    "\n",
    "            # Zoom level should be split into chunks and recorded.\n",
    "            exportLayer(zoomLevel,components,[],\n",
    "                    fromComponentLinks,toComponentLinks,\n",
    "                    rootStruct,\n",
    "                    outputPath,outputName,\n",
    "                    maxLengthComponent,maxLengthChunk,\n",
    "                    inversionThreshold=inversionThreshold,\n",
    "                    redisConn=redisConn,redisCaseID=outputName)\n",
    "        except RuntimeError as error:\n",
    "            print(error)\n",
    "            print(traceback.format_exc())\n",
    "            break\n",
    "    \n",
    "    with open(f'{outputPath}{os.path.sep}{outputName}{os.path.sep}bin2file.json','w') as f:\n",
    "        json.dump(rootStruct,f,cls=NpEncoder)\n",
    "    \n",
    "    if returnDebugData:\n",
    "        return initialLinkLengths, initialPairedLinks, initialInterconnectedLinks, initialBlockEdges, \\\n",
    "                zoomNodeToComponent,zoomComponentToNodes,zoomComponents,\\\n",
    "                zoomFromComponentLinks, zoomToComponentLinks, zoomLinkLengths, zoomPairedLinks, zoomInterconnectedLinks, \\\n",
    "                zoomOldToNewRemoval, zoomNewToOldRemoval, \\\n",
    "                zoomLinkLengthsRemoval, zoomPairedLinksRemoval, zoomInterconnectedLinksRemoval, zoomBlockEdgesRemoval, \\\n",
    "                zoomFromComponentLinksRemoval, zoomToComponentLinksRemoval, \\\n",
    "                graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3dc5c-24f1-42ed-8245-e4ecccfb290a",
   "metadata": {},
   "source": [
    "## Project generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d89fe-4577-4ae8-a18d-572c4c16648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exportProject(projectID, projectName, caseDict, pathToIndex, pathToGraphs,\n",
    "                  redisHost=None, redisPort = 6379, redisDB = 0,\n",
    "                  suffix = '',\n",
    "                  maxLengthComponent = 100, maxLengthChunk = 6,\n",
    "                  inversionThreshold = 0.5,\n",
    "                  isSeq = True,\n",
    "                  zoomLevels = [1], fillZoomLevel = True):\n",
    "    '''\n",
    "    This is the only function that should normally be used to export a set of graphs (e.g. a graph per chromosome)\n",
    "    to Pantograph Visualisation tool as a project (or interconnected structure).\n",
    "    \n",
    "    Exporting of each graph creates a case directory <projectID>_<case name><suffix> with `bin2file.json` file which describes the case overall \n",
    "    and each zoom level. At the same time, each zoom level is contained in multiple chunk JSON files, each zoom level `n` is in the directory `n` \n",
    "    inside the case directory. Each JSON chunk files contains all required information to visualise up to `maxLengthChunk` components at a given\n",
    "    zoom level.\n",
    "    \n",
    "    ALl case directories are in project directory together with `<projectID>_project.json`, which is simply provides association between case names \n",
    "    and and corresponding directory name.\n",
    "    \n",
    "    Finally, information about the project will be recorded to Pantograph Visualisation tool data index to make it discoverable by the tool.\n",
    "    \n",
    "    In addition, no metadata is recorded into these files as it inflates it very quickly. Instead, a very simple (optional) API works alongside main\n",
    "    Pantograph Visualisation tool which provides a lot of various metadata on request if API available or do nothing if not. This API uses Redis DB\n",
    "    with special DB schema.\n",
    "    \n",
    "    When graphs are exported some metadata (annotations, genome and pangenome positions) can be recorded to Redis DB. If Redis DB is not available or \n",
    "    recording of metadata is not needed, then parameter `redisHost` should be omitted. Otherwise, if Redis DB is available and metadata should be recorded, \n",
    "    then `redisHost` should be set to the hostname (or IP address) of the Redis DB server\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    `projectID`: str - this is a project id and will be the name of directory that holds all data to visualise the project.\n",
    "                  It is recommended not to have special symbols and spaces in this name\n",
    "        \n",
    "\n",
    "    `projectName`:str - Human readable short name describing the project. It should not be more than 50 symbols.\n",
    "    \n",
    "    `caseDict`: dict{str:str} - a dictionary of each individual graph within project. Usually one file per chromosome, although, in some cases \n",
    "                there can be only one graph for all chromosomes or other cases.\n",
    "                Graph names can contain path relative to `pathToGraphs` if they are in different directories.\n",
    "    \n",
    "\n",
    "    `pathToIndex`: str - along with project data structure, exporting adds information to the index.json file which is\n",
    "                    read by Pantograph visualisation component in the first instance. If it does not exist at the given path\n",
    "                    it will be created at the given path.\n",
    "    \n",
    "    `pathToGraphs`: str - path where graph files mentioned in `caseDict` can be found.\n",
    "    \n",
    "    \n",
    "    `redisHost`: str or None (Default is 'redis') - A host name of Redis DB instance. Default value is done as it is working with \n",
    "                 Docker Compose infrastructure.\n",
    "    `redisPort`: int (Default is 6379) - Port of Redis DB instance. If `redisHost` is None, then this parameter is ignorred.\n",
    "    `redisDB`: int (Default is 0) - A DB number for the given Redis DB instance. If `redisHost` is None, then this parameter is ignorred.\n",
    "    \n",
    "    `suffix`: str (Default is '') - Suffix is used for the naming of the output directories for each graph (molecule or chromosome), \n",
    "              e.g. if project is called 'pangenome_project', case is 'chr1' and suffix is '_new', then the output directory for this \n",
    "              graph visualisation data structure will be called 'pangenome_project_chr1_new'. If not set, then no suffix will be added.\n",
    "\n",
    "    `maxLengthComponent`: int (Default is 100) - defined how large (in visual columns) can the component be. If continuity allows for \n",
    "                          longer components, it will be broken into components of maximum `maxLengthComponent` columns. \n",
    "                          If not set, default of 100 is used.\n",
    "\n",
    "    `maxLengthChunk`: int (Default is 6) - defined how many components will be recorded into a single chunk file in the visualisation data structure. \n",
    "                      Smaller chunk files allows to load data into memory more precisely, but if a lot of small components should be loaded, \n",
    "                      larger chunks make it quicker. If not set, default of 6 will be used.\n",
    "\n",
    "    `inversionThreshold`: float (Default is 0.5) - Each cell has calculated inversion ratio (see user manual for details). It will be shown as \n",
    "                          inverted (shades of red) if its inversion ratio is (strictly) more than `inversionThreshold` set. If not set, default is 0.5.\n",
    "\n",
    "    `isSeq`: bool (Default is True) - If the graph is not nucleotide graph (e.g. gene graph generated from paths or annotations), `isSeq` should \n",
    "             be set to False. Default is True.\n",
    "\n",
    "    `zoomLevels`: list[int] (Default is [1]) - Zoom levels that should be generated. The number indicate how many maximum nucleotides (or genes \n",
    "                  in case of gene graph) can be displayed as single column in the visualised matrix. Zoom level of 1 is added automatically \n",
    "                  if it is not in the list. Each previous zoom level should be a factor or the next one, i.e. next zoom level should be obtained \n",
    "                  by multiplying previous one by an integer more than 1. E.g. zoom levels [1,2,4,8] is correct, but zoom levels [1,2,4,6] is \n",
    "                  incorrect because 4 is not a factor of 6. Defaults is [1], meaning to generate only lowest zoom level (single nucleotide/gene resolution).\n",
    "\n",
    "    `fillZoomLevel`: bool (Default is True) - If set to True, then on top of the highest zoom level set in `zoomLevels`, the function will keep \n",
    "                     adding more zoom levels by multiplying it by 2 until there is no non-linear links to remove left. Default is True\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    zoomLevels = adjustZoomLevels(zoomLevels)\n",
    "    \n",
    "    indexFile = f'{pathToIndex}{os.path.sep}index.json'\n",
    "    if os.path.exists(indexFile):\n",
    "        with open(indexFile) as f:\n",
    "            indexDict = json.load(f)\n",
    "    else:\n",
    "        indexDict = {}\n",
    "    \n",
    "    projectOutputPath = f'{pathToIndex}{os.path.sep}{projectID}'\n",
    "    projectDict = {}    \n",
    "    \n",
    "    for seqID,gfaFile in caseDict.items():\n",
    "\n",
    "        coreGFApath = f'{pathToGraphs}{os.path.sep}{gfaFile}'\n",
    "        \n",
    "        outputName = f'{projectID}_{seqID}{suffix}'\n",
    "        \n",
    "        projectDict[outputName] = seqID\n",
    "        \n",
    "        print()\n",
    "        print(f'Processing case {outputName}')\n",
    "        if redisHost is not None:\n",
    "            print(f'Using Redis DB server at {redisHost}:{redisPort} with db number {redisDB}.')\n",
    "            redisConn = Redis(host = redisHost, port = redisPort, db=redisDB)\n",
    "        else:\n",
    "            redisConn = None\n",
    "\n",
    "        startTime = time.time()\n",
    "        exportToPantograph(inputPath=coreGFApath, GenomeGraphParams={}, \n",
    "                                        outputPath=projectOutputPath, outputName=outputName,\n",
    "                                        isSeq=isSeq,\n",
    "                                        redisConn=redisConn,\n",
    "                                        zoomLevels=zoomLevels, \n",
    "                                        fillZoomLevels = fillZoomLevel,\n",
    "                                        maxLengthComponent=maxLengthComponent, \n",
    "                                        maxLengthChunk=maxLengthChunk, \n",
    "                                        inversionThreshold=inversionThreshold)\n",
    "        runTime = time.time() - startTime\n",
    "\n",
    "        print(f'Exporting gene graph for {seqID} took {runTime} seconds')\n",
    "    \n",
    "    redisConn.close()\n",
    "    \n",
    "    with open(f'{projectOutputPath}{os.path.sep}{projectID}_project.json','w') as f:\n",
    "        json.dump(projectDict,f)\n",
    "    \n",
    "    indexDict[projectID] = projectName\n",
    "    with open(indexFile,'w') as f:\n",
    "        json.dump(indexDict,f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
